{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**PART 3 - Generating text with KERAS**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading the .csv files containing the male and female posts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#!pip install pandas\n",
    "import pandas \n",
    "data_female = pandas.read_csv('female_posts.csv', sep=',', na_values=\".\", encoding='ISO-8859-1')\n",
    "data_male = pandas.read_csv('male_posts.csv', sep=',', na_values=\".\", encoding='ISO-8859-1')\n",
    "data_female = data_female[\"status_message\"]\n",
    "data_male = data_male[\"status_message\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Concatenating the female posts to a single text/string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Mohammed Nazili Suddicqui\\'s post advertising a payment gateway is removed for a second time.  Third instance will result in the member being removed from the group. /n/r New member post advertising a payment gateway has been removed. /n/r This is amazing! /n/r We need 20 volunteers to go with us to Daytona Beach Florida for the NASCAR fundraiser! All expense paid! /n/r All presented by Alabama STEM Education! It will definitely move you to greatness? /n/r We may be older than we thought. /n/r Geostorm is a real thing on Mars. How well is SpaceX prepared for this problem? /n/r I need to interview two helping service professionals from two different settings (i.e. school, hospital, or prison), one of which must be a clinical psychologist, by this weekend.\\r\\nAnyone here fitting the description who could help? /n/r Any flat earthers in this group who\\'d love a discussion?? /n/r Forgive me I couldn\\'t resist. /n/r Amazing Mirrorless Car Camera Announced by Mitsubishi:\\r\\nhttp://www.techhound.org/auto-tech/amazing-mirrorless-car-camera-announced-by-mitsubishi/ /n/r Galaxy Note 8 Special Edition Designed by Samsung for 2018 Winter Olympics!\\r\\nhttp://www.techhound.org/gadgets/galaxy-note-8-special-edition-designed-samsung-2018-winter-olympics/ /n/r New Vision of a Hypersonic Spy Aircraft Presented by Boeing!\\r\\nhttp://www.techhound.org/auto-tech/new-vision-hypersonic-spy-aircraft-presented-boeing/ /n/r Nissan Announced its Self-Driving IMx Concept EV at CES!\\r\\nhttp://www.techhound.org/auto-tech/nissan-announced-self-driving-imx-concept-ev-ces/ /n/r This is an awesome way to teach young men in a different atmosphere and they do it together as 1! All they need to do is submit an essay about their career goals!! /n/r Vampire hunting accessories from the 19th century. /n/r Colorful arachnids (Y) /n/r Advertising post deleted and member advised. /n/r Remember Rocky, Bulwinkle and the gang?\\r\\nWith my thanks to Robert Coates /n/r A little sci-tech humor for the first week of the new year. ;) (reposted) /n/r the global #cloudmigration market to grow from USD 1961.44 million in 2016 to USD 8678.73 million by 2023, at a Compound Annual Growth Rate (CAGR) of 23.67%. The year 2016 has been considered as the base year, while the forecast period is up-to 2023. click here http://bit.ly/2CzXzQ2 for more details. /n/r 2nd Annual Global Women\\'s March Denver - JANUARY 20, 2018 ... Spread the word! /n/r To all our wonderful members,\\r\\nA HAPPY NEW YEAR /n/r Science and society. Admins please let me know if I have crossed a line here. /n/r Science, or the lack thereof. /n/r I have just finished reading a book called: The water will come Rising seas,sinking cities,and the remaking of the civilized world. It is by author Jeff Goodell   I learned alot of things i thought i knew were not exactly the way they are but i also learned about some technology advancements in different areas.  I would highly recommend this book to everyone to read.   I would also like to know if anyone in this group is working on or trying to stop flooding in their areas or part of group trying to help relocate those who are in prone areas. /n/r What are your insights about the future? Comment down. /n/r My thanks to Ravi for sharing. /n/r I saw a similar graphic but thought it could be done a bit better (including the Republican elephant shitting out the crappy idea that the CDC can\\'t use these words anymore). Feel free to share my graphic. /n/r One post removed for solicitation and member has been warned. /n/r Nuff Said... /n/r One member blocked from the group and comments deleted due to beligerence, rudeness, incivility and unacceptable posts. /n/r One Comment deleted due to inappropriate language. A reminder, this is a discussion group and we ask, and expect, your responses and comments to show respect, civility, and acceptable language. /n/r One reported post deleted for trivial content and member advised we do not play games. /n/r 5 point full-proof guide to respond to your online reviews.\\r\\nhttp://bit.ly/2Auo9tC /n/r With so many cancer break-throughs, can we honestly believe there is no cure? This science technology page should know the most about how technology has sky rocketed in the past decades. Cancer research and treatments are making millions of dollars for the pharmaceutical companies, but still no cure? Does anybody have any insight on why? Or is there anybody that believes the cancer cure is being hidden for some reason? Looking for any information on \"proof\" there is a cancer cure. /n/r Comment deleted due to unacceptable material, and failure to respond to moderator\\'s request.\\r\\nA reminder to our members, a moderator will only request your action if there is a need such as language, sexual content, etc., etc. in a comment or a post. It is our job to see the rules are followed, and appreciate your cooperation, however not cooperating will result in the offensive language or content being removed. If ever in doubt about what is or isn\\'t acceptable, please take the the time to contact the admins of this group.  We are always available to help or answer your questions. /n/r What a Blessing!! /n/r Pay it forward!! /n/r The media no longer presents objective facts. Almost every bit of news circulating through social media are little more than opinion pieces at best, and intentional attempts to mislead and sway public opinion at its worst. hyperbole and cherry picked \"facts\" dominate the news cycles, and are used to whip up tensions and further divide our society. \\r\\n\\r\\nhttp://fair.org/take-action-now/media-activism-kit/how-to-detect-bias-in-news-media/ /n/r Post deleted for advocating violence and member warned and advised of group\\'s rules regarding such posting. /n/r Science - Technology - Society !! /n/r Let\\'s get our children acclimated to STEM Education! Give to Alabama STEM Education for the Holidays! Great tax Wright Off?? /n/r That\\'s amazing! /n/r It is the Need of The Hour....\\r\\nAwake ....INDIA...Awake...\\r\\nSave Ur  DAUGHTERS..Or\\r\\nU will be a Nation without MOTHER... /n/r Our free online course \"Biodiversity and Global Change: Science & Action\" is going to be running for the second-to-last time starting Monday (Nov 27). You can learn more about it here: https://www.coursera.org/learn/biodiversity/\\r\\nand watch our trailer here:\\r\\nhttps://www.youtube.com/watch?v=u7nQn3KTW7Q\\r\\nPlease share or like this post! /n/r One post deleted for unverified content and member informed.  Please remember no religious themed posts are allowed, and please verify your quoted material before posting.  If you are not certain, or have questions please contact an admin who will be glad to assist you. /n/r This bleeds over to all colleges, all fields. The control committees are in place. Is your job at risk? /n/r One post deleted for non-appropiate content.  If members do not have active messenger, we have no way to advise you. /n/r Why did we divided \\r\\nWhat we do not have multiplied /n/r Happy Holidays! /n/r ?? Live For Ur self ?? /n/r Word is that net neutrality ends Dec.14. /n/r What a STEMTASTIC FALL FESTIVAL ????? We had a great time ???? /n/r Oracle Code Bogot? y Buenos Aires !\\r\\n\\r\\nSi eres Desarrollador y te interesa adem?s ser Speaker en los eventos #OracleCode, aprovecha esta oportunidad y envia lo antes posible tu presentaci?n a trav?s de este formulario: \\r\\n\\r\\nhttps://lnkd.in/dMiKahz /n/r #BLACKFRIDAYDEAL This is the best offer I found so far. Grab professional courses at flat 80%OFF.  USE BF80. https://goo.gl/qxETgU /n/r Hello Hyderabad,\\r\\nNow IIMSE Started BIGDATA HADOOP class room training in Hyderabad by our Corporate Trainer Mr. Narendra (8+ Exp)\\r\\nOnly 5 students per batch, training includes Hadoop Development, Admin, Intro to Spark and Scala \\r\\nFor more details contact us @+91- 9133463111 info@iimse.com /n/r Hi All. Have anyone considered doing a research (in Analytics, DM, ML etc) instead of a Post Graduate program in Analytics ? I am thinking of this option. I am working right now. Can we do both side by side ? Will it be manageable ? If there is someone who is already doing this, i would like to get in touch with you. Please help. /n/r Don\\'t forget to like our Facebook page ?\\r\\n? https://www.facebook.com/CouthonConseil/\\r\\n\\r\\nJoins us on Twitter ?\\r\\n? https://www.twitter.com/CouthonConseil \\r\\n\\r\\nFollow us on LinkedIn ?\\r\\n? https://www.linkedin.com/m/company/10107092/ /n/r Join our 48 hour hackathon on a moving train starting from London, traveling to Brighton, Cambridge, Paris, Bordeaux and returning to London for pitches!!!\\r\\n\\r\\nMore information: hackathon.hacktrain.com\\r\\n\\r\\nhttps://www.facebook.com/events/248309182349938/ /n/r Transform your organization with powerful analytics. Learn how from industry experts from Qlik, IDC, Cloudera and more in this free Virtual Talkshow on 14 Nov: http://bit.ly/vywTalkshow /n/r Hello Guys,\\r\\n\\r\\nIf anybody is interested to provide training in R programming, please inbox or comment.\\r\\nTraining Mode : Online /n/r Visit Spotinst at Serverlessconf NYC\\r\\nSpotinst CEO Amiram Shachar will talk about multi-#Cloud Functions as a service & #Serverless strategy. /n/r PromptCloud, a leading web scraping service company, launched a community forum where like minded people, who work with data, can connect and share knowledge & ideas about web data extraction with each other. It\\'s an open and friendly community where people can ask questions, share tips and discuss all things web scraping. You can join the community here - https://webscrapingforum.promptcloud.com/ /n/r Learn MIS & BI with tableau with specially crafted courses for you, With TechandMate! Choose experts for training and save a Huge amount.\\r\\nGet Trained and Get Assured Placement\\r\\n#TechandMate\\r\\n#BookyourfreeDemoHere https://www.techandmate.com/request_your_demo /n/r Hello Everyone,\\r\\nIf anybody is interested to provide training of Excel and VBA in Hyderabad Location ..\\r\\nPlease inbox or comment here. /n/r Exclusive Data Series - Power BI program by Microsoft faculty only on Millionlights.\\r\\nClick here to watch now : http://www.yupptv.in/#!/play/Millionlights-TV\\r\\nYuppTV IndiaMillionlights - Nayi Shiksha - Nayi Soch\\r\\n#mobileTV #learning #microsoft #skilling #jobs /n/r A Data Analyst from Bangalore looking for opportunities, His Notice period is 2 months , Can mail requirements to veena@nichetalent.in\\r\\n\\r\\nA bright talented and self motivated Data Analyst  who has excellent organizational skills, is highly efficient \\r\\nand has a good eye for detail. Has extensive experience of assisting in the development and upgrading of database \\r\\nsystems and analytical techniques. Looking for a good opportunity as Data Analyst \\r\\n\\r\\nSkills includes: R, Python, Tableau, SQL, Analytics, Sap BO, Stats \\r\\nEducation : MCA /n/r Don\\'t forget to like our Facebook page ?\\r\\n? https://www.facebook.com/CouthonConseil/\\r\\n\\r\\nJoins us on Twitter ?\\r\\n? https://www.twitter.com/CouthonConseil \\r\\n\\r\\nFollow us on LinkedIn ?\\r\\n? https://www.linkedin.com/m/company/10107092/ /n/r Are you a Forex or Stock Exchange player? Get our ready made forecasts for EUR/USD and DAX - Over 90% accuracy! \\r\\nGet 7 days free trial at www.exmetrix.com #ExMetrix /n/r Require to setup hadoop framework on my system with Windows10. Plz suggest any free Vm service to practice and learn. /n/r V EDICION - OCTUBRE 2017, Preinscripciones abiertas. M?STER en ESTAD?STICA APLICADA con R SOFTWARE: T?cnicas Cl?sicas, Robustas, Avanzadas y Multivariantes. + INFO: http://bit.ly/2wnnx6j /n/r #TravelokaPH is looking for cool guys like you! Travel with us!\\r\\nWhy join us? Simple, we prioritize our people over anything, and we call it #PeopleFirst culture. We spoil and pamper our people to ensure that they are empowered. Not enough reason? What about if we tell you that you\\'ll have these benefits?:\\r\\n> Business travel opportunities\\r\\n> Free lunch\\r\\n> Free afternoon snacks\\r\\n> Free coffee/tea all day long\\r\\n> Cool offices all over asia\\r\\n> Health insurance covering the employee and 4 of his or her dependents\\r\\n> International culture\\r\\n> Get the chance to work with some of the brightest people on earth! We have a lot of people from Harvard, Wharton, Oxford!\\r\\n> AND MANY MORE!\\r\\n\\r\\nANALYST VACANCIES:\\r\\n- Pricing and Data Analyst  \\r\\nhttp://smrtr.io/tubJZw\\r\\n\\r\\n- Promotions Analyst/Associate\\r\\nhttp://smrtr.io/KncZ2g\\r\\n\\r\\n- Quality Assurance Analyst   \\r\\nhttp://smrtr.io/5UTYvg /n/r #TravelokaPH is looking for cool guys like you! Travel with us!\\r\\nWhy join us? Simple, we prioritize our people over anything, and we call it #PeopleFirst culture. We spoil and pamper our people to ensure that they are empowered. Not enough reason? What about if we tell you that you\\'ll have these benefits?:\\r\\n> Business travel opportunities\\r\\n> Free lunch\\r\\n> Free afternoon snacks\\r\\n> Free coffee/tea all day long\\r\\n> Cool offices all over asia\\r\\n> Health insurance covering the employee and 4 of his or her dependents\\r\\n> International culture\\r\\n> Get the chance to work with some of the brightest people on earth! We have a lot of people from Harvard, Wharton, Oxford!\\r\\n> AND MANY MORE\\r\\n\\r\\nUrgent vacancies:\\r\\n- Senior Product Marketing Manager / PH Marketing Head \\r\\nhttp://smrtr.io/ekisTg\\r\\n\\r\\n- Customer Excellence / Vendor Management Supervisor \\r\\nhttp://smrtr.io/cH3WMw\\r\\n\\r\\n- Ticketing Agents \\r\\nhttp://smrtr.io/cH3WMw /n/r Want to be a part of the #Serverless #MultiCloud World trend? Spotinst next meetup is for you!\\r\\nSpotinsttinst & Serverless are joining forces - next meetup. August 30th @ WeWork, 600 California St, San Francisco\\r\\nRSVP > http://bit.ly/2wAtmdV /n/r Don\\'t forget to like our Facebook page ?\\r\\n? https://www.facebook.com/CouthonConseil/\\r\\n\\r\\nJoins us on Twitter ?\\r\\n? https://www.twitter.com/CouthonConseil \\r\\n\\r\\nFollow us on LinkedIn ?\\r\\n? https://www.linkedin.com/m/company/10107092/ /n/r Don\\'t forget to like our Facebook page ?\\r\\n? https://www.facebook.com/CouthonConseil/\\r\\n\\r\\nJoins us on Twitter ?\\r\\n? https://www.twitter.com/CouthonConseil \\r\\n\\r\\nFollow us on LinkedIn ?\\r\\n? https://www.linkedin.com/m/company/10107092/ /n/r Check your basics in #bigdata with this quick free practice test: http://bit.ly/2n9SpQi /n/r Big Data Developer@ Bangalore 1-3 Yrs experience Strong knowledge & exp. in Event modelling, designing in Streaming & Lambda architectures BigData analytics stack - Hadoop, Spark, Storm, HBASE, YARN, Zookeeper; Query tools like Hive, Pig Python/Scala/Go/Java/Perl Machine Learning Models, Data mining Mail resume to veena@nichetalent.in\\r\\nMail resume with proper subject line with below details \\r\\nTotal Exp\\r\\nSkills \\r\\nCurrent CTC\\r\\nExpected CTC\\r\\nNotice period \\r\\nCurrent Location \\r\\nVeena , Niche Talent , Please mail me to  veena@nichetalent.in ,  +91 9483526863\\r\\nhttps://www.linkedin.com/in/veenarsrrecruiter/ /n/r Looking for Data Scientist @ Bangalore with 2- 6 Yrs Experience Develop data and programming specifications - Leverage complex data algorithms and numerical methods - Know stat techniques, machine learning & algos Mail resume to veena@nichetalent.in\\r\\nMail resume with proper subject line with below details \\r\\nTotal Exp\\r\\nSkills \\r\\nCurrent CTC\\r\\nExpected CTC\\r\\nNotice period \\r\\nCurrent Location \\r\\nVeena , Niche Talent , Please mail me to  veena@nichetalent.in ,  +91 9483526863\\r\\nhttps://www.linkedin.com/in/veenarsrrecruiter/ /n/r Big Data Developer@ Bangalore \\r\\n1-3 Yrs experience \\r\\nStrong knowledge & exp. in\\r\\nEvent modelling, designing in Streaming &Lambda architectures\\r\\nBigData analytics stack - Hadoop, Spark, Storm, HBASE, YARN, Zookeeper; Query tools like Hive, Pig\\r\\nPython/Scala/Go/Java/Perl\\r\\nMachine Learning Models, Data mining\\r\\n\\r\\nMail resume to veena@nichetalent.in /n/r Hi, I\\'m the Marketing Manager at Spotinst\\r\\nFollowing several, global and successful events we are arriving to NYC!\\r\\nSpotinsttinst next meetup together with #AWS on August 11th at the #AWS NY Loft.\\r\\nLearn how to Leverage the use of #EC2 Spot Instance for production workloads #Devops #cloud \\r\\nReg > http://bit.ly/2u9NaSR /n/r Hey, I\\'m the Marketing Manager at www.spotinst.com\\r\\nI would like to invite you to to like Spotinst page to get our awesome content, and event updates.\\r\\nOur next meetup is only ONE DAYS AWAY! Aug 2nd, Bay area #SFBAY.\\r\\nTogether with Rancher Labs and Jobvite find out how to leverage #AWS #Spot instances usage, maintain business continuity and application availability.\\r\\nCut #cloud costs, optimize #container and #app performance and meet cool people. #Devops \\r\\nReg > http://bit.ly/2u9Dacj /n/r Want a secured job in IT sector? Do an Online, Self Paced - Data Science course by Microsoft for Free. To register leave your email id in the comment box (10,436 registrations done in 10 days) /n/r #DataScience Workflow.\\r\\nRegister for Querying with Transact-SQL now!\\r\\nClick here : www.millionlights.co.in/registration /n/r #Testimonial #Millionlights #Datascienceorientation #Microsoft\\r\\nwww.millionlights.org /n/r \"Big Data: visi?n general\", Num?rica Resumiendo, No. 7\\r\\nhttps://goo.gl/cZMDY2  \\r\\n#BigData #NIID #NIIDResumiendo #DataScience #DataArtist #IoT /n/r I am searching for a freelance analytics engineer for an immediate project at DKV, the boat private health insurance company in Brussels, Belgium.\\r\\n\\r\\nThe freelancer needs to be a real crack in programming in Microsoft powerBI, and is very good in R and SQL.  Good analytical skills are considered a plus. \\r\\n\\r\\nProject should ideally stay next Monday and may take from 3 weeks to 3-5 months, depending on the chemistry with the team.\\r\\n\\r\\nOnly people who can start on minimally 2 days per week will be considered, and with a ramp up to 4-5 days a week with 4-6 weeks.\\r\\n\\r\\nPermanent job opening available too.\\r\\n\\r\\nIf you are interested then please PM your proof cases of experience and your daily rate /n/r Cluster analysis using SAS and R\\r\\nCluster analysis is the bread and butter of marketing. It has many aspects like \\r\\n\\x95 hierarchical clustering, \\r\\n\\x95 non-hierarchical cluster (k means clustering) and many lingo associated with the same like dendogram, scree plot, Euclidean distance, simple linkage, wards method etc. \\r\\n\\r\\nThe tutorial on clustering  is one of the easiest way to master these concepts. This explains cluster analysis using SAS along with various options of SAS, working of hierarchical and non-hierarchical algorithms, step by step explanation of SAS output, differentiation with objective segmentation techniques etc.\\r\\nIt also explains the \\r\\n\\x95 clustering using R\\r\\n\\x95 cluster analysis guideline for data mining scenario\\r\\n\\x95 quiz and assignment (along with model solution for assignment)\\r\\nlink \\x96 https://www.udemy.com/cluster-analysis-motivation-theory-practical-application/?couponCode=SP_CA_01 /n/r Don\\'t forget to like our Facebook page ?\\r\\n? https://www.facebook.com/CouthonConseil/\\r\\n\\r\\nJoins us on Twitter ?\\r\\n? https://www.twitter.com/CouthonConseil \\r\\n\\r\\nFollow us on LinkedIn ?\\r\\n? https://www.linkedin.com/m/company/10107092/ /n/r Don\\'t forget to like our Facebook page ?\\r\\n? https://www.facebook.com/CouthonConseil/\\r\\n\\r\\nJoins us on Twitter ?\\r\\n? https://www.twitter.com/CouthonConseil \\r\\n\\r\\nFollow us on LinkedIn ?\\r\\n? https://www.linkedin.com/m/company/10107092/ /n/r Ultimate Course on Principal Component and Factor Analysis using SAS and R\\r\\nThe course explains one of the important aspect of machine learning - Principal component analysis and factor analysis in a very easy to understand manner. It explains theory as well as demonstrates how to use SAS and R for the purpose. \\r\\n\\r\\nThe course provides entire course content available to download in PDF format, data set and code files. The detail course content is as follows.\\r\\n\\r\\nIntuitive Understanding of PCA 2D Case\\r\\n1.what is the variance in the data in different dimensions?\\r\\n2. what is principal component?\\r\\nFormal definition of PCs\\r\\n1. Understand the formal definition of PCA\\r\\n2. Properties of Principal Components\\r\\n3. Understanding principal component analysis (PCA) definition using a 3D image\\r\\nProperties of Principal Components\\r\\n1. Summarize PCA concepts\\r\\n2. Understand why first eigen value is bigger than second, second is bigger than third and so on\\r\\nData Treatment for conducting PCA\\r\\n1. How to treat ordinal variables?\\r\\n2. How to treat numeric variables?\\r\\nConduct PCA using SAS: Understand\\r\\n1. Correlation Matrix\\r\\n2. Eigen value table\\r\\n3. Scree plot\\r\\nHow many pricipal components one should keep?\\r\\nHow is principal components getting derived?\\r\\n1. Conduct PCA using R\\r\\nIntroduction to Factor Analysis\\r\\n1. Introduction to factor analysis\\r\\n2. Factor analysis vs PCA side by side\\r\\n3. Factor Analysis Using R\\r\\n4. Factor Analysis Using SAS\\r\\nTheory for using PCA for Variable Selection\\r\\nDemo of using PCA for Variable Selection\\r\\nhttps://www.udemy.com/principal-component-analysis-pca-and-factor-analysis/ /n/r SAS Programming from scratch\\r\\nLearn SAS programming from industrial usage perspective. SAS is one of the most used tool for data science, analytics and statistical analysis domain. This course is designed to give you good start on SAS programming quickly. The course has following highlights\\r\\n\\x95 Getting free access to SAS\\r\\n\\x95 Importing data\\r\\n\\x95 Getting basic feel of data- through contents, print, freq, univariate \\r\\n\\x95 Data operations like merge, append, sort, truncate \\x96 rows / columns wise, \\r\\n\\x95 derive new fields, \\r\\n\\x95 analysis for each class \\r\\n\\x95 Graphs \\x96 vertical bar chart / pie charts / stacked charts \\r\\n\\x95 Statistical procedure\\r\\n\\x95 Title and labels for enhancements\\r\\n\\x95 Tabulate for pivot table kind of work\\r\\nhttps://www.udemy.com/sas-programming-made-easy-with-examples/?couponCode=SP_ST_1 /n/r An overview of the most popular anomaly detection algorithms for time series and their pros and cons. /n/r Ultimate Course on Basic Statistics using Excel-\\r\\nStatistics has been probably one of the most difficult course to master across the globe for MBA students. It is all because it is being explained in a wrong way. Across the globe most of the faulty are not been able to simplify it for students, even if they know it in great detail. \\r\\nThe right way of learning statistics is when one can see the demonstration of concepts through simulation of a concept and then go through the theoretical concept. The tutorial Statistics by example explains statistics by simulation. The tutorial covers\\r\\n\\x95 Probability and expectations\\r\\n\\x95 Central tendencies and dispersion\\r\\n\\x95 Central limit theorem\\r\\n\\x95 Sampling distribution \\r\\n\\x95 Hypothesis testing \\r\\n\\x95 Linear regression\\r\\n\\x95 Categorical data analysis and chi square test of independence\\r\\n\\x95 ANOVA\\r\\n\\x95 Non parametric tests\\r\\nlink \\x96  https://www.udemy.com/statistics-by-example/?couponCode=SP_STS_01 /n/r Want to get started with SAS?\\r\\nRead our recent Article \\'Things Which You Should Be Familiar About SAS Before You Gather Knowledge Regarding the Software\\'. http://bit.ly/2sw1K7w\\r\\n\\r\\n#SAS #OnlineProfessionalCertificationCourse #SASCertificationCourse #BigDataAndAnalyticsCertificationCourse /n/r Artificial Neural Network made super easy https://www.udemy.com/artificial-neural-networks-tutorial-theory-applications/?couponCode=SP_01_ANN /n/r Ultimate Course on Logistic Regression -\\r\\nPredictive analytics : Credit scoring - logistic regression using SAS\\r\\nLogistic regression is one of the most used technique in the industry for credit scoring. The technique is applicable in predicting risk (who will default), response (who will take offer), collectability (who will make payment of delinquent balance) etc. SAS is a global leader for developing predictive models. \\r\\nThe tutorial on credit scoring is one of the most authentic source to learn logistic regression modelling (credit scoring) using SAS for industrial application. It has helped many students to master the subjects. Many students testimony are available for viewing.\\r\\nThis explains \\r\\n\\x95 econometrics / statistical theory, \\r\\n\\x95 SAS program detail, \\r\\n\\x95 step by step data workout and \\r\\n\\x95 deep dive into SAS output. \\r\\nThe tutorial is quite complete in the sense that it explains every step by model building like model design, data audit guideline, variable selection (categorical and numeric), multi collinearity removal, final model development, power of a model (KS / GINI / concordance) as well as reject inference\\r\\nlink \\x96 https://www.udemy.com/logistic-regression-credit-scoring-modelling-using-sas/?couponCode=SP_LRS_01 /n/r Get ready to Boost your career in Big Data with Hive. \\r\\nhttps://www.youtube.com/watch?v=9nCjBqhKZSI\\r\\n\\r\\nThis tutorial explains how Hive can be used for Big Data processing and is a part of series of video-based e-learning content offered by SchoalrsPro to learn various Big Data technologies. \\r\\nTo know more about Big Data Bundle self-paced e-learning, write into us at info@scholarspro.com.\\r\\n\\r\\n#Hive #BigData #HiveForBigDataProcessing #BigDataAndAnalyticsCertificationCourses #OnlineProfessionalCertificationCourses /n/r Predictive Analytics : Classification and regression tree (decision tree)  using R\\r\\nDecision tree is one of the easiest technique of machine learning that yields tremendous benefit to the business. It has several aspects like \\r\\n\\x95 how to develop the decision tree, \\r\\n\\x95 how to interpret the output, \\r\\n\\x95 How to design data for modeling?\\r\\n\\x95 algorithm to understand how it\\'s working, \\r\\n\\x95 How to understand it\\'s business benefit?\\r\\n\\x95 Assignment, quiz to fortify your learning\\r\\nThe tutorial explains how to develop decision tree using R, what is GINI / Entropy / CART / CHAID / random forest method etc. Students across the globe has found it to be complete tutorial for learning decision tree \\r\\nlink \\x96 https://www.udemy.com/decision-tree-theory-application-and-modeling-using-r/?couponCode=SP_DT_01 /n/r It\\'s time to uplift your career and get certified in the most leading career skill.\\r\\nRead our recent blog \"Evolve Out By Enhancing Yourself With SAS Certification\" at http://bit.ly/2rwY5Jv.\\r\\n\\r\\n#SAS #SASCertification #OnlineProfessionalCertificationCourses /n/r Logistic Regression using R (workshop)\\r\\nLogistic regression is one of the most used technique in the industry for credit scoring. The technique is applicable in predicting risk (who will default), response (who will take offer), collectability (who will make payment of delinquent balance) etc. \\r\\nThe workshop tutorial doesn\\'t have details on theory. It has \\r\\n\\r\\n\\x95 R program detail, \\r\\n\\x95 step by step data workout and \\r\\n\\x95 deep dive into R output. \\r\\nThe tutorial is for someone who quickly wants to develop credit scoring models using R. It gives the syntax as well as details of interpretation. \\r\\nLink - https://www.udemy.com/logistic-regression-workshop-using-r-step-by-step-modeling/?couponCode=SP_LRR_01 /n/r Looking for SAS Online Training?\\r\\nThings You Have A Duty To Know About SAS Online Training.. http://bit.ly/2qV4ona\\r\\n\\r\\n#SAS #SASOnlineTraining #OnlineProfessionalCertificationCourses #BigDataAndAnalyticsCertificationCourses /n/r A very prestigious worldwide credential is the SAS Certified Advanced Programmer. SAS Base Programming Exam and SAS Advanced Programming Exam are the two exams which a person needs to pass out to get this certificate.\\r\\nFind out the Way of Successfully Obtaining Good Results in SAS Certification Exam..\\r\\nhttps://scholarspro.tumblr.com/post/160911147369/the-way-of-successfully-obtaining-good-results-in?is_highlighted_post=1\\r\\n\\r\\n#SASCertification #SAS #AdvancedSAS #BaseSAS #SASCertifiedAdvancedProgrammer #SASCertificationExam #OnlineProfessionalCertificationCourses /n/r A study revealed that 83% of career professionals accounted that receiving an SAS certification helped them get a promotion, and 66% reported earning an SAS certification assisted them to get a hike in salary.\\r\\n\\r\\nDo you know SAS Certified Professionals Are Considering The Analytics? http://bit.ly/2qfNuxv.\\r\\n\\r\\n#SAS #SASCertifiedProfessionals #Analytics /n/r Do you know Big Data usually plays a big role in Uplifting Total Enactments Of A Company? \\r\\nhttp://bit.ly/2qOW20e\\r\\n\\r\\n#bigdata /n/r Don\\'t forget to like our Facebook page ?\\r\\n? https://www.facebook.com/CouthonConseil/\\r\\n\\r\\nJoins us on Twitter ?\\r\\n? https://www.twitter.com/CouthonConseil \\r\\n\\r\\nFollow us on LinkedIn ?\\r\\n? https://www.linkedin.com/m/company/10107092/ /n/r Don\\'t forget to like our Facebook page ?\\r\\n? https://www.facebook.com/CouthonConseil/\\r\\n\\r\\nJoins us on Twitter ?\\r\\n? https://www.twitter.com/CouthonConseil \\r\\n\\r\\nFollow us on LinkedIn ?\\r\\n? https://www.linkedin.com/m/company/10107092/ /n/r Shape your data with #LogPacker, smart service for log collection and analysis. Get the idea of the #startup at https://www.producthunt.com/ /n/r Cluster analysis using SAS and R\\r\\nCluster analysis is the bread and butter of marketing. It has many aspects like \\r\\n\\x95 hierarchical clustering, \\r\\n\\x95 non-hierarchical cluster (k means clustering) and many lingo associated with the same like dendogram, scree plot, Euclidean distance, simple linkage, wards method etc. \\r\\n\\r\\nThe tutorial on clustering  is one of the easiest way to master these concepts. This explains cluster analysis using SAS along with various options of SAS, working of hierarchical and non-hierarchical algorithms, step by step explanation of SAS output, differentiation with objective segmentation techniques etc.\\r\\nIt also explains the \\r\\n\\x95 clustering using R\\r\\n\\x95 cluster analysis guideline for data mining scenario\\r\\n\\x95 quiz and assignment (along with model solution for assignment)\\r\\nlink \\x96 https://www.udemy.com/cluster-analysis-motivation-theory-practical-application/?couponCode=SP_CA_01 /n/r Ultimate Course in Basic Statistics\\r\\nStatistics has been probably one of the most difficult course to master across the globe for MBA students. It is all because it is being explained in a wrong way. Across the globe most of the faulty are not been able to simplify it for students, even if they know it in great detail. \\r\\nThe right way of learning statistics is when one can see the demonstration of concepts through simulation of a concept and then go through the theoretical concept. The tutorial Statistics by example explains statistics by simulation. The tutorial covers\\r\\n\\x95 Probability and expectations\\r\\n\\x95 Central tendencies and dispersion\\r\\n\\x95 Central limit theorem\\r\\n\\x95 Sampling distribution \\r\\n\\x95 Hypothesis testing \\r\\n\\x95 Linear regression\\r\\n\\x95 Categorical data analysis and chi square test of independence\\r\\n\\x95 ANOVA\\r\\n\\x95 Non parametric tests\\r\\nlink \\x96  https://www.udemy.com/statistics-by-example/?couponCode=SP_STS_01 /n/r What do you wanna learn? Just send your query at info@uplatz.com or call us at +44 7836212635 or visit our page to know more https://www.uplatz.com. /n/r Don\\'t forget to like our Facebook page ?\\r\\n? https://www.facebook.com/CouthonConseil/\\r\\n\\r\\nJoins us on Twitter ?\\r\\n? https://www.twitter.com/CouthonConseil \\r\\n\\r\\nFollow us on LinkedIn ?\\r\\n? https://www.linkedin.com/m/company/10107092/ /n/r Predictive Analytics : Classification and regression tree (decision tree)  using R\\r\\nDecision tree is one of the easiest technique of machine learning that yields tremendous benefit to the business. It has several aspects like \\r\\n\\x95 how to develop the decision tree, \\r\\n\\x95 how to interpret the output, \\r\\n\\x95 How to design data for modeling?\\r\\n\\x95 algorithm to understand how it\\'s working, \\r\\n\\x95 How to understand it\\'s business benefit?\\r\\n\\x95 Assignment, quiz to fortify your learning\\r\\nThe tutorial explains how to develop decision tree using R, what is GINI / Entropy / CART / CHAID / random forest method etc. Students across the globe has found it to be complete tutorial for learning decision tree \\r\\nlink \\x96 https://www.udemy.com/decision-tree-theory-application-and-modeling-using-r/?couponCode=SP_DT_01 /n/r Don\\'t forget to like our Facebook page ?\\r\\n? https://www.facebook.com/CouthonConseil/\\r\\n\\r\\nJoins us on Twitter ?\\r\\n? https://www.twitter.com/CouthonConseil /n/r Where is a #Career in #Hadoop headed? Hurry! #Promotional #Offer - Use Code AP1712JB10 and Get 10% #Discount http://pos.li/Db JanBasktraining /n/r Hello All,\\r\\nDo you know any expert who can teach on Microsoft Excel reporting and Analytics (Little bit of Statistics background is added advantage) at a nominal cost during weekends. . . someone living close to Frazer town/Banasawadi vicinity.\\r\\nThank you for your attention to this request.\\r\\nImportant note: Send only personal messages with contact details, address and landmark.\\r\\nThnx /n/r Check your basics in #bigdata with this quick free practice test: http://bit.ly/2n9SpQi /n/r Looking for Sr Data Scientist with  2-3 Yrs  Exp skills of  Machine learning , Python , NLTK , Open NLP , R (Optional), Weka (Optional) ,  Hadoop , Spark , Data mining (Mandate), Text mining (Mandate) , Textual Anaysis (Mandate), should be strong in Algo and DS, \\r\\n\\r\\nLocation : Bangalore\\r\\n\\r\\nMail resume to veena.r@practical-methods.com /n/r Grab the chance because we have lift off! https://www.facebook.com/events/283944272064088/\\r\\n#Janbask #StudyAtJanbask #Learn #Win #Launch #Website /n/r I\\'m looking for collaborators to help me with regular data science/journalism projects around a dataset I own. \\r\\n\\r\\nI\\'m the founder of Sapio (GetSapio.com), a dating app that I launched about 6 months ago. Our Sapio users answer open-ended questions, and have so far answered nearly 600,000 times. These answers have user metadata attached (age/gender/race/orientation/location/etc.) but are otherwise anonymized. I\\'m looking to partner with someone who would be interested in working with me on telling interesting stories using our data. I have a small budget, so I\\'m not looking for something for free, but It isnt much and was hoping to find someone who would be interested in exclusive access to this data to help me tell some really compelling stories that couldnt otherwise be told. If any of you are interested, please email me at Kristin@frac.tl   Thanks! /n/r The tutorial below helps you learn R programming with comfort by working on data. It explains all the concepts through data workout rather than explaining you by the methods of lingo like vector, metrics etc. It is being like by students as this is quite concise and gives you confidence that you can easily learn it.\\r\\nlink \\x96 https://www.udemy.com/introduction-to-r-programming-learn-r-syntax-by-example/?couponCode=SP_RP_01\\r\\n /n/r Excel though used by many folks but rarely people know that one can do pair T test, ANOVA, linear optimization, Linear regression, Pareto Analysis etc. using Excel. \\r\\nMost of the tutorial on advance excel across the globe just explains pivot table. \\r\\nThe tutorial on advance analysis using Excel explain data analysis toolpak, solver and goal seek. It is quite concise yet sufficiently good to help one master application of Microsoft Excel for advance analytics. This enhances one\\'s ability to use excel for many statistical analysis and optimization purpose. \\r\\nlink \\x96 https://www.udemy.com/data-analysis-statistical-optimization-using-ms-excel/?couponCode=SP_ADEX_01\\r\\n /n/r Cluster analysis using SAS and R\\r\\nCluster analysis is the bread and butter of marketing. It has many aspects like \\r\\n\\x95\\thierarchical clustering, \\r\\n\\x95\\tnon-hierarchical cluster (k means clustering) and many lingo associated with the same like dendogram, scree plot, Euclidean distance, simple linkage, wards method etc. \\r\\n\\r\\nThe tutorial on clustering  is one of the easiest way to master these concepts. This explains cluster analysis using SAS along with various options of SAS, working of hierarchical and non-hierarchical algorithms, step by step explanation of SAS output, differentiation with objective segmentation techniques etc.\\r\\nIt also explains the \\r\\n\\x95\\tclustering using R\\r\\n\\x95\\tcluster analysis guideline for data mining scenario\\r\\n\\x95\\tquiz and assignment (along with model solution for assignment)\\r\\nlink \\x96 https://www.udemy.com/cluster-analysis-motivation-theory-practical-application/?couponCode=SP_CA_01\\r\\n /n/r This is an open invitation for everyone looking to specialise in the booming fields of Internet-of-Things, Digital Security, Mobile Computing Systems, and/or Data Science. Join this webinar and take the opportunity here: https://goo.gl/eixZ37 /n/r Learn #Bigdata and #Hadoop..!!\\r\\n\\r\\nAttend the free live webinar on 18th March and learn Bigdata & Hadoop.\\r\\nWebinar\\'s Timing: \\r\\n07:00 AM PST \\r\\n07:30 PM IST\\r\\nREGISTER now at https://goo.gl/atY0yJ /n/r Can i have a solution? /n/r Check your basics in #bigdata with this quick free practice test: http://bit.ly/2n9SpQi /n/r Learn #Bigdata and #Hadoop..!!\\r\\n\\r\\nAttend the demo session for free by our industry expert Prashant S.R and take a closer look in the world of Hadoop and Bigdata. REGISTER now at https://goo.gl/jPRHbz /n/r #InternationalWomensDay #IWD2017 /n/r Looking for an Affordable Master in Artificial Intelligence and Computer Science in Europe? Join Online Open Day and learn everything about this unique program at the West University of Timisoara ! Save your place here: https://goo.gl/iQu2o3 /n/r Free Live #Webinar...!!\\r\\nLearn #Bigdata and #Hadoop from an Industry Expert. Attend the first session for free. Register now at https://goo.gl/aBHz3x /n/r Big Data and Wrestling? WWE Uses Data to Guide Success www.linkedin.com/hp/update/6241942955503034369 /n/r What is the best laptop for a beginner in data science practice? :) /n/r SCHOLARSHIPS AVAILABLE! Study Data and Computer Science with a Scholarship in Italy! Join our Online Open Day and find out everything about this 2 year programme from the University of Genoa!  Get your scholarship here: https://goo.gl/HGkzfp /n/r Big Data: Why NASA Can Now Visualize Its Lessons Learned www.linkedin.com/hp/update/6240114050919952384/ /n/r Shri A.K.Mittal, Dept. of Comm. & IT (GoI)\\r\\nHere he talks about IoT and M2M\\r\\nAnd using it to transform Supply Chain Logistics\\r\\nWatch complete talk: https://goo.gl/RufBq2 /n/r Big news! We have released LogPacker Unity Plugin that collects logs in one centralized dashboard for you. https://logpacker.com/registration?key1=fb /n/r Get hands on experience with first Project based Hadoop training! Attend the first session for free! Click here - https://www.dezyre.com/free-webinar/project-based-hadoop-training/120 /n/r Supply Chain in business using IoT:\\r\\nMr. Bhabajit Nandi: Supply Chain Security Manager, Microsoft Devices Group \\r\\nCol. Ashwani Sindhwani: National Security Manager, DHL Express India\\r\\nFind out what they have to say: https://goo.gl/L0dwCw /n/r We let you acquire the business intelligence through company profiling. Check out our work. #Thefinansol #companyprofile #microsoft #windows /n/r Are you looking for practice questions for #Bigdata? Here we have it for you for #FREE.  http://bit.ly/2kscHFM /n/r Hey guys! For those of you who want to meet talented, like-minded people and build innovative solutions over the weekend in London, we\\'re hosting a 48-hour hackathon for Department for Transport from the 24th to the 26th of March! \\r\\n\\r\\nWe really would love for data scientists to attend as our lead of contact for Department for Transport is a data scientist with his own Data Science meetups so he has his favorites. :P\\r\\n\\r\\nWe\\'ll have loads of datasets and challenges for you guys to play with for sure! \\r\\n\\r\\nIf you\\'re interested in the event, just register at register.dfthacks.com!\\r\\n\\r\\nHave a good day :)\\r\\n\\r\\nhttps://www.facebook.com/events/240781796368113/ /n/r Digital Health Conference, London 2017 - Call For Papers Deadline EXTENDED to February 20th!\\r\\nwww.acm-digitalhealth.org\\r\\nIn partnership with University College London, IRDR /n/r Call For Papers Deadline February 13th!\\r\\nDigital Health Conference with special focus on Big Data and Digital Health in Disaster, Emergency and Humanitarian Contexts, London July 2017\\r\\nwww.acm-digitalhealth.org\\r\\nIn partnership with University College London, IRDR /n/r Hello Dear Members, i am currently a student from Frankfurt University of Applied Science have the task to find out why specially the 21-Must-Know Data Science interview Questions and Answers are important for a data mining interview.\\r\\n\\r\\nI have edit this question \"Q15. Explain Edward Tufte\\'s concept of \"chart junk.\" and understood it. \\r\\nDoes anyone have an idea why this question could be important or why Greory Platsky have choose this Question ? I will be very thanksfull for your ideas and informations. /n/r Looking for a #DataScience training with an Industry expert? Join the Live #Online training and get trained by the best trainer. Click here to join the first session for #free - https://www.dezyre.com/free-webinar/attending-the-first-session-of-data-science-for-free-/119 /n/r Hello guys,\\r\\nI have a question regarding using PCA sklearn.\\r\\n-------------------------------------------------------\\r\\nif I have a (n, 512) matrix \\r\\nwhere (n) is a number of images, and each 512 is a feature vector\\r\\nwhat happens if I do the following?\\r\\n PCA = PCA(512 ,whiten=True)\\r\\n pca.fit(feats)\\r\\n\\r\\nI see examples to define PCA with 2 components to see the variance in the points such in this figure.\\r\\nbut why to define 512 components? what is the point to do it this way? /n/r --- Posting on behalf of Friend ---\\r\\nHello Sir/Madam,\\r\\nDo you know any Data analyst/ Data Scientist who are willing to teach Fundamentals of Statistics and Data Analytics using Advanced Excel or R/ Power BI? \\r\\n+ If not, do you know anyone who can coach 1st PUC and 2nd PUC Statistics subject.\\r\\nPreferred location in and around Frazer town or Banasawadi surroundings.\\r\\n---NO INSTITUTE REFERENCES PLEASE---\\r\\nTIA /n/r --- Posting on behalf of Friend ---\\r\\nHello Sir/Madam,\\r\\nDo you know any Data analyst/ Data Scientist who are willing to teach Fundamentals of Statistics and Data Analytics using Advanced Excel or R/ Power BI? \\r\\n+ If not, do you know anyone who can coach 1st PUC and 2nd PUC Statistics subject.\\r\\nPreferred location in and around Frazer town or Banasawadi surroundings.\\r\\n---NO INSTITUTE REFERENCES PLEASE---\\r\\nTIA /n/r Free #Live Webinar..!!!\\r\\n\\r\\nJoin the webinar and discover everything about #DataScience. \\r\\n\\r\\nWebinar\\'s Date & Time: Jan-28-2017 at 07:00 AM PST \\r\\n\\r\\nRegister now at https://goo.gl/PpxRa6 /n/r \"Data is the new science. #BigData holds the answers.\" \\x96 Pat Gelsinger\\r\\n\\r\\nEnroll now to learn the science of #data. Register yourself at https://goo.gl/Z4XVW1\\r\\n\\r\\nP.S. - Attend the first session for #free.. Hurry up..!!! /n/r #Live Webinar on #Hadoop\\r\\nAttend the first two live sessions for #Free! \\r\\nREGISTER now at https://goo.gl/MM7AG7 /n/r - SEATTLE DATA SCIENTISTS - \\r\\nIf there are any Apache Spark enthusiasts here, Galvanize now gives you 50% off Intro to Spark for DS weekend workshop. ? ? \\r\\nCheck it out or share with a friend.\\r\\nThanks! :) /n/r Live #Hadoop Project based #training. Attend the first session for free. REGISTER now at https://goo.gl/NnRLlu /n/r #Hackerday - Live Online session\\r\\n\\r\\nWork on an online interactive #hadoop project. Register for this free live #webinar now. Webinar\\'s Date & Time: Jan-14-2017 at 07:00 AM (PST), Jan-14-2017 at 08:30 PM (IST)\\r\\n\\r\\nRegistration link : https://goo.gl/kccRjJ /n/r plz statistics pg project title tell me no /n/r We invite you to participate in the 3rd international conference \"Interdisciplinary Cultural Group Research: Youth Subcultures, Worldviews and Lifestyles\" . Furthermore, members of subcultures express themselves and communicate with each other not only in the \"real\" spaces such as streets, bars, concerts and festivals but also in \"virtual\" space: social networks (like Facebook, Twitter, Instagram and others), blogs, websites and etc.Therefore, this conference aims to bring together scholars from diverse disciplines to share their achievements in applying not only traditional research methods but also computer tools in their work.\\r\\nhttps://www.facebook.com/events/1254715614612506/ /n/r Keep up with the big data trends: http://buff.ly/2iuS2gc /n/r Anyone from the EU here is interested in job in Germany? Big Data, Hadoop /n/r I am TCS - HR, Join to my group and know recent recruitment news as well as carrier enhancement info on regular basis. Request you to share this link in your friend circle.\\r\\nLink - https://www.facebook.com/groups/1866862883544761/ /n/r Dai Viet Group is recruting :\\r\\nJob title : Data mining analyst description \\r\\nJob description\\r\\n- Provide expertise in data analytics, data cluster and data classification;\\r\\n- Collect data from different sources and create datasets and databases;\\r\\n- Research and build models for data mining, text mining, recommendation system and machine learning;\\r\\n- Establish and optimize data structure;\\r\\nRequirements\\r\\n- Minimum 1 year experience in data mining, statistic programing and modeling at large scales;\\r\\n- Being proficient in any of the following languages: R, Python, Java, C/C++, Scala.\\r\\n- Solid background in recommendation algorithms, experience in machine learning and text mining\\r\\n- Proven experience in large scale data analytics;\\r\\n- Fluent English, TOEIC above 600 (or equivalent);\\r\\n- Background in Computer Science, Statistics, Mathematics or a related Quantitative Discipline.\\r\\n- \\r\\nPreference\\r\\n- Master Degree in Computer Science, Applied Statistics, Mathematics\\r\\n- Experience using language processing systems, such as Word segmentation, Automatic Summarization, POS tagging, NER, Topic Modeling, Sentiment analysis\\x85\\r\\n- Exposure to Search, Analytics base\\r\\nExperience working with systems such as Hadoop eco system, Spark, Cassandra, Neo4j\\x85\\r\\nLet contact via 0902.360.586 (Ms.Th??ng) to know more information about this position. /n/r #WEBINAR ON #DATA SCIENCE\\r\\nAttend the first session for #free on #DataScience by our expert Pradeepta Mishra. REGISTER now at https://goo.gl/a1yjYT /n/r Job Description\\r\\n\\r\\nSkill sets : Solid knowledge of several modeling and learning techniques including Statistical predictive and prescriptive analytics, Optimization, Machine learning, Data mining, AI, Simulation, Social network analysis, Churn prediction, Segmentation analysis, Text analysis and Streaming data analytics. Being able to work with big data using tools such as Python.\\r\\nResponsibilities : Hands on responsibility as part of the team engaged in data analytics and solving critical business problems using data. Build demonstrable high performing models which can be taken to larger audience within the organization.\\r\\nPersonal Attributes : Leadership Capability, Self-motivated, Exceptional analytical and problem solving skills and ability to present convincingly to technical and non-technical audience.\\r\\n\\r\\nSkills/competencies required\\r\\n\\r\\nMust  have a deep understanding of  Python , Excel , SQL queries, Big data, (MATLAB, SPSS or SAS - advantage)  \\r\\nStrong English,reading, writing and speaking.\\r\\nMust have a masters degree in operations research, applied statistics, machine learning or a related quantitative discipline\\r\\nMust have a deep understanding of statistical and predictive modeling concepts, machine-learning approaches, clustering and classification techniques, and recommendation and optimization algorithms.\\r\\n3 years of experience delivering world-class data science outcomes.\\r\\nHave a keen desire to solve business problems, and live to find patterns and insights within structured and unstructured data\\r\\nAble to propose analytics strategies and solutions that challenge and expand the thinking of everyone around you\\r\\n\\r\\n**please mention your salary expectations /n/r \"Combinations of data\" \\x96 that\\'s a thing now? Get ahead of the top BI trends of 2017 on Jan 19: http://bit.ly/2hsECQL /n/r I am TCS - HR, Join to my group and know recent recruitment news as well as carrier enhancement info on regular basis. Request you to share this link in your friend circle.\\r\\nLink - https://www.facebook.com/groups/1866862883544761/ /n/r I\\'m waiting for an AMD manager approval so I can send an offer letter to neural network architect /n/r Manage your purchase #orders more efficiently and find new ways to #save and handle procurement http://bit.ly/2dr7kW2 /n/r Learn Job Oriented  Data Science Training from real-time expects with 100% job assurance.... https://goo.gl/dqqwO4 \\r\\n\\r\\nEnroll now. Special Offer Begins... Hurry UP /n/r Operate with #speed, #agility and responsiveness with a complete #retail #software suite http://bit.ly/29F5YW6 /n/r Are you tired of managing your #machinery instead of your #business? http://bit.ly/2atkCOF /n/r Reasons to adopt a #pos #software http://bit.ly/29F5YW6 /n/r Multi-channel #retail #ERP solution starts here http://bit.ly/29F5YW6 /n/r Looking for analytics insights for your sales teams? Join Lenovo and Qlik webinar on Dec 14 to see how scaling self-service analytics impacts sales decision-making http://bit.ly/2h8onfK /n/r For Beginners How Easy to Learn #Big #Data & #Hadoop #Training\\r\\n\\r\\nvisit @ https://goo.gl/CcPyXF /n/r Say goodbye to the headaches of maintaining #on-premise #ERP http://bit.ly/29q4EE1 #GrowthHacking /n/r The only #Business #Intelligence #Software embedded in #ERP http://bit.ly/2bSSnGl /n/r Asia\\'s leading get-together of big data & analytics professionals across industries. Confirmed speakers are from organisations such as DBS Bank, JLL, Credit Suisse, Spotify, GSK, A*STAR, Unilever, BP & many others. /n/r #TuesdayTips #SocialMedia #UserComments #SocialInteraction http://bit.ly/29q4EE1 /n/r Do you really want to know about data science and big data? \\r\\nSave your date and visit http://datascienceweekend.id/\\r\\n\\r\\nDon\\'t miss it!!\\r\\nSo many data scientist ??? /n/r My son is seeking an entry level Data Analytics position. His resume is attached. He would prefer to work in Florida or the Southeastern US. Does anybody know of entry level positions available? /n/r Don\\'t overlook the need for #eCommerce #ERP Integration http://bit.ly/29F5YW6 /n/r The only #software that integrates and supports #complex processes in #real-time in the #logistics industry http://bit.ly/2gJjb21 /n/r Wish to learn Big Data & Hadoop? Hurry!\\r\\n\\r\\nCheckout offers specially crafted for you. New batches starting soon. The course curriculum is in line with industry requirements & curated by industry experts themselves!\\r\\nEnroll for the live & interactive online course today from industry experts.\\r\\ninfo@itobj.com\\r\\n408-63-ITOBJ\\r\\n:) :) :D :D /n/r Just checking if the group members would be interested in the following data projects for auction: \\r\\n\\r\\nPROJECT LIST :\\r\\n1. SF Bay Area Homelessness - Project #2\\r\\n2. Health Record and Sequenced Genome Correlation  Analysis\\r\\n3. Care in Old Age - Long Term Care Insurance Keep or Drop?\\r\\n4. San Francisco Real Estate Value Forecasting\\r\\n5. SF Bay Area Homelessness - Project 3: Analysis strategies to evaluate point in time homeless counts and homeless projects\\r\\n\\r\\nWe are actually on an early launch of a Big Data Marketplace based in San Francisco. These projects are not a recruitment for an office-based job, but rather for a community-based collaboration projects. \\r\\n\\r\\nYou can check the project details here: https://goo.gl/jKTK1a /n/r Great opportunity for you to change your domain to #BigData the Booming Technology to boost your career.\\r\\nDSRI has launched weekend classes for Big Data Analytics.\\r\\nWe train from scratch to the advance technology. \\r\\nGet a chance to work on live projects use cases with Industrial experts.\\r\\n*******Limited Seat\\r\\nTraining on : Hadoop, MapReduce, Hive, Pig, Sqoop, Flume, Hbase, RHadoop, Data Analytics and Data Modeling Using R Programming.\\r\\nEach topic is covered with Real-time use cases\\r\\nFor more Details Please give us call mob:9620622125/9620622390/9538093274\\r\\nEmail-id: info@dsresearch.in, admissions@dsresearch.in\\r\\nVenue Details:\\r\\n#14, 2nd floor Srinidhi Building( Above Apollo Pharmacy)\\r\\nJambu Savari Dinne /n/r Great opportunity for you to change your domain to #BigData the Booming Technology to boost your career.\\r\\nDSRI has launched weekend classes for Big Data Analytics.\\r\\nWe train from scratch to the advance technology. \\r\\nGet a chance to work on live projects use cases with Industrial experts.\\r\\n*******Limited Seat\\r\\nTraining on : Hadoop, MapReduce, Hive, Pig, Sqoop, Flume, Hbase, RHadoop, Data Analytics and Data Modeling Using R Programming.\\r\\nEach topic is covered with Real-time use cases\\r\\nFor more Details Please give us call mob:9620622125/9620622390/9538093274\\r\\nEmail-id: info@dsresearch.in, admissions@dsresearch.in\\r\\nVenue Details:\\r\\n#14, 2nd floor Srinidhi Building( Above Apollo Pharmacy)\\r\\nJambu Savari Dinne Circle, JP Nagar 8th Phase\\r\\nBangalore-560076 /n/r Great opportunity for you to change your domain to #BigData the Booming Technology to boost your career.\\r\\nDSRI has launched weekend classes for Big Data Analytics.\\r\\nWe train from scratch to the advance technology. \\r\\nGet a chance to work on live projects use cases with Industrial experts.\\r\\n*******Limited Seat\\r\\nTraining on : Hadoop, MapReduce, Hive, Pig, Sqoop, Flume, Hbase, RHadoop, Data Analytics and Data Modeling Using R Programming.\\r\\nEach topic is covered with Real-time use cases\\r\\nFor more Details Please give us call mob:9620622125/9620622390/9538093274\\r\\nEmail-id: info@dsresearch.in, admissions@dsresearch.in\\r\\nVenue Details:\\r\\n#14, 2nd floor Srinidhi Building( Above Apollo Pharmacy)\\r\\nJambu Savari Dinne Circle, JP Nagar 8th Phase\\r\\nBangalore-560076 /n/r Great opportunity for you to change your domain to #BigData the Booming Technology to boost your career.\\r\\nDSRI has launched weekend classes for Big Data Analytics.\\r\\nWe train from scratch to the advance technology. \\r\\nGet a chance to work on live projects use cases with Industrial experts.\\r\\n*******Limited Seat\\r\\nTraining on : Hadoop, MapReduce, Hive, Pig, Sqoop, Flume, Hbase, RHadoop, Data Analytics and Data Modeling Using R Programming.\\r\\nEach topic is covered with Real-time use cases\\r\\nFor more Details Please give us call mob:9620622125/9620622390/9538093274\\r\\nEmail-id: info@dsresearch.in, admissions@dsresearch.in\\r\\nVenue Details:\\r\\n#14, 2nd floor Srinidhi Building( Above Apollo Pharmacy)\\r\\nJambu Savari Dinne Circle, JP Nagar 8th Phase\\r\\nBangalore-560076 /n/r Great opportunity for you to change your domain to #BigData the Booming Technology to boost your career.\\r\\nDSRI has launched weekend classes for Big Data Analytics.\\r\\nWe train from scratch to the advance technology. \\r\\nGet a chance to work on live projects use cases with Industrial experts.\\r\\n*******Limited Seat\\r\\nTraining on : Hadoop, MapReduce, Hive, Pig, Sqoop, Flume, Hbase, RHadoop, Data Analytics and Data Modeling Using R Programming.\\r\\nEach topic is covered with Real-time use cases\\r\\nFor more Details Please give us call mob:9620622125/9620622390/9538093274\\r\\nEmail-id: info@dsresearch.in, admissions@dsresearch.in\\r\\nwebsite: www.dsresearch.in\\r\\nVenue Details:\\r\\n#14, 2nd floor Srinidhi Building( Above Apollo Pharmacy)\\r\\nJambu Savari Dinne Circle, JP Nagar 8th Phase\\r\\nBangalore-560076 /n/r Great opportunity for you to change your domain to #BigData the Booming Technology to boost your career.\\r\\nDSRI has launched weekend classes for Big Data Analytics.\\r\\nWe train from scratch to the advance technology. \\r\\nGet a chance to work on live projects use cases with Industrial experts.\\r\\n*******Limited Seat\\r\\nTraining on : Hadoop, MapReduce, Hive, Pig, Sqoop, Flume, Hbase, RHadoop, Data Analytics and Data Modeling Using R Programming.\\r\\nEach topic is covered with Real-time use cases\\r\\nFor more Details Please give us call mob:9620622125/9620622390/9538093274\\r\\nEmail-id: info@dsresearch.in, admissions@dsresearch.in\\r\\nVenue Details:\\r\\n#14, 2nd floor Srinidhi Building( Above Apollo Pharmacy)\\r\\nJambu Savari Dinne Circle, JP Nagar 8th Phase\\r\\nBangalore-560076 /n/r Almost 200 registrations! Don\\'t miss out on the hottest monthly event in town! #AI & The #City in #Amsterdam #CityHall tomorrow! Free tickets amsai1.eventbrite.com #smartcity #techcity #amstechcity #govtech #artificialintelligence #techtrends /n/r Are you looking for:\\r\\n- Comprehensive data science training?\\r\\n- Data science & analytic project in region?\\r\\n- Data and tech professional and talent for your big data project?\\r\\n\\r\\nData Science Indonesia (DSI) have invited > 30 data&tech professional to train-advocate you on big data tech-analytic:\\r\\n- CTO Cloudera : Amr Awadallah\\r\\n- Head of Google Analytic - Solution Enggineer lead, Google Cloud\\r\\n- Adrianus Hitijahubessy\\r\\n- William Tjhi Phd\\r\\n- Dr. Karl Ng : Director of Innovation Capital (Mdec)\\r\\n- Nadia Alatas (Cybertrend)\\r\\n\\r\\nGet early bird ticket now: datascienceweekend.id/ticketing/ /n/r Problems Using Data Mining  to Build Regression Models,Part 2\\r\\nby Jim Frost (Penn State University) \\r\\nPlease share with students and researchers\\r\\n\\r\\nhttp://blog.minitab.com/blog/adventures-in-statistics/problems-using-data-mining-to-build-regression-models-part-two /n/r We are looking for a 4+ year experienced freelancer candidate to conduct online training on Hadoop. if anybody interested please. contact us : Email at jyoti.rajput425@gmail.com /n/r I would love your feedback!! In exchange, I\\'m offering a chance to win 1 of 2 $50 Amazon gift cards :) http://www.data-mania.com/blog/data-science-training-paradise-beyond/ /n/r Hi! This is the official account of #DataScienceWeekend, a festive weekend of data science brought to you by datascience.or.id and Invisio Statistika UII which will be held on December 3rd - 5th 2016 in Yogyakarta .\\r\\n\\r\\nAt this great #DataScienceWeekend,\\r\\nwe want you to see the linkage between data science and actionable insight.\\r\\n\\r\\n At this great #DataScienceWeekend,\\r\\nwe want you to listen more about the story about how a data grow into an invaluable decision.\\r\\n\\r\\nAt this great #DataScienceWeekend,\\r\\nyou will find imagination, cognitive approaches, creative process as connectors between data & insight.\\r\\n\\r\\nThe great #DataScienceWeekend is the time when data meets creativity! \\r\\n\\r\\nIt\\'s CREATIVITIME!!\\r\\nwww.datascienceweekend.id /n/r #Tagmycollege invites you to a FREE Webinar to help you decide whether you should opt for an #MBA or PGDM.\\r\\nAnyone interested, comment or left your email id and Mobile No /n/r ?#?Tagmycollege? invites you to a FREE Webinar to help you decide whether you should opt for an ?#?MBA? or ?#?PGDM?.\\r\\nInterested to join, then comment or left your email id /n/r Does anyone know a good visualizer for postgresql (like periscopedata or modeanalytics?) /n/r Understanding Analysis of Variance (ANOVA) and the F-test\\r\\nby Jim Frost (Penn State University)\\r\\n \\r\\nJim publishes new Statistics articles every 2 weeks for MINITAB\\r\\nPlease share with students and colleagues:\\r\\n \\r\\nhttp://blog.minitab.com/blog/adventures-in-statistics/understanding-analysis-of-variance-anova-and-the-f-te /n/r Join Free Webinar On Scaling & Sharding Of Data\\r\\nRegister Now- http://goo.gl/exTM4C /n/r Join Few Hours Left to Join this free webinar to learn \"Implementing #MachineLearning Algorithm on #TwitterData (with R).\" \\r\\nRegister NOW- http://goo.gl/FeHNSl /n/r Top 10 Don\\'ts of #BigData Projects: http://bit.ly/1qCJHKM /n/r Join Live Webinar of Face Detection using Hadoop MapReduce Framework\\r\\nRegister Now-http://goo.gl/cKRmjO\\r\\nToday 20th April 2016 at 6:30PM /n/r Best Way to Analyze Likert Item Data: Two Sample T-Test versus Mann-Whitney -- by Jim Frost (Penn State University)\\r\\n\\r\\nJim publishes new Statistics articles every 2 weeks for MINITAB\\r\\nPlease share with students and colleagues:\\r\\n\\r\\nhttp://blog.minitab.com/blog/adventures-in-statistics/best-way-to-analyze-likert-item-data%3A-two-sample-t-test-versus-mann-whitney /n/r Big Data has already become the BIG way of staying agile with a strong competitive edge. Read the article to be prepared for Big Data\\'s tomorrow by understanding all new regulations & predictions:  http://bit.ly/216kLIU /n/r As per Gartner\\'s report, \\'For the fourth year, Tableau is a proven leader in the Magic Quadrant for Business Intelligence and Analytics Platforms Report\\'. Read the full report below. \\r\\n\\r\\nGartner\\'s report on Tableau - http://tabsoft.co/1OfKbKV\\r\\n\\r\\nTableau is recognized for the expansion of the range of data source connectivity, an increase in the analytical depth of the tool, the flexible choice between server and online data interaction, and the high variety of use-cases it can be deployed against. Below are some of the remarkable Tutorials which teaches and explains Tableau, its features and benefits in detail, enough to make you a Tableau ninja\\r\\n\\r\\nLevel of Detail Expressions                 - http://bit.ly/2183O0P\\r\\nAmazing features in Tableau 9.0             - http://bit.ly/1TnAS2a\\r\\nCreating square Choropleth map in Tableau   - http://bit.ly/1onKgai\\r\\nHow to create Calendar Heat Maps in Tableau - http://bit.ly/1Kp1TRr\\r\\nNetwork Diagram using Path Shelf in Tableau - http://bit.ly/1KVLofw\\r\\nConnecting to MS Access Database in Tableau - http://bit.ly/1KVLt2U /n/r Is #bigdata still a thing? The big data landscape in 2016. Full image: http://bit.ly/1Kftm7W /n/r It\\'s forecasted that Big Data will touch the whopping $88 billion mark by 2021. Read how #BDaaS will play a role: http://bit.ly/1nXBLDk /n/r Do u know how u can turn big data initiatives into large-sized profits? Here are 5 tips that can help your market growth:  http://bit.ly/1KTiR55 /n/r Do you know that big data can become central to your digital strategy? See how big data can deliver massive business outcomes for a wide range for corporate goals:  http://bit.ly/20aVX5u /n/r There\\'s a lot that has been written with regards to Hadoop1.0 & #Hadoop 2.0. Take a quick look at their main features and the differences that exist between the two: http://bit.ly/1SGMLkL /n/r Did you know that 53% of 1,217 companies had undertaken at least one Big Data initiative? Check out why you should get Certified: http://bit.ly/1T8r5NB /n/r The checklist of Big Data best practices requires small-scale investments in specialized skills. Read More: http://bit.ly/1NfM9dU /n/r How big is Big Data Project? Now how do Big Data Problems Look like? Read more at: http://bit.ly/1SPV8JI /n/r Check out this #Infographic about how to become a #BigData Developer.  http://bit.ly/1nd2meZ /n/r India Ideathon 3.0 promotes the spirit of Entrepreneurship. Funds upto Rs 1,00,000, for student\\'s/ Professional\\'s ideas/projects. \\r\\nHave a brilliant Idea? Wanna make it Real? Take it to the next level with us.\\r\\nRegister now: http://bit.ly/1QZvktx\\r\\nDeadline: 10th January, 2016 /n/r Need advice-for my son,,\\r\\ndeciding between ACtuaries and Data science analyst. He is BSc in Maths from Mumbai University-Jaihind college. Not astrong computerscience background. Can u suggest??\\r\\nI feel actuaries is a great line however TOUGH and very long -15 Exams .\\r\\nIf he considers Data analytics, can u suggest any great where he can doe the course from if u think he is eligible ? /n/r Hello! I work for a media platform in Berlin that focuses on Data Science and I am looking to talk with anyone from the Big Data community that knows the scene in Tel Aviv or they are based in Tel Aviv, Israel - please, PM.:) Thanks! /n/r Just think of social media messages going viral in minutes, the speed at which credit card transactions are checked for\\r\\nhttps://www.linkedin.com/pulse/when-comes-monetization-all-data-created-equal-vivek-upadhyay-1?trk=pulse_spock-articles /n/r I need synthetic data streams are\\r\\ngenerated by using IBM synthetic data generator proposed\\r\\nby Agrawal and Srikant (1994). Two synthetic data\\r\\nstreams, denoted by T5.I4.D1000K and T15.I6.D1000K.\\r\\ncan you help me?? /n/r To celebrate the first day of Data Natives, on 19th of November we are throwing a kick ass party for all you Data lovers out there, at PLATOON Kunsthalle in Mitte, from 9.30 till the early morning. And it\\'s free of charge!\\r\\n\\r\\nSo make sure you reserve a slot on Eventbrite and come along to party like a Data Native! Whoop whoop...or should I say 0010101101?!\\r\\n\\r\\nCheck the event link for more info! /n/r A note on Data Visualisation:\\r\\nhttp://anjusthoughts.blogspot.in/2015/10/data-visualization.html /n/r Introduction of Database and Database Management System (DBMS)\\r\\n\\r\\nhttp://dwbimaster.com/data-warehouse/introduction-of-database-and-database-management-system-dbms/ /n/r MODERN DATA WAREHOUSE WEBINAR - Turning the Corner: What It Takes to Build a Modern Data Warehouse\\r\\n\\r\\nModern Data Warehouse Roundtable Webcast - 15th October\\r\\n\\r\\nREGISTER: http://tinyurl.com/o2lvc87\\r\\n\\r\\nIf necessity is the mother of all invention, then it\\'s no wonder the technology space moves so fast. From the consumerization of IT to the steady emergence of innovative solutions, organizations often struggle to stay afloat in the sea of options. As we march steady to the pace of Moore\\'s Law, analysts and business users expect quicker and easier access to enterprise data, and the data warehouse of yore must modernize to meet those demands. \\r\\n\\r\\nRegister for this Roundtable Webcast with Analyst David Loshin, Dwaine Snow of IBM and Heine Krog Iversen of TimeXtender. This expert panel will discuss the market forces that brought data warehousing to its current state and the key factors driving today\\'s innovation. They will explore how new data types and sources are raising analytic expectations and changing the way enterprises design information architectures. /n/r How to become a #DataScientist for Free https://lnkd.in/bxdKtMS #bigdata #java /n/r what does it means this code (with scala on apache spark)\\r\\n\\r\\ndef hash(rdd: RDD[String]): RDD[Vector] = {\\r\\nval doc=rdd.map(line => line.split(\",\").toSeq)\\r\\n      val hashingTF = new HashingTF()\\r\\n      val tf = hashingTF.transform(doc)\\r\\n      tf.foreach(println) \\r\\n      val idf = new IDF().fit(tf)\\r\\n      idf.transform(tf)\\r\\n} /n/r #BigData As A Service #Market Worth 7.0 Billion USD By 2020! \\r\\nCheckout #Article at: http://goo.gl/Mi5aO9\\r\\n#DataScience #DataScientist /n/r What Are The Key Traits Of A #DataScientist?\\r\\nClick here to Know the answer: http://goo.gl/15XFnz \\r\\n#DataScience #BigData /n/r What Are The Types Of #Virtulization ? \\r\\nCheckout complete #Infographic at: http://goo.gl/VCKQO7 \\r\\n#DataCenter #BigData /n/r The term #Bigdata and #ApacheHadoop goes hand in hand. Aimed at storing and processing big data, Hadoop has core components that have individual functions. Furthermore, Hadoop can also be combined with multiple applications for beneficial outcomes. \\r\\n http://www.greycampus.com/blog/big-data/all-you-need-to-know-about-hadoop /n/r Who Are The Most Notable And Influential #DataScientists? \\r\\nTo Join this #Discussion visit here: http://goo.gl/BVP4E1 \\r\\n#BigData #DataScience /n/r C?mo analizar grandes vol?menes de datos en las investigaciones en salud y biomedicina?\\r\\n\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\\r\\nEstimados  Profesionales e Investigadores, los invitamos a participar en el Programa de especializaci?n \"CIENCIA DE DATOS PARA LA INVESTIGACI?N EN SALUD Y BIOMEDICINA\" en sus dos modalidades presencial  y virtual. \\r\\n* Se pueden inscribir m?nimo con el 40% del costo.\\r\\n* Expositor: Dr. Erwin Kraenau Espinal\\r\\n* Inicio Modalidad Presencial: 03 octubre al 12 diciembre 2015\\r\\n* Inicio Modalidad Virtual: 05 octubre al 11 diciembre 2015.\\r\\n* Modalidades: Presencial y Virtual\\r\\nhttp://www.urp.edu.pe/programasespecializacion/portal/index.php?idprogramas=190\\r\\n========Quedan pocas vacantes!!!\\r\\n\\r\\n****************************************************\\r\\nINSCRIPCIONES:\\r\\n****************************************************\\r\\nPara INSCRIBIRSE por favor escribirnos a destadistico@gmail.com, para enviarle los Formatos de inscripci?n.\\r\\nRPC 993 477 990; RPM #968 248 582\\r\\nWeb URP\\r\\nhttp://www.urp.edu.pe/programasespecializacion/portal/index.php?idprogramas=190\\r\\nVisite: https://www.facebook.com/events/577595239081058/ /n/r What Are The Best #Blogs For #DataScientists To Read?\\r\\nTo join this Discussion visit here: http://goo.gl/chQO5j #DataScience /n/r What Are The Best #Blogs For #DataScientists To Read?\\r\\nTo join this Discussion visit here: http://goo.gl/chQO5j #DataScience /n/r More about sentiment analysis:\\r\\nhttp://anjusthoughts.blogspot.in/2015/08/more-about-sentiment-analysis.html /n/r What Are The #Downsides Of Being A #DataScientist?\\r\\nTo join this #Discussion visit here: http://goo.gl/LJZbCB \\r\\n#BigData #DataScience /n/r What Can A #DataScientist Create In 1 Hour, 1 Day, 1 Week, Or 1 Month?\\r\\nTo join this #Discussion visit here: http://goo.gl/Ht3fBm\\r\\n#DataScience #BigData /n/r What Are Some Good #Thesis Topics In #DataScience?\\r\\nTo join this #Discussion at: http://goo.gl/KnTdc4 \\r\\n#DataScientist /n/r What Can #Economists Learn From #DataScientists?\\r\\nTo join this #Discussion visit here: http://goo.gl/reSXsL\\r\\n#BigData #DataScience /n/r \"Save Money With #BigData\"! To checkout complete #Infographic visit here: http://goo.gl/SqietF #DataScientist #DataScience /n/r What Are The Most Influential #Papers In The #World Of #BigData? Why?\\r\\nTo join this #Discussion visit here: http://goo.gl/sVzElR \\r\\n#DataScientist #DataScience /n/r How Can The #BankingSector Benefit From #BigDataAnalysis?\\r\\nTo join this #Discussion visit here: http://goo.gl/nMKtxF #BigData #BigDataAnalytics /n/r Why Do Companies Like Uber & Lyft Need #DataScientists And #DataEngineers?\\r\\nTo join this #Discussion visit here: http://goo.gl/NeAWwS \\r\\n#BigData /n/r What Qualities Should A Budding #DataScientist Have?\\r\\nTo join this #Discussion visit here: http://goo.gl/ClFeBV\\r\\n#BigData #DataScience /n/r Innoplexus is hiring... Do you wanna be a part of Innoplexus family?\\r\\n\\r\\nInbox me your CVs... before 22nd Sept. 2015...\\r\\n\\r\\nHurry up... the clock is ticking!!! /n/r Is #DataScientist Shortage Overhyped?\\r\\nTo join this #Discussion at: http://goo.gl/FLiiZF \\r\\n#DataScience #BigData /n/r 9 NoSQL Pioneers Who Modernized Data Management bit.ly/1JG915B /n/r What Are The Best #DataScience Masters Programs In The US? \\r\\nTo join this #Discussion visit here: http://goo.gl/E989nx \\r\\n#BigData #DataScientist /n/r CTO of Lyft, CEO & Founder of Kaggle and Head of Data at Reddit are just some of the speakers coming along to Extract Conf in SF. Awesome keynotes, workshops and data daiquiris all day. \\r\\n\\r\\nEarly bird tickets end this week so grab one and save a few bucks: extractconf.com /n/r What Statistics Courses Should I Take To Become A #DataScientist?\\r\\nTo join this #Discussion visit here: http://goo.gl/0qGYDP \\r\\n#BigData #DataScience /n/r Do you want to learn how to master the #hadoop cluster? Become a successful Hadoop Architect by attending this interesting webinar for free on #Bigdata and #HadoopDeveloper @GreyCampus.Register here for free  http://bit.ly/1OSbTQA /n/r Can anyone let me know , in the context of #Bigdata which of the following is correct with respect to this statement : HDFS Data blocks can be read in parallel \\r\\n\\r\\nA True \\r\\nB False /n/r Are you interested in knowing about #BigData Use Cases ? Here is the way to  know all about it at OpenCampus here : http://bit.ly/1Ev7wu2 /n/r What Are Some Actual #Projects #DataScientists Have Worked On?\\r\\nTo join this #Discussion visit here: http://goo.gl/M8hVNv \\r\\n#DataScience #BigData /n/r What Are Some Actual #Projects #DataScientists Have Worked On?\\r\\nTo join this #Discussion visit here: http://goo.gl/M8hVNv \\r\\n#DataScience #BigData /n/r What Are Some Actual #Projects #DataScientists Have Worked On?\\r\\nTo join this #Discussion visit here: http://goo.gl/M8hVNv \\r\\n#DataScience #BigData /n/r Big data is no more a hush hush thing. We know that Enterprises are leaving no stone unturned to churn this big data and harness some actionable insights. So, what are their particular want. What enterprises do with big data? Know more here. - https://www.promptcloud.com/blog/what-enterprises-do-with-big-data/ /n/r Can anyone let me know which of the following is true with respect to this statement: #Hadoop is Open source  \\r\\n\\r\\nA True only for Apache Hadoop\\r\\nB Always true\\r\\nC True only for Apache and Cloudera Hadoop \\r\\nD Always False /n/r Once again in #Bangalore, Registrations open for #Cloudera Data Analyst Training.\\r\\n\\r\\nDate:- 24 - 27 Sep\\'15\\r\\nCost:- INR 74,400+ ST\\r\\n\\r\\nBook your seat.\\r\\nContact us: sbagga@xebia .com | 0124 - 4700265 /n/r What Are Some Good Resources To Practice #SQL At The Level Of A #Facebook #DataScience Interview? \\r\\nTo join this #Discussion visit here: http://goo.gl/sEKW49 \\r\\n#DataScientist #BigData /n/r Do you want to learn how master the hadoop cluster? Become a successful Hadoop Architect by attending this interesting webinar for free on #Bigdataand #HadoopDeveloper @GreyCampus.Register here for free \\r\\nhttp://bit.ly/1OSbTQA /n/r Web 2.0 engages its users in a more interactive approach. Simply, users can publish their thoughts, create social contents, participate in live social debates and can express their views as comments. Nowadays, we tend to share almost every little happening on the social channels. At times, published product reviews and usage experiences on these social surfaces help us pick an informed purchasing decision. So, in every way Web 2.0 is more integrated with the world of the internet of things (IOT) which own our daily lives.- https://www.promptcloud.com/blog/web-crawling-web-2.0-web-spider-view /n/r What Is The Relationship Between #Statisticians And #DataScientists? \\r\\nTo join this #Discussion visit here: http://goo.gl/h0NlKj \\r\\n#BigData #DataScience /n/r Do you want to learn how master the hadoop cluster? Become a successful Hadoop Architect by attending this interesting webinar for free on #Bigdata and #HadoopDeveloper @GreyCampus.Register here for free : http://bit.ly/1OSbTQA /n/r I\\'m looking for technology writers and bloggers. Pls inbox me at preeti.juneja@loginworks.com /n/r What Are The Key Traits Of A #DataScientist?\\r\\nTo join this #Discussion visit here: http://goo.gl/MA61yB \\r\\n#BigData #DataScience /n/r #Alibaba Unveils, \" #China\\'s first artificial intelligence platform\"!\\r\\nCheckout #News at: http://goo.gl/r4FKtH \\r\\n#DataCenter #DataScientist /n/r Can anyone let me know , which of the following are not metadata items in the context of #Bigdata ? \\r\\n\\r\\nA HDFS block locations\\r\\nB List of HDFS files\\r\\nC Replication factor of files\\r\\nD Access rights \\r\\nE File Records Distributio /n/r Do you want to learn how master the hadoop cluster? Become a successful Hadoop Architect by attending this interesting webinar for free on #Bigdata and #HadoopDeveloper @GreyCampus.Register here for free : http://bit.ly/1OSbTQA /n/r Do you want to learn how master the hadoop cluster? Become a successful Hadoop Architect by attending this interesting webinar for free on #Bigdata and #HadoopDeveloper @GreyCampus.Register here for free :http://bit.ly/1hHEuNO /n/r What Are The Top Algorithms That Every #DataScientist Should Have In Their Toolbox?\\r\\nTo join this #Discussion visit here: http://goo.gl/hm1p6t \\r\\n#DataScience #BigData /n/r Join folks like Andrew Ng & Anthony Goldbloom at Extract Conf, 30th October in SF (extractconf.com)\\r\\n\\r\\nReddit, Tableau, Lyft, Twitter, BigmL & more gathering for a day of fun and data-learning! /n/r Do you want to know about the Current #BigData market in terms of job opportunities? Join us for a free #Webinar on #Bigdata and#HadoopDeveloper @GreyCampus . Register here for free :http://bit.ly/1OSbTQA /n/r Big data is snowballing and with every passing day, cloud storages across the planet and other alike services, generally branded as \\'cloud as a service\\'(Caas) or Infrastructure as a service(Iaas) are getting burdened with colossal amounts of complex data processing requests. Now, supporting this fact, data-intensive applications which exist far away from its connected data center, slog to get their requests done. - https://www.promptcloud.com/blog/big-data-processing-edge-computing /n/r Are you looking for practice questions for #Bigdata? Here we have it for you. Get FREE Practice Questions for Bigdata here :http://www.greycampus.com/opencampus/big-data-developer /n/r It seems #Ola, #Uber and #TaxiForSure forgot the basics of supply and demand analytics! IVK, the co-founder and CTO of Crayon Data, backs this hypothesis with hard facts \\r\\nhttp://ow.ly/R50eq /n/r What Are Some Actual #Projects #DataScientists Have Worked On? \\r\\nTo join this #Discussion visit here: http://goo.gl/6FfULE \\r\\n#BigData #DataScience /n/r Become a successful Hadoop Architect by attending an interesting webinar for free on #Bigdata and #HadoopAdministration @GreyCampus.Register here for free :http://bit.ly/1hHEuNO /n/r Can anyone let me know , what are the stages to the IBM #Bigdata & Analytics Maturity Model? \\r\\n\\r\\nA Novice, Builder. Leader, Master\\r\\nB Ad Hoc, Foundational, Competitive, Differentiating. Breakaway\\r\\nC Descriptive Analytics, Diagnostic Analytics, Predictive Analytics, Prescriptive Analytics\\r\\nD Initial, Repeatable, Defined, Managed, Optimizing /n/r Do you want to learn how master the hadoop cluster? Become a successful Hadoop Architect by attending this interesting webinar for free on #Bigdata and #HadoopAdministration @GreyCampus.Register here for free :http://bit.ly/1hHEuNO /n/r Which Startups In India Are Focusing On #MachineLearning, #BigData And #DataScience?\\r\\nTo join this #Discussion visit here: http://goo.gl/m13QfE /n/r Do you have the skill to master the hadoop cluster? Become a successful Hadoop Architect by attending this interesting webinar for free on #Bigdata and #HadoopAdministration @GreyCampus.Register here for free :http://bit.ly/1hHEuNO /n/r What Are The Fields Where #BigDataAnalysis Is Used?\\r\\nTo join this Discussion visit here: http://goo.gl/zD1APk \\r\\n#BigData #DataScience #DataScientist /n/r Can anyone let me which is the option suitable to this sentence : #Hadoop is designed to scale up from a single server to thousands of machines, but with ? \\r\\n\\r\\nA No Fault tolerance  \\r\\nB A very low degree of fault tolerance \\r\\nC A very high degree of fault tolerance \\r\\nD None of the above /n/r Celebrations Start with The Freedom Month ! Grab your seat in the Cloudera Apache Hadoop Training scheduled at different location in India. Contact us :- Swati Bagga 0124- 4700265 | sbagga@xebia.com /n/r #IndependenceDay offer from H2kinfosys\\r\\n\\r\\nRegister today Get Best Discounts on All Courses.\\r\\n\\r\\n#QATesting Training\\r\\n\\r\\n#BusinessAnalyst Training\\r\\n\\r\\n#JAVA #J2EE Training\\r\\n\\r\\n#Hadoop #BIGDATA Training\\r\\n\\r\\n#Selenium Training\\r\\n\\r\\n#Informatica Training\\r\\n\\r\\n#QTP #HPUFT Training                                     \\r\\n\\r\\n#ManualTesting Training\\r\\n\\r\\n#Mobileappstesting Training\\r\\n\\r\\n#SAPtesting Training\\r\\n\\r\\n#DotNet Training\\r\\n\\r\\n#SoapuiTesting Training\\r\\n\\r\\n#DatabaseTesting Training\\r\\n\\r\\n#ETLTesting Training\\r\\n\\r\\nContact us: \\r\\nwww.h2kinfosys.com\\r\\nCall: +1-770-777-1269\\r\\nEmail: h2kinfosys@gmail.com\\r\\n\\r\\n#IndependenceDayOffers   #SpecialDiscounts /n/r What Are The Most Marketable Skills In The Field Of #Data, #Analysis, And #DataScience?\\r\\nTo join this #Discussion visit here: http://goo.gl/qGICCQ \\r\\n#BigData #DataScientist #bigdataanalysis /n/r Big data congress \\r\\nPlease join us and share this event\\r\\n#Daily registration is available for the #Medical #Speciality sessions, plaese visit the web site #medicres http://t.co/JruDUyos8y #GMR2015 #biostatistics conference #bioethics conference /n/r What #Skills Are Needed For #MachineLearning #Jobs?\\r\\nTo join this #Discussion visit here: http://goo.gl/oIn9eq \\r\\n#BigData #DataScience /n/r Why The Current Obsession With #BigData?\\r\\nTo Join this #Discussion at: http://goo.gl/qGFuaC \\r\\n#DataScience #DataScientist /n/r Why The Current Obsession With #BigData?\\r\\nTo Join this #Discussion at: http://goo.gl/qGFuaC \\r\\n#DataScience #DataScientist /n/r Can anyone let me know , which of the following is the outer most part of HBase data model in the context of #Bigdata ?\\r\\n\\r\\nA Row key\\r\\nB Column family\\r\\nC Database\\r\\nD Table /n/r What Is The Difference Between #DataAnalytics, #DataAnalysis, #DataMining, #DataScience, #MachineLearning, And #BigData ?\\r\\nTo join this #Discussion visit here: http://goo.gl/IrtSuK /n/r Can anyone let me know , which of the following operations can\\'t use reducer as combiner also in the context of #Bigdata ? \\r\\n\\r\\nA Group by Maximum\\r\\nB Group by Minimum\\r\\nC Group by Average\\r\\nD Group by Count /n/r Hey all. Hosting Extract Data Conf in SF, 30th October. Love to have you join. Got awesome folks like Andrew NG from Baidu speaking :-)\\r\\n\\r\\nhttp://extractconf.com/ /n/r What is Big Data Hadoop? Is this right time to switch your domain to Big data Hadoop? What is needed to learn it and will you be able to learn it? Get answers to many more similar questions which you have in your mind from industry expert in first free online session on Monday 10th Aug-2015 at 9.30 PM - 11 PM IST.\\r\\n\\r\\nRegister at below link:\\r\\nhttp://data-flair.com/big-data-hadoop-training-first-free-session/\\r\\n\\r\\nFeatures of our training:\\r\\n- Learn from trainer who has trained thousands of candidates and helped them in boosting their career in this technology\\r\\n- Gain handson knowledge through practicals, workshops, POCs and live project\\r\\n- Get yourself prepared for interviews and Cloudera certification\\r\\n- Get course completion certificate from DataFlair /n/r What Are The Most Common Mistakes Made By Aspirational #DataScientists?\\r\\nTo join this #Discussion at: http://goo.gl/Ihhsyh \\r\\n#BigData #DataScience /n/r 5 Ways in which #BigData will help your #Business!\\r\\nCheckout complete #Infographic at: http://goo.gl/yVS9Cs /n/r What #Classes Should I take If I Want To Become A #DataScientist ?\\r\\nTo join this #Discussion visit here: http://goo.gl/PGmUh2 \\r\\n#BigData #DataScience /n/r When You are developing a combiner that takes as input Text keys, IntWritable values, and emits Text keys, IntWritable values. In your opinion , which interface should your class implement in the context of  #Bigdata? \\r\\n \\r\\nA.  Combiner <Text, IntWritable,Text, IntWritable>\\r\\nB  Reducer <Text, IntWritable,Text, IntWritable>\\r\\nC  Combiner <Text,Text, IntWritable, IntWritable>\\r\\nD  Combiner <Text, Text, IntWritable, IntWritable> /n/r Sentiment Analysis Tools\\r\\nhttp://anjusthoughts.blogspot.in/2015/08/tools-used-for-sentiment-analysis.html?m=0 /n/r What Are The Best Methods For Testing #BigData Applications?\\r\\nTo join this Discussion visit here: http://goo.gl/eIgD8j\\r\\n#DataScience /n/r How #Cloud & #SMBs Can Help Each Other! \\r\\nCheckout #Blog at: http://goo.gl/jyCU0X \\r\\n#CloudComputing #SMEs /n/r Vacancies available at Blue Kangaroo, Amman: PHP developer, Front End Engineer, iOS developer. To apply, send your CV to careers@bluekangaroo.com. /n/r Willing to know what is Big data Hadoop and why whole world is behind learning it?Learn basics of Big Data Hadoop in first free online session on Thursday 6 Aug-15 at 7.30 AM - 9 AM IST.\\r\\n\\r\\nRegister at below link:\\r\\nhttp://data-flair.com/big-data-hadoop-training-first-free-session/\\r\\n\\r\\nFeatures of our training:\\r\\n- Learn from industry expert having 18 years professional experience and more than 5 years of training experience\\r\\n- Gain handson knowledge through practicals, workshops, POCs and live project\\r\\n- Get yourself prepared for interviews and Cloudera certification\\r\\n- Get course completion certificate from DataFlair /n/r What \" #BigData \" Opportunities Will Be Most Interesting And Profitable For #Startups?\\r\\nTo join this #Discussion visit here: http://goo.gl/c5GPmo \\r\\n#DataScience /n/r Can anyone let me know what does Hive data models represent in the context of #Bigdata ?\\r\\n\\r\\nA Table in HDFS\\r\\nB Table in Metastore DB \\r\\nC None of the Options \\r\\nD Directories in HDFS /n/r Do you want to know about the Current #BigData market in terms of job opportunities? Join us for a free #Webinar on #Bigdata and #HadoopDeveloper @GreyCampus . Register here for free : http://bit.ly/1OSbTQA /n/r Can anyone let me know in which of the following categories does the sliding window operations fall in the context of #Bigdata ? \\r\\n\\r\\nA Big Data Batch Processing\\r\\nB OLTP Transactions\\r\\nC Small Batch Processing\\r\\nD Big Data Real Time Processing /n/r Boost Your Career with Free Video Tutorials of upcoming Technology Big Data:\\r\\n\\r\\nhttp://data-flair.com/big-data-and-hadoop-training-videos/\\r\\n\\r\\nWhy learn Hadoop?\\r\\nWe create 2.5 quintillion bytes of data every day. So much that 90% of the data in the world today has been created in the last two years alone (Source: IBM). These extremely large datasets are hard to deal with using legacy systems such as RDBMS as data exceed the storage and processing capacity of database. The legacy systems are becoming obsolete.\\r\\n\\r\\nAccording to Gartner: ?Big Data is new Oil?. Big Data is all about finding the needle of value in a haystack of Structured, Semi-structured and Un-structured data. Hadoop (the Solution of All Big Data Problems) has become the most important component in the data stack, which enables rapid processing of data at petabyte scale. Hadoop is expected to be at the core of more than half of all analytics software within the next two years. /n/r #NEC, #Andhra Govt. To Collaborate On #SmartCitiesProject! \\r\\nCheckout #News at: http://goo.gl/ty4CxF /n/r Are you interested in knowing about the Current #BigData market in terms of job opportunities? Join us for a free #Webinar on #Bigdata and #Hadoop Developer @GreyCampus .Register here for free : http://bit.ly/1OSbTQA /n/r Why Is It So Hard To Recruit Top #DataScientists? \\r\\nTo join this #Discussion visit here: http://goo.gl/ikwbNv \\r\\n#BigData #DataScience /n/r Why Is #Python A Language Of Choice For #DataScientists? \\r\\nTo join this #Discussion at: http://goo.gl/d0bgNM #BigData #DataScience /n/r Hadoop Online Training by H2KInfosys is known for their excellent and professional Training. We have designed our Big Data course as per the demand in the present IT industry. Based on this demand, We started providing Hadoop online training course. \\r\\n\\r\\nHadoop Online Training course explains the purpose of Hadoop Technology, how to setup Hadoop Cluster, how to store Big Data using Hadoop (HDFS) and how to process/analyze the BigData using Map-Reduce Programming or by using other Hadoop ecosystems.\\r\\n\\r\\nWe have trained many students on Apache Hadoop Technology and provided efficient Big Data Tutorials during the course.\\r\\n\\r\\n\"Core Java \"  is offered free for those people who opt for Hadoop Online Training Course.\\r\\n\\r\\nEnroll for our Hadoop online Training Demo now!\\r\\n\\r\\nFor more details:\\r\\nhttp://www.h2kinfosys.us/courses/hadoop-big-data-online-training/\\r\\nEmail: h2kinfosys@gmail.com\\r\\nCall us:\\r\\nUSA: +1 770-777-1269\\r\\nUK: (020) 3371 7615. /n/r What Are Some Important Questions To Ask A #Recruiter When #Interviewing For A #DataScience Job?\\r\\nTo join this #Discussion visit here: http://goo.gl/GNcdBC \\r\\n#BigData #DataScientist /n/r Enroll for our Free Webinar on #Bigdata and #Hadoop on 3 rd August, 2015 8:00pm to 11:00pm IST. Register Now .\\r\\nhttp://www.greycampus.com/training/big-data-hadoop-webinar /n/r What Are The Best #BigDataAnalysis #Companies?\\r\\nTo join this #Discussion visit here: http://goo.gl/oFPuwg \\r\\n#BigData #DataScience /n/r Enroll for our Free Webinar on #Bigdata and #Hadoop on 3 rd August , 2015 8:00pm to 11:00pm IST. Register Now .\\r\\nhttp://www.greycampus.com/training/big-data-hadoop-webinar /n/r Can anyone let me know which of the following is true for #Hive? \\r\\n\\r\\nA Hive supports schema checking\\r\\nB Hive is the database of Hadoop \\r\\nC Hive can replace an OLTP system\\r\\nD Hive doesn\\'t allow row level updates /n/r Guys are you willing to become successful Hadoop developer which requires java knowledge along with in depth knowledge of Map Reduce that is the heart of Hadoop, that too without affecting much on your pocket?\\r\\n\\r\\nAttend first free online class on 29 July at 9.30 PM - 11 PM IST and check the level of training yourself.Register at below link:\\r\\nhttp://data-flair.com/big-data-hadoop-training-first-free-session/\\r\\n\\r\\nFeatures of our training:\\r\\n- 40 hours of live instructor led session\\r\\n- Instructor having 18 years of experience with 5 years of training experience\\r\\n- More than 15 hours on Map Reduce concepts to provide complete hands on knowledge\\r\\n- Batches as per your time convenience\\r\\n- Complementary self paced java course to provide java essential for Hadoop /n/r What Is The Future Of #DataScience?\\r\\nTo Join this #Discussion visit here: http://goo.gl/GFxb7N #BigData #DataScientist /n/r Can anyone let me know when is the earliest point at which the reduce method of a given reducer be called in the context of #Bigdata?\\r\\n\\r\\nA Not untill all mappers have finished processing all records \\r\\nB As soon as a mapper has emitted atleast one record\\r\\nC It depends on the input format used for the job \\r\\nD As soon as atleast one mapper has finished processing its input split /n/r We are proud to announce that Xebia has been awarded the \"Best Training Company in cutting-edge technologies\" at World Education Congress Summit /n/r Willing to know what is Big data Hadoop and why whole world is behind learning it?Learn basics of Big Data Hadoop free on 26 July at 8.00 AM - 10 AM IST!!!\\r\\n\\r\\nRegister at below link:\\r\\nhttp://data-flair.com/big-data-hadoop-training-first-free-session/\\r\\n\\r\\nFeatures of our training:\\r\\n- Learn from industry expert having 18 years professional experience and more than 5 years of training experience\\r\\n- Gain handson knowledge through practicals, workshops, POCs and live project\\r\\n- Get yourself prepared for interviews and Cloudera certification\\r\\n- Get course completion certificate from DataFlair /n/r How Do I Learn #DataMining?\\r\\nCheckout #Discussion at: http://goo.gl/WDR727 \\r\\n#BigData #DataScience /n/r \"Modern #DataScientist\" \\r\\nCheckout #Infographic at: http://goo.gl/ZXAvgJ \\r\\n#BigData #DataScience /n/r Coming to a computer screen near you! CRACKING CODE - a contest to reward some o the coding folk! \\r\\nThey have 2 rounds. If you make it through, you get to take your pick from an exciting list of rewards! So gear up, save the date and get ready to crack some code! \\r\\nTo know more about bigdata-madesimple, visit www.bigdata-madesimple.com /n/r Can anyone let me know which of the following is true for #Hadoop Pseudo Distributed mode ?\\r\\n\\r\\nA Runs on single machines with all daemons \\r\\nB It runs on multiple machines\\r\\nC Runs on single machines without all daemons\\r\\nD Runs on multiple machines without all daemons /n/r Can anyone let me know , which of the following decides number of mappers for a #Mapreduce job ? \\r\\n\\r\\nA File Location\\r\\nB Mapred.map.tasks parameter\\r\\nC Input file size \\r\\nD Input splits /n/r What Are Some Interesting Beginner Level Projects That Can Be Built Using Apache #Hadoop?\\r\\nTo join this #Discussion visit here: http://goo.gl/7z3HhK #BigData #DataScience /n/r Big Data, Big Security: Defense in Depth bit.ly/1OyK7Zx /n/r What Kinds Of Large #Datasets Open To The Public Do You Analyze The Mostly? \\r\\nTo join this #discussion visit here: http://goo.gl/XEVVvG #BigData /n/r Can anyone let me know , what should be an upper limit for counters of a #MapReduce job? \\r\\n\\r\\nA ~5s\\r\\nB ~15\\r\\nC ~150\\r\\nD ~50 /n/r How Can I Improve My Profile To Get A #DataScientist Job In #Google/ #Facebook/ #LinkedIn? \\r\\nTo join this #discussion visit here: http://goo.gl/8ZOzAs /n/r Guys are you willing to become successful Hadoop developer which requires java knowledge along with in depth knowledge of Map Reduce that is the heart of Hadoop, that too without affecting much on your pocket?\\r\\n\\r\\nAttend first free class on Sunday 26th July-15 at 8 AM-10 AM IST and check the level of training yourself. Register at below link:\\r\\nhttp://data-flair.com/big-data-hadoop-training-first-free-session/\\r\\n\\r\\nFeatures of our training:\\r\\n- 40 hours of live instructor led session\\r\\n- Instructor having 18 years of experience with 5 years of training experience\\r\\n- More than 15 hours on Map Reduce concepts to provide complete hands on knowledge\\r\\n- Batches as per your time convenience\\r\\n- Complementary self paced java course to provide java essential for Hadoop /n/r Did you miss our #webinar on #Bigdata and #HadoopDeveloper ? We have a recorded video of it .Here is the link where you can watch it http://certification.greycampus.com/big-data-and-hadoop-webinar/ /n/r Can anyone let me know , which of the following #Hadoop config files is used to define the heap size?\\r\\n\\r\\nA hdfs-site.xml\\r\\nB core-site.xml\\r\\nC hadoop-env.sh\\r\\nD Slaves /n/r Can anyone let me know , what does commodity hardware in #Hadoop mean ?\\r\\n\\r\\nA Industrial Standard hardware\\r\\nB Discarded hardware \\r\\nC Very Cheap Hardware\\r\\nD Low Specifications industry grade hardware /n/r An overview of Sentiment Analysis:\\r\\nhttp://anjusthoughts.blogspot.in/2015/07/sentiment-analysis_18.html?m=0 /n/r #DataScience 2015: What\\'s Hot And What\\'s Not! Checkout #Infographic at: http://goo.gl/H524kP #BigData /n/r What is Big Data Hadoop? Is this right time to switch your domain to Big data Hadoop? What is needed to learn it and will you be able to learn it? Get answers to many more similar questions which you have in your mind from industry expert free on 14 July at 9.30 PM-11 PM IST.\\r\\n\\r\\nRegister at below link:\\r\\nhttp://data-flair.com/big-data-hadoop-training-first-free-session/\\r\\n\\r\\nFeatures of our training:\\r\\n- Learn from trainer who has trained thousands of candidates and helped them in boosting their career in this technology\\r\\n- Gain handson knowledge through practicals, workshops, POCs and live project\\r\\n- Get yourself prepared for interviews and Cloudera certification\\r\\n- Get course completion certificate from DataFlair /n/r Can anyone let me know what is  correct the order of the following phases of a #MapReduce program in the order that they execute? \\r\\nPartitioner \\r\\nMapper \\r\\nCombiner \\r\\nShuffle/Sort\\r\\nA. Mapper Combiner Partitioner Shuffle/Sort\\r\\nB. Mapper Combiner Shuffle/Sort Partitioner\\r\\nC. Mapper Partitioner Shuffle/Sort Combiner\\r\\nD. Mapper Shuffle/Sort Combiner Partitioner\\r\\nE. Partitioner Mapper Shuffle/Sort Combiner\\r\\nF. Combiner Mapper Shuffle/Sort Partitioner /n/r What Are Some Important Questions To Ask A Recruiter When Interviewing For A #DataScience Job? \\r\\nTo join this #Discussion at: http://goo.gl/84dHOr #BigData /n/r Willing to know what is Big data Hadoop and why whole world is behind learning it?Learn basics of Big Data Hadoop free on 17 July at 9.30 PM-11.30 PM IST!!!\\r\\n\\r\\nRegister at below link:\\r\\nhttp://data-flair.com/big-data-hadoop-training-first-free-session/\\r\\n\\r\\nFeatures of our training:\\r\\n- Learn from industry expert having 17 years professional experience and more than 3 years of training experience\\r\\n- Gain handson knowledge through practicals, workshops, POCs and live project\\r\\n- Get yourself prepared for interviews and Cloudera certification\\r\\n- Learn java essentials for Hadoop as complementary\\r\\n- Get lifetime access to complete study material and recorded sessions\\r\\n- Get course completion certificate from DataFlair /n/r 4 #BigData Challenges In #Healthcare! Checkout #Slideshare at: http://goo.gl/eez34D /n/r Do I Need A Masters/PhD To Become A #DataScientist?\\r\\nTo join this #Discussion visit our site: http://goo.gl/MTTjgR /n/r Enroll for our Free Webinar on #Bigdata and Hadoop Developer. JULY 15, 2015 , 8:00pm to 11:00pm IST. Training conducted by Nag Bhushan\\r\\nhttp://www.greycampus.com/training/big-data-hadoop-webinar /n/r Enroll for our Free Webinar on #Bigdata&HadoopDeveloper. JULY 15, 2015 , 8:00pm to 11:00pm IST. Training conducted by Nag Bhushan\\r\\nhttp://www.greycampus.com/training/big-data-hadoop-webinar /n/r What Are The Best Hosted #Hadoop Providers?\\r\\nTo join this #Discussion visit our site: http://goo.gl/7FZYP1 \\r\\n#BigData /n/r With the right analytics in place ,  in your opinion , for what purposes can #Bigdata be used  ?  You can refer : http://bit.ly/1JRhZSf \\r\\n\\r\\nA Correcting consumer Behaviour\\r\\nB Analyzing driving patterns for accurate insurance premium estimations\\r\\nC Including semantics driven strategy for clearbottom line figures /n/r Willing to know about Biggest Buzz Word - Big data ?\\r\\n\\r\\nhttp://goo.gl/05cEYo\\r\\n\\r\\nTraining Objectives:\\r\\n- Lead you on the path to become a certified Hadoop professional\\r\\n- Prepare to lead Hadoop movement and become employable in Big Data Industry\\r\\n- Learn how to Develop Game-Changing Hadoop Applications\\r\\n- Prepare for Interviews and Cloudera certification /n/r Can anyone let me know the correct choice for this sentence .MapReduce can best be described as a programming model used to develop #Hadoop based applications that can process massive amounts of unstructured data  \\r\\n\\r\\nA True\\r\\nB False /n/r What Are Some Examples Of Bad #DataScience Leading To Bad Decisions?\\r\\nTo join this #Discussion visit here: http://goo.gl/5thrkG /n/r Enroll for our Free Webinar on #Bigdata&HadoopDeveloper. JULY 15, 2015 , 8:00pm to 11:00pm IST. Training conducted by Nag Bhushan\\r\\nhttp://www.greycampus.com/training/big-data-hadoop-webinar /n/r Are you still confused whether to learn latest technology Big Data Hadoop when your friends have already started working in it? Attend free session on Big Data Hadoop on 14 July at 9.30 PM-11.30 PM IST and get answers to your qeueries.\\r\\n\\r\\nRegister at below link:\\r\\nhttp://data-flair.com/big-data-hadoop-training-first-free-session/\\r\\n\\r\\nFeatures of our training:\\r\\n- Learn from trainer who has trained thousands of candidates and helped them in boosting their career in this technology\\r\\n- Gain handson knowledge through practicals, workshops, POCs and live project\\r\\n- Get yourself prepared for interviews and Cloudera certification\\r\\n- Get course completion certificate from DataFlair /n/r Enroll for our Free Webinar on #Bigdata&HadoopDeveloper. JULY 15, 2015 , 8:00pm to 11:00pm IST. Training conducted by Nag Bhushan\\r\\nhttp://www.greycampus.com/training/big-data-hadoop-webinar /n/r Enroll for our Free Webinar on #Bigdata&HadoopDeveloper. JULY 15, 2015 , 8:00pm to 11:00pm IST. Training conducted by Nag Bhushan\\r\\nhttp://www.greycampus.com/training/big-data-hadoop-webinar /n/r How Do #DataScientists Use Statistics?\\r\\nTo join this Discussion visit here: http://goo.gl/mO0i16 #BigData #HPCAsia /n/r #CloudComputing For Hospitality! \\r\\nCheckout #Infographic at: http://goo.gl/0w8y4Q /n/r According to you ,which of the following is included in additional capabilities as companies move past the experimental phase with #Hadoop ?\\r\\n\\r\\nA Improved data storage and information retrieval\\r\\nB Improved extract, transform and load features for data integration\\r\\nC Improved data warehousing functionality\\r\\nD Improved security, workload management and SQL support /n/r Enroll for our Free Webinar on #Bigdata and Hadoop Developer. JULY 15, 2015 , 8:00pm to 11:00pm IST. Training conducted by Nag Bhushan\\r\\nhttp://www.greycampus.com/training/big-data-hadoop-webinar /n/r Is it right time to learn Big Data?? Should I learn Big Data ??\\r\\nGet Answer to all your Questions, Watch the exclusive recording:\\r\\n\\r\\nhttps://www.youtube.com/watch?v=VYkA4-EajIw\\r\\n\\r\\nBig Data\\r\\nBig Data is now part of agenda of most of the fortune 500 companies. Big Data technology is maturing fast and Leading corporations in Banking and Financial Services, Utilities, Telco and Retail have embraced Big Data as foundation technology for information management.\\r\\nAcross the world, Big Data is on boom. Companies especially in the finance, retail and e-commerce sectors are relying more heavily on big data to make decisions that impact their sales, operations and workforce. There is such a dearth of Big Data talent that these companies are willing to pay high salaries for the right skill set. /n/r What Are Some Interesting #BigData And Analytics Startups? \\r\\nTo join this #Discussion visit here: http://goo.gl/tiiij8 /n/r 4 Insights For #BigData Tool Makers! \\r\\nCheckout #Slideshare at: http://goo.gl/L551UA /n/r According to you, for what can traditional IT systems provide a foundation when they\\'re integrated with big data technologies like #Hadoop? \\r\\n\\r\\n A Big data management and data mining\\r\\n B Data warehousing and business intelligence\\r\\n C Management of Hadoop clusters\\r\\n D Collecting and storing unstructured data /n/r What Should Be Ideal Size, Skill Set And Composition Of Team For A Successful #BigData Implementation In An #Organization? \\r\\nTo join this #Discussion visit here: http://goo.gl/vgcgM4 #hpc /n/r Carve Your Career with Big Data..!! Learn From Industry Experts:\\r\\n\\r\\nhttp://data-flair.com/course/big-data-and-hadoop-training/\\r\\n\\r\\nTraining features:\\r\\n- Preparation for interview and Cloudera certification\\r\\n- Instructor led live Online Sessions\\r\\n- 24x7x365 Lifetime Access\\r\\n- 40Hrs Classes, 40Hrs Lab & Live Project\\r\\n- Complementary Java training /n/r How Do I Setup A Basic \\'#BigData\\' Infrastructure To Sort & Manage A 300-500GB #Dataset? \\r\\nJoin this #Discussion at: http://goo.gl/0HICps /n/r Install Cloudera Hadoop CDH5 on Ubuntu - A Step by step tutorial for Beginners: http://data-flair.com/blog/install-cloudera-hadoop-cdh5-on-ubuntu/ Alternatively watch video tutorial: https://www.youtube.com/watch?v=489ELbWzWCE /n/r What \"Useful\" Data Products Can I Build As A Aspiring #DataScientist ? \\r\\nJoin this #Discussion at: http://goo.gl/t2BboK #BigData /n/r People. Stop cracking the \"everyone talks about it no one knows it\" joke about Big Data and figure out what it is!\\r\\nHere\\'s something to help you out. \\r\\nA complete glossary of big data terms.http://goo.gl/RofKdZ /n/r Can anyone let me know , how can #bigdata help in getting better insights about customers? You can refer : http://bit.ly/1aX5Z1I /n/r How Do I Learn #DataScience In A Time-Efficient Manner? \\r\\nTo Join this #Discussion at: http://goo.gl/nm9VN9 #BigData /n/r Searching for some books to get acquainted with the basics of AI? I personally don\\'t know a thing about AI but Number 14 on here is about the history of AI! THAT I can understand! This list pretty much covers it all.\\r\\nhttp://goo.gl/pf2UN8 /n/r Can anyone let me know , how does getting on top of the challenges of #bigdata enable sophisticated approaches for product marketing, development, and sales ? You can refer : http://bit.ly/1JbEajU /n/r #DataScience 2015: What\\'s Hot And What\\'s Not! \\r\\nCheckout complete #Infographic at: http://goo.gl/jYsk0K #HPC /n/r Can anyone let me know , in which directory #Hadoop is installed ? /n/r What Are The Fields Where #BigData Analysis Is Used?\\r\\nJoin this #Discussion at: http://goo.gl/1vfWM0 #HPC /n/r Peter Coveney To Keynote #ISC Cloud & #BigData Conference! Checkout complete story at: http://goo.gl/mdy78L #HPC #CloudComputing /n/r What Are The Best #BigData Analysis Companies?\\r\\nTo join this #Discussion : http://goo.gl/JH6d6e #hpc /n/r #BigDataAnalytics, #InternetOfEverything And Advanced AI Add Momentum To Healthcare Innovation! \\r\\nCheckout #GuestArticle at: http://goo.gl/kNsXJW #HPC /n/r 5 advantages and disadvantages of Cloud Storage\\r\\nhttp://goo.gl/6NHXT0 /n/r Can anyone let me know , which of the following can the workflows expressed in #Oozie contain ?\\r\\n\\r\\nA.Iterative repetition of MapReduce jobs until a desired answer or state is reached.\\r\\nB. Sequences of MapReduce and Pig jobs. These are limited to linear sequences of actions with exception handlers but no forks.\\r\\nC. Sequences of MapReduce jobs only; no Pig or Hive tasks or jobs. These MapReduce sequences can be combined with forks and path joins.\\r\\nD. Sequences of MapReduce and Pig. These sequences can be combined with other actions including forks, decision points, and path joins /n/r Data science is being touted as the hottest career option in the 21st century. Time to get your hands dirty with data! \\r\\nhttp://bigdata-madesimple.com/data-scientist-hacker-programmer-analyst-story-teller-artist/ /n/r Can  anyone  let  me  know  in  the  context  of #bigdata, how  is  the  volume  measured  by  data  warehouses  ? You  can  refer : http://goo.gl/pb8LVx \\r\\n\\r\\nA Gigabytes\\r\\nB Gigabytes per hour\\r\\nC Terabytes \\r\\nD Petabytes /n/r Can anyone let me know , what is the largest data source used by organizations in the context of #bigdata? \\r\\n\\r\\nA Emails \\r\\nB Business Transactions \\r\\nC Log Data\\r\\nD Social Media /n/r Watch exclusive recordings of Big Data & Hadoop Training:\\r\\nhttp://data-flair.com/big-data-and-hadoop-training-videos/\\r\\n\\r\\nTraining features:\\r\\n- Instructor led Live Session\\r\\n\\r\\n- Preparation for interview and Cloudera certification\\r\\n- Become employable in Big Data Industry\\r\\n- Complementary java training /n/r Free Session on Big Data on 1st July 2015 @ 9.30PM IST\\r\\nhttp://goo.gl/tS6P06 /n/r What Tools Do #DataScientists Use For Professional Presentations? To Join this #Discussion visit here: http://goo.gl/EIIOOM #BigData /n/r Can anyone let me know , in the context of #bigdata which is faster : Map-side join or Reduce-side join ? Why? \\r\\n\\r\\nA. Both techniques have about the the same performance expectations.\\r\\nB. Reduce-side join because join operation is done on HDFS.\\r\\nC. Map-side join is faster because join operation is done in memory.\\r\\nD. Reduce-side join because it is executed on a the namenode which will have faster CPU and more memory. /n/r #SmartCities- The Indian Perspective! \\r\\nTo have a look of complete #Infographic visit here: http://goo.gl/dgt0tP #SmartCitiesMission /n/r How Does Working As A #Facebook #DataScientist (or similar job) Change Your Basic Beliefs About People And Behavior?\\r\\nJoin this #Discussion at: http://goo.gl/BlNnES /n/r A data analyst knows everything about SQL. He wants to run adhoc analysis on data . In your opinion , which of the following is a data warehousing software built on top of #ApacheHadoop that defines a simple SQL-like query language well-suited for this kind of user? \\r\\n\\r\\nA Pig\\r\\nB Hue\\r\\nC Hive\\r\\nD Sqoop /n/r The Internet of Things: Tracking the \\'Cross-Everywhere\\' Consumer bit.ly/1LDtYA8 /n/r Can anyone  let  me know ,  which #bigdata tool  will  be  used  to  generate Java classes which will  process  data  imported  into HDFS  from relational database ?\\r\\n\\r\\nA  Pig\\r\\nB  Hue\\r\\nC  Hive\\r\\nD  Sqoop /n/r What Are The Downsides Of Being A #DataScientist? To Join this #Discussion group visit here: http://goo.gl/m3OeSa #HPC #SuperComputing /n/r Proven #DataScience For Competitive And Digital Marketing Intelligence! Checkout #Blog at: http://goo.gl/5p8K32 #HPC #SuperComputing /n/r How should I Prepare For Statistics Questions For A #DataScience Interview?\\r\\nTo join this #Discussion visit here: http://goo.gl/Nkkyie /n/r Can anyone let me know , how are Pig and #MapReduce related ?  \\r\\n\\r\\nA Pig provides additional capabilities that allow certain types of data manipulation not possible with MapReduce.\\r\\nB Pig provides no additional capabilities to MapReduce. Pig programs are executed as MapReduce jobs via the Pig interpreter.\\r\\nC Pig programs rely on MapReduce but are extensible, allowing developers to do special-purpose processing not provided by MapReduce.\\r\\nD Pig provides the additional capability of allowing you to control the flow of multiple MapReduce jobs. /n/r The #Hadoop framework provides a mechanism for coping with machine issues such as faulty configuration or impending hardware failure. MapReduce detects that one or a number of machines are performing poorly and starts more copies of a map or reduce task. All the tasks run simultaneously and the task finish first are used. In your opinion , what is this called ? \\r\\n\\r\\n A IdentityMapper\\r\\n B Combine\\r\\n C Default Partitioner\\r\\n D IdentityReducer\\r\\n E Speculative Execution /n/r Are Too Many People Training To Become #DataScientists? \\r\\nTo join this #Discussion visit here: http://goo.gl/g8AOBD #HPC /n/r Can anyone let me know, what is the most widely used #Bigdata store ? \\r\\n\\r\\nA Hadoop HDFS \\r\\nB Relational Databases\\r\\nC Mongo DB\\r\\nD Analytic Databases /n/r What Are Some Better Sites For #DataScientists To Freelance On Other Than Elance? \\r\\nTo join this Discussion visit here: http://goo.gl/FVa2nV #HPC #BigData /n/r Can anyone let me know , what is included in the common cohorts in #Hadoop , which is a framework that works with a variety of related tools?  \\r\\n\\r\\nA MapReduce, MySQL and Google Apps\\r\\nB MapReduce, Hummer and Iguana\\r\\nC MapReduce, Heron and Trumpet\\r\\nD MapReduce, Hive and HBase /n/r How Good Is R For #DataVisualization? \\r\\nTo Join this #Discussion visit here: http://goo.gl/VWIaw9 #HPC #BigData /n/r Few more hours to go . Join the Free Webinar from GreyCampus on #BigData and Hadoop Developer course. Calling all aspirants. See you all on 18 June at 8 P.M. Enroll Now!\\r\\nhttp://www.greycampus.com/training/big-data-hadoop-webinar /n/r What Are The Best Books For Self-Studying Statistics And #DataScience? \\r\\nTo join this #Discussion click here: http://goo.gl/Lsgklr \\r\\n#HPC /n/r Reminding all about our free webinar  from GreyCampus on #BigData and Hadoop Developer course. Calling all aspirants. See you all on 18 June at 8 P.M. Enroll Now!\\r\\nhttp://www.greycampus.com/training/big-data-hadoop-webinar /n/r #BigData Transforming Gaming! checkout complete #Blog at: http://goo.gl/EJM9oN #HPC /n/r Can anyone let me know , which organization is currently using maximum number of #Hadoop clusters  ? \\r\\n\\r\\nA Microsoft\\r\\nB IBM\\r\\nC Yahoo\\r\\nD HCL /n/r Can anyone let me know , which of the following statements most accurately describes the relationship between #MapReduce and Pig? \\r\\n\\r\\nA. Pig provides additional capabilities that allow certain types of data manipulation not possible with MapReduce.\\r\\nB. Pig provides no additional capabilities to MapReduce. Pig programs are executed as MapReduce jobs via the Pig interpreter.\\r\\nC. Pig programs rely on MapReduce but are extensible, allowing developers to do special-purpose processing not provided by MapReduce.\\r\\nD. Pig provides the additional capability of allowing you to control the flow of multiple MapReduce jobs. /n/r Join the Free Webinar from GreyCampus on #BigData and Hadoop Developer course. Calling all aspirants. See you all on 18 June at 8 P.M. Enroll Now!\\r\\nhttp://www.greycampus.com/training/big-data-hadoop-webinar /n/r TCS Generated Over $500 Million In Annual Revenue From Cloud Platforms! checkout #News at: http://goo.gl/oUHB9I #CloudComputing /n/r Can anyone let me know , what are map files and why are they important in #Bigdata ?  \\r\\n\\r\\nA.  Map files are stored on the namenode and capture the metadata for all blocks on a particular rack. This is how Hadoop is \"rack aware\" \\r\\nB.  Map files are the files that show how the data is distributed in the Hadoop cluster. \\r\\nC.  Map files are generated by Map-Reduce after the reduce step. They show the task distribution during job execution \\r\\nD.  Map files are sorted sequence files that also have an index. The index allows fast data look up. /n/r What Are Some #DataAnalysis Projects I Can Do As A #DataScience Beginner?\\r\\nTo Join this Discussion Visit here: http://goo.gl/DbGhbj \\r\\n#HPC #SuperComputing /n/r Join the Free Webinar from GreyCampus on #BigData and Hadoop Developer course. Calling all aspirants. See you all on 18 June at 8 P.M. Enroll Now!\\r\\nhttp://www.greycampus.com/training/big-data-hadoop-webinar /n/r Join the Free Webinar from GreyCampus on #BigData and Hadoop Developer course. Calling all aspirants. See you all on 18 June at 8 P.M. Enroll Now!\\r\\n http://www.greycampus.com/training/big-data-hadoop-webinar /n/r #Hadoop API uses basic Java types such as LongWritable, Text, IntWritable. They have mostly same features as default java classes. In your opinion , what are these writable data types optimized for? \\r\\n\\r\\nA. Writable data types are specifically optimized for network transmissions \\r\\nB. Writable data types are specifically optimized for file system storage \\r\\nC. Writable data types are specifically optimized for map-reduce processing \\r\\nD. Writable data types are specifically optimized for data retrieval /n/r Old Statistics: Is It What We Are Talking About? Checkout complete #Blog at: http://ow.ly/O74PS #HPC #BigData /n/r What Is The Role Big Data Will Play In Materials Science Research? \\r\\nTo Join this Discussion visit here: http://goo.gl/TvnD5i \\r\\n#HPC #SuperComputing /n/r Can anyone let me know , why would a developer create a #mapreduce without the reduce step?  \\r\\n\\r\\nA.  Developers should design Map-Reduce jobs without reducers only if no reduce slots are available on the cluster. \\r\\nB.  Developers should never design Map-Reduce jobs without reducers. An error will occur upon compile. \\r\\nC.  There is a CPU intensive step that occurs between the map and reduce steps. Disabling the reduce step speeds up data processing. \\r\\nD.  It is not possible to create a map-reduce job without at least one reduce step. A developer may decide to limit to one reducer for debugging purposes. /n/r Can anyone let me know , which #MapReduce phase is theoretically able to utilize features of the underlying file system in order to optimize parallel execution? \\r\\n\\r\\nA. Split\\r\\nB. Map\\r\\nC. Combine /n/r What Are The Best Blogs For #DataMiners And #DataScientists To Read?\\r\\nJoin this Discussion at: http://goo.gl/gGkCQp #HPC /n/r #Cloud Can Help Increase #GDP for India! checkout #Blog at: http://goo.gl/Bo5R9K #HPC #SuperComputing /n/r Which type of data scientist are you? To analyze and work upon your strengths and weakness as a data scientist, take this survey\\r\\nhttp://goo.gl/Gz2tq8 /n/r Which companies are currently hiring for #BigData in India? Join this Discussion at: http://goo.gl/j8Gkuj #HPC /n/r Stereotypes In The Age Of #BigData! Checkout complete #Blog at: http://goo.gl/vS9SJs #HPC #SuperComputing /n/r Can anyone let me know , what are the steps in the lifecycle of mapper in the context of #Bigdata ? \\r\\n\\r\\nA) JobTracker calls the map method of the mapper\\r\\nB) Jobtracker calls the task tracker and task tracker calls the map method\\r\\nC) Task tracker calls the job tracker.which inturn calls the map method /n/r What Are Some Good Projects That Can Be Made In 1 To 2 Months In \"Machine Learning\" Or \"Big Data\" Or \"Cloud Computing\"? To join this discussion visit here: http://ow.ly/NS12g /n/r How to design and implement Hive Table with partitions? Learn here with the help of an example.\\r\\n\\r\\nhttp://bit.ly/1cFUoox /n/r What Are Some Big Data Companies In Bangalore? \\r\\nTo join this Discussion visit here: http://goo.gl/ZyAhwn \\r\\n#HPC #SuperComputing /n/r #Alibaba Focusing On international Investment To Grow Its Business By Building A Financial Data Platform! Checkout #News at: http://goo.gl/PsC9Tx #BigData #HPC /n/r Can anyone let me know , which of the following #MapReduce execution frameworks focus on execution in shared-memory environments? \\r\\n \\r\\nA. Hadoop\\r\\nB. Twister\\r\\nC. Phoenix /n/r How to become our agent?\\r\\nHow much commission you will get?\\r\\nWe mainly offer mobile crushers, stationary crushers, sand-making machines, grinding mills and complete plants.\\r\\nhttp://www.crushers-mills.com/quotation-for-new-type-crawler-mobile-crusher/\\r\\nhttp://www.jaw-crushers.org/infonews/Agent-Distributor-Needed.html /n/r Can anyone let me know , which of the following utilities allows to create and run #MapReduce jobs with any executable or script as the mapper or the reducer? \\r\\n\\r\\nA. Oozie\\r\\nB. Sqoop\\r\\nC. Flume\\r\\nD. Hadoop Streaming /n/r Can India Compete With China In #BigData, #CloudComputing And The #InternetOfThings?\\r\\nTo join this Discussion click here: http://goo.gl/gERV5H /n/r Can anyone let me know , which of the following responsibility of the Job tracker have been split into separate daemons in the #MapReduce v2 ? \\r\\n\\r\\nA. Managing files\\r\\nB. Job scheduling\\r\\nC. Resource management\\r\\nD. MR job statistics /n/r Brazilian #CloudComputing Market To Reach $1.1bn By 2017! Checkout #News at: http://goo.gl/IRJ6cz #HPC #SuperComputing /n/r What Topics Of #BigData Should A #DataScientist Know And Why? \\r\\nTo join this Discussion visit here: http://goo.gl/i0dO0Q \\r\\n#HPC #SuperComputing /n/r BIG Isn\\'t Big Anymore! checkout complete #Article at: http://goo.gl/7c0fkS #HPC #SuperComputing #BigData /n/r Why Is Python A Language Of Choice For #DataScientists?\\r\\nTo join this #Discussion visit our site at: http://goo.gl/ST6Fbe \\r\\n#HPC #SuperComputing /n/r Salesforce Working Out With #BigData, New Analytics Tool! Checkout complete #News at: http://ow.ly/NH3Oo #HPC #SuperComputing /n/r In a #MapReduce program, the reducer receives all values associated with the same key. In your opinion ,which statement is most accurate about the ordering of these values? \\r\\n\\r\\nA) The values will be ordered in which they were emitted from the mappers\\r\\nB) values are in sorted order\\r\\nC) values are arbitarily ordered, but multiple runs of the same MapReduce job will have the same ordering\\r\\nD) Since the values come from mappers o/p, there will be contiguous sections of sorted values.\\r\\nE) The values are arbitarily ordered, and the ordering may vary from run to run of the same MapReduce job /n/r What Are The Downsides Of Being A #DataScientist? \\r\\nTo join this Discussion visit here: http://goo.gl/ITNDMZ \\r\\n#HPC #BigData /n/r \"BHASKARA\",A #Supercomputer To Help Meteorologist In Weather Forecasting! Checkout #News at: http://goo.gl/3fm2DL \\r\\n#HPC #SuperComputing /n/r Is A masters In #DataScience Worth It? \\r\\nTo join this #Discussion: http://goo.gl/7vCa2M #HPC #SuperComputing /n/r #BigData In Financial Services! checkout complete #Blog at: http://goo.gl/VtVwxx \\r\\n#HPC #SuperComputing /n/r #BigData In Financial Services! checkout complete #Blog at: http://goo.gl/VtVwxx \\r\\n#HPC #SuperComputing /n/r Can anyone let me know , which TACC resource has support for #Hadoop MapReduce?\\r\\n\\r\\nA. Ranger\\r\\nB. Longhorn\\r\\nC. Lonestar\\r\\nD. Spur /n/r China-Guiyang #BigData Expo! Checkout complete #News at: http://ow.ly/Nu0it #HPC #SuperComputing /n/r \"Culture of Cloud In #Asia\" \\r\\nTo checkout complete #Blog visit here: http://ow.ly/NtEzx \\r\\n#HPC #SuperComputing #CloudComputing /n/r According to you , which of the following statement is true when archiving #Hadoop files ? \\r\\n\\r\\nA.Archived files will display with the extension .arc\\r\\nB MapReduce processes the original files names even after files are archived\\r\\nC Archived files must be unarchived for HDFS and MapReduce to access the original, small files\\r\\nD Archive is intended for files that need to be saved but no longer accessed by HDFS /n/r #BigData And Big Brother: Do You Care To Be Watched? Checkout #Blog at: http://goo.gl/QuLN51   #HPC /n/r Is The IoT The Next Hype After #BigData? \\r\\nTo join this #Discussion visit here: http://goo.gl/4HHyb9\\r\\n#HPC #SuperComputing /n/r Can anyone let me know,which of the following is not an input format in #Hadoop ? \\r\\n\\r\\nA TextInputFormat\\r\\nB  ByteInputFormat\\r\\nC SequenceFileInputFormat\\r\\nD KeyValueInputFormat /n/r #BigData: Changing The E-Market As We Know It! checkout complete #Blog at: http://goo.gl/bUeC9Y #HPC /n/r #BigData: Changing The E-Market As We Know It! checkout complete #Blog at: http://goo.gl/bUeC9Y #HPC /n/r #BigData And The #InternetOfThings (IoT) Are Expected To Give A Boost To Taiwan\\'s Information Services Industry! Checkout complete #News at: http://goo.gl/Yomm0x #HPC /n/r Can anyone let me know , what should be carefully coordinated by an administrator when decommissioning multiple DataNodes in a cluster in the context of #BigData ?\\r\\n\\r\\nA.Immediately after decommissioning, the Web UI count for \"dead nodes\" should increase.\\r\\nB.TaskTracker should be stopped in rapid succession on all the decommissioned DataNodes.\\r\\nC.A sufficient number DataNodes should remain to provide adequate data replication.\\r\\nD.The JobTracker \"exclude\" file on the DataNodes should be read within 1 minute of the NameNode reading its \"exclude\" file. /n/r For A Newbie What Is The Best Way To Start With #BigData Technology?\\r\\nTo join this Discussion visit here: http://ow.ly/NhBhF \\r\\n#HPC #SuperComputing /n/r Are #Supercomputers Hidden Power Center Of Silicon Valley? Checkout #News at: http://goo.gl/MwT3tj \\r\\n#HPC #SuperComputing /n/r Don\\'t miss our free webinar today . Learn #BigData and Hadoop Developer course for better job opportunities with Bigger organisations on the technology front . Click here to register \\r\\nhttp://www.greycampus.com/training/big-data-hadoop-webinar /n/r #Datascience and #BIgdata \\r\\nAre you interested in the Internet of Things (IoT)?\\r\\nCome and join our Free Live Webinar where Cisco funded MobStac will talk to you about how they use Hadoop in IoT Beacon Technology.\\r\\nClick here to register - https://attendee.gotowebinar.com/register/3758402689617729025 /n/r Do Large Companies Use R For Data Analysis? Or Is it #SAS Or #Hadoop, etc.? \\r\\nJoin discussion at: http://goo.gl/TSOqvJ #BigData #HPC /n/r Which Is Better, An MS In Data Science Or A #Datacience Certification From Coursera? \\r\\nTo join discussion on this topic visit here: http://goo.gl/6dQJzc \\r\\n#HPC #SuperComputing /n/r Cloud Bigtable Launched By Google To Store #BigData ! \\r\\nCheckout complete #News at: http://goo.gl/nWy2gW #HPC #SuperComputing /n/r Stay up to date on what\\'s happening in #HPC, #CloudComputing and #BigData by subscribing to our #Newsletter at: http://hpc-asia.com/subscribe/ /n/r Stay up to date on what\\'s happening in #HPC, #CloudComputing and #BigData by subscribing to our #Newsletter at: http://hpc-asia.com/subscribe/ /n/r What Are Some #DataAnalysis Projects I Can Do As A #DataScience Beginner?\\r\\nTo Join this Discussion Visit here: http://goo.gl/FTMQX2 \\r\\n#HPC #SuperComputing /n/r Join #Discussion on this topic: \"How Is #BigData Stored? Do They Use #Databases Like Oracle? How Do They Extract Information From Them?\" at: http://goo.gl/CBCzVN /n/r Big Data has become a growing phenomenon over a period of time but what you need to know is the nature of \\'Unstructured Data\\' and how to deal with it. Still confused what it is? Get to know here.\\r\\n\\r\\nhttp://bit.ly/1E8ds6v /n/r Checkout latest #Infographic on: \"Save Money With Big Data\"! \\r\\nFor complete infographic visit here: http://goo.gl/na8Qb2 \\r\\n#BigData #HPC /n/r Understanding Dimensionality Reduction- Principal Component Analysis And Singular Value Decomposition! \\r\\nCheckout #Blog at: http://goo.gl/rjdqZS #BigData #HPC /n/r Big Data is helping the government. Research Optimus tells you how - http://www.researchoptimus.com/blog/6-ways-big-data-is-improving-government-effectiveness/ /n/r Stay up to date on what\\'s happening in #HPC, #CloudComputing and #BigData by subscribing to our #Newsletter at: http://hpc-asia.com/subscribe/ /n/r Did you know that 80% of data collected by companies is wasted?\\r\\nhttp://www.domo.com/assets/downloads/15_bi-guide.pdf /n/r Still time to register for today\\'s FREE webinar on Datameer\\'s Big Data Analytics Platform on Bigsteps Full Metal Cloud. \\r\\n\\r\\nToday,  3:00pm GMT . Register here: http://bit.ly/Datameer-on-FullMetal /n/r Hey #BigData Fans, If big data is your forte, then write an article on Big Data & Sports and participate in The Big Blogger #Contest to Get A Chance To Win INR 10,000 Gift Voucher\\r\\nClick here to participate: cwc15.hpc-asia.com/contest/ #cwc15 #sports /n/r Join tomorrow a FREE webinar about Datameer\\'s Big Data Analytics Platform on Bigsteps Full Metal Cloud. \\r\\n\\r\\nRegister here: http://bit.ly/Datameer-on-FullMetal /n/r I\\'ve been trying to learning more about business intelligence lately and found these great memes about it. I also found this new BI guide, which was super helpful. Enjoy! http://www.domo.com/assets/downloads/15_bi-guide.pdf? /n/r Hi, is there anyone here that knows something about XPath? please help me for some easy Query, Thanks /n/r Hi All, let me know if you anybody need training on Hadoop / Apache Spark. You can reach me at datamaking.consultancy@gmail.com for more details. /n/r Please register for Hadoop Online Demo Class-March-18-2015 on Mar 18, 2015 7:30 PM IST at:\\r\\n\\r\\nhttps://attendee.gotowebinar.com/register/1860620330025381890\\r\\n\\r\\nAgenda:\\r\\n----------------\\r\\nWhat is Data?\\r\\nmeasuring data?\\r\\nwhat is big data?\\r\\nhow big data is generated?\\r\\nHadoop is the solution to Big Data?\\r\\nWhy so demand for BIg Data?\\r\\nBig data jobs?\\r\\nFuture of big data?\\r\\nBig Data characteristics ? (5V\\'s)\\r\\nWhat is Hadoop ?\\r\\nPre- Requisites to learn Hadoop?\\r\\nDifference between Hadoop vs RDBMS?\\r\\nCourse Content? /n/r Register Link:\\r\\nhttps://docs.google.com/forms/d/19DwJ9hslm_UpsnilnjPNgv5xrLTSHG3pfMVPBXLtjnU/viewform /n/r Hey #cricket Fans, If big data is your forte, then write an article on Big Data & Sports and participate in The Big Blogger #Contest to Get A Chance To Win INR 10,000 Gift Voucher\\r\\nClick here to participate: cwc15.hpc-asia.com/contest/ #cwc15 #sports /n/r \"6 Rising #BigData Stars in Asia & US\" read more at: http://goo.gl/UNIKLC  #hpc #supercomputing /n/r Hey #BigData Fans, If big data is your forte, then write an article on Big Data & Sports and participate in The Big Blogger #Contest to Get A Chance To Win INR 10,000 Gift Voucher\\r\\nClick here to participate: cwc15.hpc-asia.com/contest/ #cwc15 #sports /n/r Start-up Analytics: What Should Your First Data Hire Do for You?\\r\\n\\r\\nI\\'m speaking about InstaEDU / Chegg data science at Innovation Enterprise\\'s Predictive Analytics Summit in San Diego. Registration is now open! #bigdata #analytics #datascience /n/r Top downloaded papers by #SAS Software on #Bigdata in 2014 https://lnkd.in/eUPajRF #hadoop #nosql /n/r Where can i find statistics exercises with answers ? /n/r Sign up now for b.telligent\\'s free webinar \"Better Forecasting\" in January! More info\\'s here: http://ow.ly/D9fBx /n/r Interesting breakdown of the key Data Scientist skills. \\r\\n\\r\\nWhich archetype are you? DB, DC, DD or DR? I\\'d love to get some small data from the comments below, and see what the trend is. \\r\\n\\r\\nhttp://dataconomy.com/the-22-skills-of-a-data-scientist/ /n/r Send resumes to: oacacademy@gmail.com\\r\\nOpenings for BCA or B.Sc graduates\\r\\n\\r\\nEducation: BCA or B.Sc (Computer Science/ Electronics/ Mathematics/ Physics/ Statistics / Information Technology /Information Science only)\\r\\nJob Role: Operations Executive / Testing Executive\\r\\nEligibility Criteria:\\r\\n\\x96 BCA or B.Sc graduates (Computer Science/ Electronics/ Mathematics/ Physics/ Statistics / Information Technology / Information Science\\r\\nonly)\\r\\n\\x96 Candidates who have graduated in 2008, 2009 & 2010 with experience of up to 24 months are eligible to apply\\r\\nSimple average aggregate of 60% throughout Class X, XII & Graduation.\\r\\nShould not have participated in the Infosys selection process in the last 9 months.\\r\\nShould have good communication skills.\\r\\nShould be willing to relocate and work in a 247 environment.\\r\\nSend resumes to: oacacademy@gmail.com or call at 9717244459\\r\\nhttps://www.facebook.com/groups/288324188045174/ /n/r The #DOAG today with a presentation by b.telligent. The topic: bitemporal data modeling as supreme discipline! http://ow.ly/i/7DzoA /n/r Today starts the 3-day #DOAG User Conference - also on board: b.telligent! #Oracle /n/r Send resumes to: oacacademy@gmail.com\\r\\nOpenings for BCA or B.Sc graduates\\r\\nCompany: Infosys\\r\\nEducation: BCA or B.Sc (Computer Science/ Electronics/ Mathematics/ Physics/ Statistics / Information Technology /Information Science only)\\r\\nJob Role: Operations Executive / Testing Executive\\r\\nEligibility Criteria:\\r\\n\\x96 BCA or B.Sc graduates (Computer Science/ Electronics/ Mathematics/ Physics/ Statistics / Information Technology / Information Science\\r\\nonly)\\r\\n\\x96 Candidates who have graduated in 2008, 2009 & 2010 with experience of up to 24 months are eligible to apply\\r\\nSimple average aggregate of 60% throughout Class X, XII & Graduation.\\r\\nShould not have participated in the Infosys selection process in the last 9 months.\\r\\nShould have good communication skills.\\r\\nShould be willing to relocate and work in a 247 environment.\\r\\nSend resumes to: oacacademy@gmail.com or call at 9717244459\\r\\nhttps://www.facebook.com/groups/288324188045174/ /n/r Send resumes to: oacacademy@gmail.com\\r\\nOpenings for MBA Finance- Freshers\\r\\nCompany: Infosys, HCL, Genpact\\r\\nEducation: MBA-Finance\\r\\nOther optional requirements:\\r\\n\\x96 NCFM (NSE certifications in financial markets.)\\r\\n\\x96 ICWAI certification\\\\ CFA level one certification \\r\\nJob Role: Data Management services and client support\\r\\nEligibility Criteria:\\r\\n\\x96 MBA-Finance\\r\\n\\x96 10th and 12th from CBSE board preferably\\r\\n\\x96 Cutoff: 60 percent marks in 10th and 12th. are eligible to apply\\r\\n\\x96 Should have good communication skills.\\r\\n\\x96 Should be willing to relocate and work in a 247 environment.\\r\\n\\x96 Immediate Joining required\\r\\nSend resumes to: oacacademy@gmail.com or call at 9717244459\\r\\nhttps://www.facebook.com/groups/288324188045174/ /n/r Hi All,\\r\\n                I am trying to achieve the following requirement in Pig and struck up in the middle, can you someone through inputs(tips) on this to proceed further.\\r\\n\\r\\nMy sample input data set:\\r\\nCol1,Col2,Col3,Col4\\r\\nIndia,Chennai,1,5\\r\\nIndia,Mumbai,1,3\\r\\nIndia,Bangalore,1,2\\r\\nUSA,California,1,4\\r\\nUSA,Boston,1,6\\r\\n\\r\\nMy output should be:(sum up rows value for Col3 & Col4 based on Col1)\\r\\nCol1,Col2,Col3,Col4\\r\\nIndia,Chennai,3,10\\r\\nIndia,Mumbai,3,10\\r\\nIndia,Bangalore,3,10\\r\\nUSA,California,1,4\\r\\nUSA,Boston,1,6\\r\\n\\r\\nI have tried the following steps to achieve it, I am in the middle:\\r\\n\\r\\ndb_dataset = load \\'/data/testdata.csv\\' using PigStorage(\\',\\');\\r\\n\\r\\ndb_dataset_grp = GROUP db_dataset BY $0;\\r\\n\\r\\ndb_dataset_new = FOREACH db_dataset_grp GENERATE group AS country, BagToTuple(db_dataset.$1) AS city, SUM(db_dataset.$2) as count, SUM(db_dataset.$3) as sales_amt;\\r\\n(USA,(California,Boston),2.0,10.0)\\r\\n(India,(Chennai,Mumbai,Bangalore),3.0,10.0)\\r\\n\\r\\ndb_dataset_new_split = foreach db_dataset_new generate country, TOBAG(city) as city, count, sales_amt;\\r\\n(USA,{(California,Boston)},2.0,10.0)\\r\\n(India,{(Chennai,Mumbai,Bangalore)},3.0,10.0)\\r\\n \\r\\ndb_dataset_new_split_act = foreach db_dataset_new_split generate country, flatten(city) as city; This is not transforming Tuple or Bag into rows.\\r\\n\\r\\nThanks. /n/r Send resumes to: oacacademy@gmail.com\\r\\nOpenings for BCA or B.Sc graduates\\r\\nCompany: Infosys\\r\\nEducation: BCA or B.Sc (Computer Science/ Electronics/ Mathematics/ Physics/ Statistics / Information Technology /Information Science only)\\r\\nJob Role: Operations Executive / Testing Executive\\r\\nEligibility Criteria:\\r\\n\\x96 BCA or B.Sc graduates (Computer Science/ Electronics/ Mathematics/ Physics/ Statistics / Information Technology / Information Science\\r\\nonly)\\r\\n\\x96 Candidates who have graduated in 2008, 2009 & 2010 with experience of up to 24 months are eligible to apply\\r\\nSimple average aggregate of 60% throughout Class X, XII & Graduation.\\r\\nShould not have participated in the Infosys selection process in the last 9 months.\\r\\nShould have good communication skills.\\r\\nShould be willing to relocate and work in a 247 environment.\\r\\nSend resumes to: oacacademy@gmail.com or call at 9717244459\\r\\nhttps://www.facebook.com/groups/288324188045174/ /n/r Our b.telligent experts obviously had fun at the #Swisscom Halloween party! Here are some impressions: /n/r The #PredictiveAnalytics World starts today-live with b.telligent and our #BI expert Michael Allg?wer! /n/r Send resumes to: oacacademy@gmail.com\\r\\nOpenings for BCA or B.Sc graduates\\r\\nCompany: Infosys\\r\\nEducation: BCA or B.Sc (Computer Science/ Electronics/ Mathematics/ Physics/ Statistics / Information Technology /Information Science only)\\r\\nJob Role: Operations Executive / Testing Executive\\r\\nEligibility Criteria:\\r\\n\\x96 BCA or B.Sc graduates (Computer Science/ Electronics/ Mathematics/ Physics/ Statistics / Information Technology / Information Science\\r\\nonly)\\r\\n\\x96 Candidates who have graduated in 2008, 2009 & 2010 with experience of up to 24 months are eligible to apply\\r\\nSimple average aggregate of 60% throughout Class X, XII & Graduation.\\r\\nShould not have participated in the Infosys selection process in the last 9 months.\\r\\nShould have good communication skills.\\r\\nShould be willing to relocate and work in a 247 environment.\\r\\nSend resumes to: oacacademy@gmail.com or call at 9717244459\\r\\n\\r\\nhttps://www.facebook.com/groups/288324188045174/ /n/r Send resumes to: oacacademy@gmail.com\\r\\nOpenings for BCA or B.Sc graduates\\r\\nCompany: Infosys\\r\\nEducation: BCA or B.Sc (Computer Science/ Electronics/ Mathematics/ Physics/ Statistics / Information Technology /Information Science only)\\r\\nJob Role: Operations Executive / Testing Executive\\r\\nEligibility Criteria:\\r\\n\\x96 BCA or B.Sc graduates (Computer Science/ Electronics/ Mathematics/ Physics/ Statistics / Information Technology / Information Science\\r\\nonly)\\r\\n\\x96 Candidates who have graduated in 2008, 2009 & 2010 with experience of up to 24 months are eligible to apply\\r\\nSimple average aggregate of 60% throughout Class X, XII & Graduation.\\r\\nShould not have participated in the Infosys selection process in the last 9 months.\\r\\nShould have good communication skills.\\r\\nShould be willing to relocate and work in a 247 environment.\\r\\nSend resumes to: oacacademy@gmail.com or call at 9717244459\\r\\n\\r\\nhttps://www.facebook.com/groups/288324188045174/ /n/r Please join my group for current opening for BPO & KPO\\r\\n\\r\\nOff Campus Placement Cell - NCR ( Delhi ) BPO - KPO \\r\\n\\r\\nhttps://www.facebook.com/groups/288324188045174/ /n/r What do you think if OLS does not fulfil assumption of normality of residuals in large sample data with n=600, does it give acceptable results in the field of finance and accounting? /n/r Now the top job company b.telligent also receives the certificate #berufundfamilie. Voices can be found here: /n/r ????? ????? ???? ????? ????????? ??????? ?????\\r\\n????? ????????\\r\\nhttps://www.youtube.com/watch?v=9RXzrtUcgXw /n/r Anyone here interested in working on Economic Graphs ? /n/r Rackspace adds #BigData-capable servers to OnMetal Service - http://bit.ly/1waoTcH /n/r Platfora\\'s #BigData iceberg and other stories - http://bit.ly/101iYLk /n/r The internal b.telligent University has started again. Today with the theme presentation training. We are excited! http://ow.ly/i/7erSW /n/r I\\'m giving a data science talk at Occidental College on Friday.  If you know people who are interested in transitioning from academia to tech send them my way:\\r\\n\\r\\nWhat: Physicist to Data Scientist\\r\\nTime: October 17th, 4:00pm\\r\\nLocation: Hameetman 124, Occidental College /n/r #BigData Can Guess Who You Are Based on Your #ZipCode - http://bit.ly/1sPPHyc /n/r #Salesforce Makes Its #BigData Move - http://bit.ly/1yxh4QA /n/r #BigData needs Big Brains - http://bit.ly/1vpycXx /n/r Intel execs on #BigData and privacy: It\\'s a balancing act - http://bit.ly/1CZPF9V /n/r Peter Thiel: #MobileComputing, #BigData, and edu software nothing but buzzwords - http://bit.ly/1vM6Vw7 /n/r Fortune-Tellers, Step Aside: #BigData Looks For Future Entrepreneurs - http://bit.ly/1vL1NsU /n/r How #BigData is fueling a new Age in Space Exploration - http://bit.ly/1vIaXqa /n/r Join Free Live Demo On Big Data and Hadoop\\r\\nEnroll Now - http://goo.gl/DgPzDB /n/r Join Free Live Demo Session On \"Big Data and Hadoop\"\\r\\nRegister Now! -  http://goo.gl/AH9oMJ /n/r How the #BigDataRevolution can help #DesignIdealCities - http://bit.ly/1vfyU78 /n/r Register Your Self for \"Big Data and Hadoop Training\"   \\r\\nEnroll Now - http://goo.gl/D5yJwL /n/r #BigData in Marketing: How to gain the advantage - http://bit.ly/ZB2skH /n/r Haha ;)\\r\\nData Analytics Server is less complicated than girls?\\r\\n\\r\\nLike This Page- www.facebook.com/LetsTalkAnalytics /n/r #Hadoopers Come On...Answer this Simple Question..\\r\\n\\r\\nLike This Interesting #Analytics Page- www.facebook.com/LetsTalkAnalytics /n/r Become Big Data and Hadoop Expert\\r\\nEnroll Now For free Live Demo - goo.gl/2mnACf /n/r Fighting discrimination- with #BigData - http://bit.ly/1sdQLY1 /n/r #BigData breakthrough to boost #ChildProtectionServices - http://bit.ly/1sUFuzO /n/r Live Webinar On Big Data and Hadoop Today 6:30 IST\\r\\n\\r\\nEnroll Now - http://goo.gl/7usV75 /n/r Hello frnds... pls... Like & Share dis Broucher Photo ..on this #vvpISTE2014 Event Page... :-) /n/r The Breakthrough Technology That Will Turbocharge #BigData And #CloudComputing\\r\\nhttp://bit.ly/1sFO8SS /n/r How ClearStory offers #BigData blending for easy Insights - http://bit.ly/WQON72\\r\\n#BigDataBlendingForEasyInsights /n/r Free Webinar on \"Big Data and Hadoop\" Today \\r\\nRegister Now - http://goo.gl/UU3t4E /n/r #BigData \\x97 the aggregation and analysis of #DataToOptimize performance \\x97 is transforming modern society. Sport, which has always served as a reflection of society, is no exception. While calculations have been employed since the dawn of athletics, the latest innovations in data are poised to trigger a revolution - http://bit.ly/1lViTC9 /n/r Join Big Data and #Hadoop Live Webinar at Easylearning Guru\\r\\nEnroll Now - http://goo.gl/DbW23p /n/r What do you do when a girl kindly recommends you a Free O\\'Reilly  NoSQL webcast which will be live tomorrow?:) You sign up:)\\r\\nYou will get the chance to listen to Ben Lorica and find interesting things about how Impala, Elasticsearch & Couchbase performs when scaled vertically and horizontally.\\r\\nJoin for FREE: http://oreil.ly/VH1VvK /n/r What do you do when a girl kindly recommends you an O\\'Reilly Free NoSQL webcast which will be live tomorrow?:) You sign up:)\\r\\n\\r\\nYou will get the chance to listen to Ben Lorica and find interesting things about how Impala, Elasticsearch & Couchbase performs when scaled vertically and horizontally.\\r\\n\\r\\nJoin for FREE: http://oreil.ly/VH1VvK /n/r About EBM? /n/r Unicom Learning - Globally renowned leading knowledge sharing and conference organizers in emerging technology and processes (Agile, Project Management, Testing, Programming, Big Data, Hadoop, Cloud, Mobility, Internet of Things, UXUI Design, Quality, Social, Analytics and Security) is organizing a conference on Big Data dated 13th August in Sydney and 15th August in Melbourne, Australia. This is a unique opportunity for all executives and professionals involved in data storage, data management and data analysis to gather and discuss how companies can effectively manage, protect and leverage the growing amounts of data in the enterprise. With a focus on best practices, our events allow attendees to explore strategies and technologies surrounding real-time data processing, data protection and privacy, meeting industry regulations and compliance, and data storage and provide you the opportunity to meet Big Data experts, attend keynote presentation, and participate in workshop and interactive sessions and network with experienced Business and Technical practitioners.\\r\\n\\r\\nFor any more queries kindly contact us on contact@unicomlearning.com\\r\\nFor registrations kindly visit the link:\\r\\nSydney->http://bit.ly/1nwjH0f\\r\\nMelbourne->http://bit.ly/1p527Me /n/r Online SAS course\\r\\n\\r\\nFree DEMO on:- 01.07.2014\\r\\n\\r\\nRegister Now at \\x96pragnaeduvercity@gmail.com                 \\r\\n\\r\\nWe offers high grade online courses in base SAS certification,Advance sas certification,Clinical SAS,Finance SAS,SAS BI and DI with projects.\\r\\n\\r\\nLearning the courses online saves time as well as students from different geographical locations can participate in the training session.\\r\\n\\r\\nWe provide assistance for fresher to enter into the professional field of Clinical SAS, Finance SAS and SAS BI as well to the professionals who wish to achieve higher goals in their career.\\r\\n\\r\\nFor more details please contact -  neha.eduvercity@gmail.com\\r\\n                                                      pragnaeduvercity@gmail.com                                  \\r\\n\\r\\nPhone- 08050922145 /n/r Hi, Here is an awesome course to Nail R Programming. Subscribe to the channel for future lesson updates.\\r\\n#rstats #datascience #statistics\\r\\n\\r\\nhttp://goo.gl/oy0CCb /n/r My company has some great Big Data positions to fill here in Austin. Please let me know if you are interested! :) /n/r #Oracle Unveils #MySQLFabric http://lnkd.in/bM5heJS #noql #bigdata #hadoop /n/r Get a free ipad mini! /n/r NOUVEAU DOSSIER SPECIAL :\\r\\n\"Big Data et Ressources Humaines\"\\r\\n\\r\\n-> http://www.myrhline.com/dossier-rh/big-data-et-ressources-humaines /n/r NOUVEAU DOSSIER SPECIAL :\\r\\n\"Big Data et Ressources Humaines\"\\r\\n\\r\\n-> http://www.myrhline.com/dossier-rh/big-data-et-ressources-humaines /n/r I am compiling a list of free resources for data science, data viz, and big data... Here is what I have so far. Do you have anything to add to the list? If so, will you please add it to the comments section of this note? :) I would so appreciate it and I am sure others would too. <3 /n/r #Splunk\\'s Hunk 6.1 enables faster #analytics for #Hadoop and #NoSQL Data Stores   http://lnkd.in/bkPkpXT #bigdata /n/r Get the insider scoop on big data and data science in the government sector...\\r\\n\\r\\nA Brazen Expose on Big Data in the Government Sector\\r\\nhttp://www.data-mania.com/index.php/easyblog/170-a-brazen-expose-on-big-data-in-the-government-sector /n/r The program is directed by Prof. Dr. Ram?n Reichert, whose new book will appear soon: http://www.amazon.de/Big-Data-Analysen.../dp/3837625923. WEBSITE DATA STUDIES: http://www.donau-uni.ac.at/de/studium/data-studies/index.php /n/r Are you a US data scientist, advanced #data analyst or statistician? \\r\\n\\r\\nIf so, you can GET PAID $150 FOR COMPLETING A 2 HR SURVEY. \\r\\n\\r\\nPing me for details #bigdata /n/r Using #Scala To Work With #Hadoop http://lnkd.in/bQnned5 #Cloudera #bigdata #nosql #HDFS #MapR /n/r Data exhaust created today is humongous and the need to understand its potential has become more of a necessity than just an interest ! Lets explore /n/r privacy preserving in big data, has been major issue for now, please discuss regarding dat too /n/r #MongoDB #NoSQL Database Interview Questions http://lnkd.in/b7SJy7d #hadoop #Mapreduce #Hive #Cassandra, #CouchDB /n/r *Making Pretty Data Visualizations with Python*\\r\\n\\r\\nMy latest post for AstroBetter shows you how to make a 2D density plot with histograms projected along each axis.\\r\\n\\r\\nRead more at AstroBetter:\\r\\nhttp://www.astrobetter.com/visualization-fun-with-python-2d-histogram-with-1d-histograms-on-axes/\\r\\n\\r\\n#bigdata #visualizations #python /n/r I\\'m giving an #astronomy to #datascience #career #talk today at UC Berkeley.  Come on down:  \\r\\n\\r\\nAstronomy Career Development Seminar\\r\\nTuesday (12/17) at 5pm in HFA B1. /n/r 10 Days executive SAS BASE CERTIFICATION training in Bangalore (1st Step towards Big Data & Business Intelligence Carrier)\\r\\n\\r\\nObjective of training: \\r\\n\\r\\nBase SAS certification training \\r\\nEquipped with hands on experience with SAS\\r\\nClear and strong concepts for Big Data and BI \\r\\n\\r\\nDuration & Format of the Course: \\r\\nFAST TRACK (10 Days (30 Hrs)) \\r\\nFrom 15th Dec to 25th Dec \\r\\n\\r\\nTake away \\r\\n* Practice Software \\x96 limited edition \\r\\n* Guide for SAS Certification \\x96 Hard Copy \\r\\n* Assignment Copy \\x96 Hard copy \\r\\n* Course material \\x96 Soft Copy \\r\\n* Training Notes \\x96 Soft Copy \\r\\n* Exam preparation questions \\x96 Soft Copy \\r\\n\\r\\nFees for Training Program \\r\\n* 12,000 + 12.36% Taxes \\r\\n\\r\\nWho can pursue SAS Training & Certification course. \\r\\nAs SAS is an analytical tool this can be pursuit by any Graduates & PGs students from any discipline. (MBAs, Commerce, Engineering Life sciences, Arts, Medicine, MCAs Statistics etc.). \\r\\nSAS is also for the professionals who are already working and wanted to make a good carrier move in their respective domain. \\r\\n\\r\\nAddress:\\r\\n e2e Projects Pvt. Ltd\\r\\n No: 182, 1st Floor, 13th Main, 7th Cross\\r\\n Sector 5, HSR Layout, Bangalore \\r\\nLandmark: Behind Devi Eye Hospital, beside Prakash Dental Hospital, Near BDA Bridge.\\r\\n\\r\\n Contact Person :Neha (08050922145)\\r\\n\\r\\n Mail ID : Neha.j@eduvercity.com\\r\\n\\r\\n Time : 10Am to 1 Pm /n/r IJOURNALS\\r\\nCALL FOR PAPERS\\r\\nVolume 1 Issue 4\\r\\nInternational Journal of software & Hardware Research in Engineering(ISSN: 2347-4890)\\r\\nSubmission Deadline: 10 December\\r\\nPublication Date: 15 December\\r\\nwww.ijshre.com\\r\\n\\r\\nVolume 1 Issue 1\\r\\nInternational Journal of Social Relevance & Concern(ISSN: APPLIED)\\r\\nsubmission Deadline: 30 November\\r\\nPublication Date: 1 December\\r\\nSubmit your mauscript at: ijshre@gmail.com or editor@ijournals.in\\r\\nWebsite: www.ijournals.in /n/r Is your boss talking about big data? Here\\'s two impressive bits of information related to analytics and processing that you can share (written by yours truly): http://ar.gy/4wTl /n/r Xplenty\\'s Code-free Hadoop as a service Platform Enables Companies of all Sizes to Utilize their Big Data Without Prior Programming Knowledge.\\r\\n\\r\\nhttp://online.wsj.com/article/PR-CO-20130417-911716.html /n/r Anyone interested in working with OSM to make the better place today? /n/r Are you on Twitter?? If so, show the love!! Please vote for me by Tweeting the following :) - I would REALLY appreciate it!!\\r\\n\\r\\n\"I nominate @BigDataGal for the #bigdata100 most influential #bigdataTwitter accounts...\" #opendata #bigdata /n/r Thank you all so much for supporting my Facebook Page!! I am working on turning it into a forum or portal by which data lovers can meet, greet & collaborate....  I just need a little custom development work to get the features built. Hopefully it would not take long. /n/r I just invited Mandi Bishop to this group - she will be perf here.  Hey Mandi, should we invite the other mythical unicorns... probably, huh...\\r\\n\\r\\nI am not on FB with them though /n/r Thank you for the invite and add, Lillian Pierson! Look forward to joining in the conversation. /n/r Thanks for the invite and for the add!!! /n/r '"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "female_text = \"\"\n",
    "for i in range(len(data_female)):\n",
    "    female_text = female_text + data_female[i] + \" /n/r \"  \n",
    "female_text   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "143516"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(female_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "K.set_image_dim_ordering('th')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Cleaning the data**\n",
    "\n",
    "In this part we remove unwanted characters from the text we created. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'mohammed nazili suddicqui\\'s post advertising a payment gateway is removed for a second time.  third instance will result in the member being removed from the group. /n/r new member post advertising a payment gateway has been removed. /n/r this is amazing! /n/r we need 20 volunteers to go with us to daytona beach florida for the nascar fundraiser! all expense paid! /n/r all presented by alabama stem education! it will definitely move you to greatness? /n/r we may be older than we thought. /n/r geostorm is a real thing on mars. how well is spacex prepared for this problem? /n/r i need to interview two helping service professionals from two different settings (i.e. school, hospital, or prison), one of which must be a clinical psychologist, by this weekend.\\nanyone here fitting the description who could help? /n/r any flat earthers in this group who\\'d love a discussion?? /n/r forgive me i couldn\\'t resist. /n/r amazing mirrorless car camera announced by mitsubishi:\\nhttp://www.techhound.org/auto-tech/amazing-mirrorless-car-camera-announced-by-mitsubishi/ /n/r galaxy note 8 special edition designed by samsung for 2018 winter olympics!\\nhttp://www.techhound.org/gadgets/galaxy-note-8-special-edition-designed-samsung-2018-winter-olympics/ /n/r new vision of a hypersonic spy aircraft presented by boeing!\\nhttp://www.techhound.org/auto-tech/new-vision-hypersonic-spy-aircraft-presented-boeing/ /n/r nissan announced its self-driving imx concept ev at ces!\\nhttp://www.techhound.org/auto-tech/nissan-announced-self-driving-imx-concept-ev-ces/ /n/r this is an awesome way to teach young men in a different atmosphere and they do it together as 1! all they need to do is submit an essay about their career goals!! /n/r vampire hunting accessories from the 19th century. /n/r colorful arachnids (y) /n/r advertising post deleted and member advised. /n/r remember rocky, bulwinkle and the gang?\\nwith my thanks to robert coates /n/r a little sci-tech humor for the first week of the new year. ;) (reposted) /n/r the global #cloudmigration market to grow from usd 1961.44 million in 2016 to usd 8678.73 million by 2023, at a compound annual growth rate (cagr) of 23.67%. the year 2016 has been considered as the base year, while the forecast period is up-to 2023. click here http://bit.ly/2czxzq2 for more details. /n/r 2nd annual global women\\'s march denver - january 20, 2018 ... spread the word! /n/r to all our wonderful members,\\na happy new year /n/r science and society. admins please let me know if i have crossed a line here. /n/r science, or the lack thereof. /n/r i have just finished reading a book called: the water will come rising seas,sinking cities,and the remaking of the civilized world. it is by author jeff goodell   i learned alot of things i thought i knew were not exactly the way they are but i also learned about some technology advancements in different areas.  i would highly recommend this book to everyone to read.   i would also like to know if anyone in this group is working on or trying to stop flooding in their areas or part of group trying to help relocate those who are in prone areas. /n/r what are your insights about the future? comment down. /n/r my thanks to ravi for sharing. /n/r i saw a similar graphic but thought it could be done a bit better (including the republican elephant shitting out the crappy idea that the cdc can\\'t use these words anymore). feel free to share my graphic. /n/r one post removed for solicitation and member has been warned. /n/r nuff said... /n/r one member blocked from the group and comments deleted due to beligerence, rudeness, incivility and unacceptable posts. /n/r one comment deleted due to inappropriate language. a reminder, this is a discussion group and we ask, and expect, your responses and comments to show respect, civility, and acceptable language. /n/r one reported post deleted for trivial content and member advised we do not play games. /n/r 5 point full-proof guide to respond to your online reviews.\\nhttp://bit.ly/2auo9tc /n/r with so many cancer break-throughs, can we honestly believe there is no cure? this science technology page should know the most about how technology has sky rocketed in the past decades. cancer research and treatments are making millions of dollars for the pharmaceutical companies, but still no cure? does anybody have any insight on why? or is there anybody that believes the cancer cure is being hidden for some reason? looking for any information on \"proof\" there is a cancer cure. /n/r comment deleted due to unacceptable material, and failure to respond to moderator\\'s request.\\na reminder to our members, a moderator will only request your action if there is a need such as language, sexual content, etc., etc. in a comment or a post. it is our job to see the rules are followed, and appreciate your cooperation, however not cooperating will result in the offensive language or content being removed. if ever in doubt about what is or isn\\'t acceptable, please take the the time to contact the admins of this group.  we are always available to help or answer your questions. /n/r what a blessing!! /n/r pay it forward!! /n/r the media no longer presents objective facts. almost every bit of news circulating through social media are little more than opinion pieces at best, and intentional attempts to mislead and sway public opinion at its worst. hyperbole and cherry picked \"facts\" dominate the news cycles, and are used to whip up tensions and further divide our society. \\n\\nhttp://fair.org/take-action-now/media-activism-kit/how-to-detect-bias-in-news-media/ /n/r post deleted for advocating violence and member warned and advised of group\\'s rules regarding such posting. /n/r science - technology - society !! /n/r let\\'s get our children acclimated to stem education! give to alabama stem education for the holidays! great tax wright off?? /n/r that\\'s amazing! /n/r it is the need of the hour....\\nawake ....india...awake...\\nsave ur  daughters..or\\nu will be a nation without mother... /n/r our free online course \"biodiversity and global change: science & action\" is going to be running for the second-to-last time starting monday (nov 27). you can learn more about it here: https://www.coursera.org/learn/biodiversity/\\nand watch our trailer here:\\nhttps://www.youtube.com/watch?v=u7nqn3ktw7q\\nplease share or like this post! /n/r one post deleted for unverified content and member informed.  please remember no religious themed posts are allowed, and please verify your quoted material before posting.  if you are not certain, or have questions please contact an admin who will be glad to assist you. /n/r this bleeds over to all colleges, all fields. the control committees are in place. is your job at risk? /n/r one post deleted for non-appropiate content.  if members do not have active messenger, we have no way to advise you. /n/r why did we divided \\nwhat we do not have multiplied /n/r happy holidays! /n/r ?? live for ur self ?? /n/r word is that net neutrality ends dec.14. /n/r what a stemtastic fall festival ????? we had a great time ???? /n/r oracle code bogot? y buenos aires !\\n\\nsi eres desarrollador y te interesa adem?s ser speaker en los eventos #oraclecode, aprovecha esta oportunidad y envia lo antes posible tu presentaci?n a trav?s de este formulario: \\n\\nhttps://lnkd.in/dmikahz /n/r #blackfridaydeal this is the best offer i found so far. grab professional courses at flat 80%off.  use bf80. https://goo.gl/qxetgu /n/r hello hyderabad,\\nnow iimse started bigdata hadoop class room training in hyderabad by our corporate trainer mr. narendra (8+ exp)\\nonly 5 students per batch, training includes hadoop development, admin, intro to spark and scala \\nfor more details contact us @+91- 9133463111 info@iimse.com /n/r hi all. have anyone considered doing a research (in analytics, dm, ml etc) instead of a post graduate program in analytics ? i am thinking of this option. i am working right now. can we do both side by side ? will it be manageable ? if there is someone who is already doing this, i would like to get in touch with you. please help. /n/r don\\'t forget to like our facebook page ?\\n? https://www.facebook.com/couthonconseil/\\n\\njoins us on twitter ?\\n? https://www.twitter.com/couthonconseil \\n\\nfollow us on linkedin ?\\n? https://www.linkedin.com/m/company/10107092/ /n/r join our 48 hour hackathon on a moving train starting from london, traveling to brighton, cambridge, paris, bordeaux and returning to london for pitches!!!\\n\\nmore information: hackathon.hacktrain.com\\n\\nhttps://www.facebook.com/events/248309182349938/ /n/r transform your organization with powerful analytics. learn how from industry experts from qlik, idc, cloudera and more in this free virtual talkshow on 14 nov: http://bit.ly/vywtalkshow /n/r hello guys,\\n\\nif anybody is interested to provide training in r programming, please inbox or comment.\\ntraining mode : online /n/r visit spotinst at serverlessconf nyc\\nspotinst ceo amiram shachar will talk about multi-#cloud functions as a service & #serverless strategy. /n/r promptcloud, a leading web scraping service company, launched a community forum where like minded people, who work with data, can connect and share knowledge & ideas about web data extraction with each other. it\\'s an open and friendly community where people can ask questions, share tips and discuss all things web scraping. you can join the community here - https://webscrapingforum.promptcloud.com/ /n/r learn mis & bi with tableau with specially crafted courses for you, with techandmate! choose experts for training and save a huge amount.\\nget trained and get assured placement\\n#techandmate\\n#bookyourfreedemohere https://www.techandmate.com/request_your_demo /n/r hello everyone,\\nif anybody is interested to provide training of excel and vba in hyderabad location ..\\nplease inbox or comment here. /n/r exclusive data series - power bi program by microsoft faculty only on millionlights.\\nclick here to watch now : http://www.yupptv.in/#!/play/millionlights-tv\\nyupptv indiamillionlights - nayi shiksha - nayi soch\\n#mobiletv #learning #microsoft #skilling #jobs /n/r a data analyst from bangalore looking for opportunities, his notice period is 2 months , can mail requirements to veena@nichetalent.in\\n\\na bright talented and self motivated data analyst  who has excellent organizational skills, is highly efficient \\nand has a good eye for detail. has extensive experience of assisting in the development and upgrading of database \\nsystems and analytical techniques. looking for a good opportunity as data analyst \\n\\nskills includes: r, python, tableau, sql, analytics, sap bo, stats \\neducation : mca /n/r don\\'t forget to like our facebook page ?\\n? https://www.facebook.com/couthonconseil/\\n\\njoins us on twitter ?\\n? https://www.twitter.com/couthonconseil \\n\\nfollow us on linkedin ?\\n? https://www.linkedin.com/m/company/10107092/ /n/r are you a forex or stock exchange player? get our ready made forecasts for eur/usd and dax - over 90% accuracy! \\nget 7 days free trial at www.exmetrix.com #exmetrix /n/r require to setup hadoop framework on my system with windows10. plz suggest any free vm service to practice and learn. /n/r v edicion - octubre 2017, preinscripciones abiertas. m?ster en estad?stica aplicada con r software: t?cnicas cl?sicas, robustas, avanzadas y multivariantes. + info: http://bit.ly/2wnnx6j /n/r #travelokaph is looking for cool guys like you! travel with us!\\nwhy join us? simple, we prioritize our people over anything, and we call it #peoplefirst culture. we spoil and pamper our people to ensure that they are empowered. not enough reason? what about if we tell you that you\\'ll have these benefits?:\\n> business travel opportunities\\n> free lunch\\n> free afternoon snacks\\n> free coffee/tea all day long\\n> cool offices all over asia\\n> health insurance covering the employee and 4 of his or her dependents\\n> international culture\\n> get the chance to work with some of the brightest people on earth! we have a lot of people from harvard, wharton, oxford!\\n> and many more!\\n\\nanalyst vacancies:\\n- pricing and data analyst  \\nhttp://smrtr.io/tubjzw\\n\\n- promotions analyst/associate\\nhttp://smrtr.io/kncz2g\\n\\n- quality assurance analyst   \\nhttp://smrtr.io/5utyvg /n/r #travelokaph is looking for cool guys like you! travel with us!\\nwhy join us? simple, we prioritize our people over anything, and we call it #peoplefirst culture. we spoil and pamper our people to ensure that they are empowered. not enough reason? what about if we tell you that you\\'ll have these benefits?:\\n> business travel opportunities\\n> free lunch\\n> free afternoon snacks\\n> free coffee/tea all day long\\n> cool offices all over asia\\n> health insurance covering the employee and 4 of his or her dependents\\n> international culture\\n> get the chance to work with some of the brightest people on earth! we have a lot of people from harvard, wharton, oxford!\\n> and many more\\n\\nurgent vacancies:\\n- senior product marketing manager / ph marketing head \\nhttp://smrtr.io/ekistg\\n\\n- customer excellence / vendor management supervisor \\nhttp://smrtr.io/ch3wmw\\n\\n- ticketing agents \\nhttp://smrtr.io/ch3wmw /n/r want to be a part of the #serverless #multicloud world trend? spotinst next meetup is for you!\\nspotinsttinst & serverless are joining forces - next meetup. august 30th @ wework, 600 california st, san francisco\\nrsvp > http://bit.ly/2watmdv /n/r don\\'t forget to like our facebook page ?\\n? https://www.facebook.com/couthonconseil/\\n\\njoins us on twitter ?\\n? https://www.twitter.com/couthonconseil \\n\\nfollow us on linkedin ?\\n? https://www.linkedin.com/m/company/10107092/ /n/r don\\'t forget to like our facebook page ?\\n? https://www.facebook.com/couthonconseil/\\n\\njoins us on twitter ?\\n? https://www.twitter.com/couthonconseil \\n\\nfollow us on linkedin ?\\n? https://www.linkedin.com/m/company/10107092/ /n/r check your basics in #bigdata with this quick free practice test: http://bit.ly/2n9spqi /n/r big data developer@ bangalore 1-3 yrs experience strong knowledge & exp. in event modelling, designing in streaming & lambda architectures bigdata analytics stack - hadoop, spark, storm, hbase, yarn, zookeeper; query tools like hive, pig python/scala/go/java/perl machine learning models, data mining mail resume to veena@nichetalent.in\\nmail resume with proper subject line with below details \\ntotal exp\\nskills \\ncurrent ctc\\nexpected ctc\\nnotice period \\ncurrent location \\nveena , niche talent , please mail me to  veena@nichetalent.in ,  +91 9483526863\\nhttps://www.linkedin.com/in/veenarsrrecruiter/ /n/r looking for data scientist @ bangalore with 2- 6 yrs experience develop data and programming specifications - leverage complex data algorithms and numerical methods - know stat techniques, machine learning & algos mail resume to veena@nichetalent.in\\nmail resume with proper subject line with below details \\ntotal exp\\nskills \\ncurrent ctc\\nexpected ctc\\nnotice period \\ncurrent location \\nveena , niche talent , please mail me to  veena@nichetalent.in ,  +91 9483526863\\nhttps://www.linkedin.com/in/veenarsrrecruiter/ /n/r big data developer@ bangalore \\n1-3 yrs experience \\nstrong knowledge & exp. in\\nevent modelling, designing in streaming &lambda architectures\\nbigdata analytics stack - hadoop, spark, storm, hbase, yarn, zookeeper; query tools like hive, pig\\npython/scala/go/java/perl\\nmachine learning models, data mining\\n\\nmail resume to veena@nichetalent.in /n/r hi, i\\'m the marketing manager at spotinst\\nfollowing several, global and successful events we are arriving to nyc!\\nspotinsttinst next meetup together with #aws on august 11th at the #aws ny loft.\\nlearn how to leverage the use of #ec2 spot instance for production workloads #devops #cloud \\nreg > http://bit.ly/2u9nasr /n/r hey, i\\'m the marketing manager at www.spotinst.com\\ni would like to invite you to to like spotinst page to get our awesome content, and event updates.\\nour next meetup is only one days away! aug 2nd, bay area #sfbay.\\ntogether with rancher labs and jobvite find out how to leverage #aws #spot instances usage, maintain business continuity and application availability.\\ncut #cloud costs, optimize #container and #app performance and meet cool people. #devops \\nreg > http://bit.ly/2u9dacj /n/r want a secured job in it sector? do an online, self paced - data science course by microsoft for free. to register leave your email id in the comment box (10,436 registrations done in 10 days) /n/r #datascience workflow.\\nregister for querying with transact-sql now!\\nclick here : www.millionlights.co.in/registration /n/r #testimonial #millionlights #datascienceorientation #microsoft\\nwww.millionlights.org /n/r \"big data: visi?n general\", num?rica resumiendo, no. 7\\nhttps://goo.gl/czmdy2  \\n#bigdata #niid #niidresumiendo #datascience #dataartist #iot /n/r i am searching for a freelance analytics engineer for an immediate project at dkv, the boat private health insurance company in brussels, belgium.\\n\\nthe freelancer needs to be a real crack in programming in microsoft powerbi, and is very good in r and sql.  good analytical skills are considered a plus. \\n\\nproject should ideally stay next monday and may take from 3 weeks to 3-5 months, depending on the chemistry with the team.\\n\\nonly people who can start on minimally 2 days per week will be considered, and with a ramp up to 4-5 days a week with 4-6 weeks.\\n\\npermanent job opening available too.\\n\\nif you are interested then please pm your proof cases of experience and your daily rate /n/r cluster analysis using sas and r\\ncluster analysis is the bread and butter of marketing. it has many aspects like \\n\\x95 hierarchical clustering, \\n\\x95 non-hierarchical cluster (k means clustering) and many lingo associated with the same like dendogram, scree plot, euclidean distance, simple linkage, wards method etc. \\n\\nthe tutorial on clustering  is one of the easiest way to master these concepts. this explains cluster analysis using sas along with various options of sas, working of hierarchical and non-hierarchical algorithms, step by step explanation of sas output, differentiation with objective segmentation techniques etc.\\nit also explains the \\n\\x95 clustering using r\\n\\x95 cluster analysis guideline for data mining scenario\\n\\x95 quiz and assignment (along with model solution for assignment)\\nlink \\x96 https://www.udemy.com/cluster-analysis-motivation-theory-practical-application/?couponcode=sp_ca_01 /n/r don\\'t forget to like our facebook page ?\\n? https://www.facebook.com/couthonconseil/\\n\\njoins us on twitter ?\\n? https://www.twitter.com/couthonconseil \\n\\nfollow us on linkedin ?\\n? https://www.linkedin.com/m/company/10107092/ /n/r don\\'t forget to like our facebook page ?\\n? https://www.facebook.com/couthonconseil/\\n\\njoins us on twitter ?\\n? https://www.twitter.com/couthonconseil \\n\\nfollow us on linkedin ?\\n? https://www.linkedin.com/m/company/10107092/ /n/r ultimate course on principal component and factor analysis using sas and r\\nthe course explains one of the important aspect of machine learning - principal component analysis and factor analysis in a very easy to understand manner. it explains theory as well as demonstrates how to use sas and r for the purpose. \\n\\nthe course provides entire course content available to download in pdf format, data set and code files. the detail course content is as follows.\\n\\nintuitive understanding of pca 2d case\\n1.what is the variance in the data in different dimensions?\\n2. what is principal component?\\nformal definition of pcs\\n1. understand the formal definition of pca\\n2. properties of principal components\\n3. understanding principal component analysis (pca) definition using a 3d image\\nproperties of principal components\\n1. summarize pca concepts\\n2. understand why first eigen value is bigger than second, second is bigger than third and so on\\ndata treatment for conducting pca\\n1. how to treat ordinal variables?\\n2. how to treat numeric variables?\\nconduct pca using sas: understand\\n1. correlation matrix\\n2. eigen value table\\n3. scree plot\\nhow many pricipal components one should keep?\\nhow is principal components getting derived?\\n1. conduct pca using r\\nintroduction to factor analysis\\n1. introduction to factor analysis\\n2. factor analysis vs pca side by side\\n3. factor analysis using r\\n4. factor analysis using sas\\ntheory for using pca for variable selection\\ndemo of using pca for variable selection\\nhttps://www.udemy.com/principal-component-analysis-pca-and-factor-analysis/ /n/r sas programming from scratch\\nlearn sas programming from industrial usage perspective. sas is one of the most used tool for data science, analytics and statistical analysis domain. this course is designed to give you good start on sas programming quickly. the course has following highlights\\n\\x95 getting free access to sas\\n\\x95 importing data\\n\\x95 getting basic feel of data- through contents, print, freq, univariate \\n\\x95 data operations like merge, append, sort, truncate \\x96 rows / columns wise, \\n\\x95 derive new fields, \\n\\x95 analysis for each class \\n\\x95 graphs \\x96 vertical bar chart / pie charts / stacked charts \\n\\x95 statistical procedure\\n\\x95 title and labels for enhancements\\n\\x95 tabulate for pivot table kind of work\\nhttps://www.udemy.com/sas-programming-made-easy-with-examples/?couponcode=sp_st_1 /n/r an overview of the most popular anomaly detection algorithms for time series and their pros and cons. /n/r ultimate course on basic statistics using excel-\\nstatistics has been probably one of the most difficult course to master across the globe for mba students. it is all because it is being explained in a wrong way. across the globe most of the faulty are not been able to simplify it for students, even if they know it in great detail. \\nthe right way of learning statistics is when one can see the demonstration of concepts through simulation of a concept and then go through the theoretical concept. the tutorial statistics by example explains statistics by simulation. the tutorial covers\\n\\x95 probability and expectations\\n\\x95 central tendencies and dispersion\\n\\x95 central limit theorem\\n\\x95 sampling distribution \\n\\x95 hypothesis testing \\n\\x95 linear regression\\n\\x95 categorical data analysis and chi square test of independence\\n\\x95 anova\\n\\x95 non parametric tests\\nlink \\x96  https://www.udemy.com/statistics-by-example/?couponcode=sp_sts_01 /n/r want to get started with sas?\\nread our recent article \\'things which you should be familiar about sas before you gather knowledge regarding the software\\'. http://bit.ly/2sw1k7w\\n\\n#sas #onlineprofessionalcertificationcourse #sascertificationcourse #bigdataandanalyticscertificationcourse /n/r artificial neural network made super easy https://www.udemy.com/artificial-neural-networks-tutorial-theory-applications/?couponcode=sp_01_ann /n/r ultimate course on logistic regression -\\npredictive analytics : credit scoring - logistic regression using sas\\nlogistic regression is one of the most used technique in the industry for credit scoring. the technique is applicable in predicting risk (who will default), response (who will take offer), collectability (who will make payment of delinquent balance) etc. sas is a global leader for developing predictive models. \\nthe tutorial on credit scoring is one of the most authentic source to learn logistic regression modelling (credit scoring) using sas for industrial application. it has helped many students to master the subjects. many students testimony are available for viewing.\\nthis explains \\n\\x95 econometrics / statistical theory, \\n\\x95 sas program detail, \\n\\x95 step by step data workout and \\n\\x95 deep dive into sas output. \\nthe tutorial is quite complete in the sense that it explains every step by model building like model design, data audit guideline, variable selection (categorical and numeric), multi collinearity removal, final model development, power of a model (ks / gini / concordance) as well as reject inference\\nlink \\x96 https://www.udemy.com/logistic-regression-credit-scoring-modelling-using-sas/?couponcode=sp_lrs_01 /n/r get ready to boost your career in big data with hive. \\nhttps://www.youtube.com/watch?v=9ncjbqhkzsi\\n\\nthis tutorial explains how hive can be used for big data processing and is a part of series of video-based e-learning content offered by schoalrspro to learn various big data technologies. \\nto know more about big data bundle self-paced e-learning, write into us at info@scholarspro.com.\\n\\n#hive #bigdata #hiveforbigdataprocessing #bigdataandanalyticscertificationcourses #onlineprofessionalcertificationcourses /n/r predictive analytics : classification and regression tree (decision tree)  using r\\ndecision tree is one of the easiest technique of machine learning that yields tremendous benefit to the business. it has several aspects like \\n\\x95 how to develop the decision tree, \\n\\x95 how to interpret the output, \\n\\x95 how to design data for modeling?\\n\\x95 algorithm to understand how it\\'s working, \\n\\x95 how to understand it\\'s business benefit?\\n\\x95 assignment, quiz to fortify your learning\\nthe tutorial explains how to develop decision tree using r, what is gini / entropy / cart / chaid / random forest method etc. students across the globe has found it to be complete tutorial for learning decision tree \\nlink \\x96 https://www.udemy.com/decision-tree-theory-application-and-modeling-using-r/?couponcode=sp_dt_01 /n/r it\\'s time to uplift your career and get certified in the most leading career skill.\\nread our recent blog \"evolve out by enhancing yourself with sas certification\" at http://bit.ly/2rwy5jv.\\n\\n#sas #sascertification #onlineprofessionalcertificationcourses /n/r logistic regression using r (workshop)\\nlogistic regression is one of the most used technique in the industry for credit scoring. the technique is applicable in predicting risk (who will default), response (who will take offer), collectability (who will make payment of delinquent balance) etc. \\nthe workshop tutorial doesn\\'t have details on theory. it has \\n\\n\\x95 r program detail, \\n\\x95 step by step data workout and \\n\\x95 deep dive into r output. \\nthe tutorial is for someone who quickly wants to develop credit scoring models using r. it gives the syntax as well as details of interpretation. \\nlink - https://www.udemy.com/logistic-regression-workshop-using-r-step-by-step-modeling/?couponcode=sp_lrr_01 /n/r looking for sas online training?\\nthings you have a duty to know about sas online training.. http://bit.ly/2qv4ona\\n\\n#sas #sasonlinetraining #onlineprofessionalcertificationcourses #bigdataandanalyticscertificationcourses /n/r a very prestigious worldwide credential is the sas certified advanced programmer. sas base programming exam and sas advanced programming exam are the two exams which a person needs to pass out to get this certificate.\\nfind out the way of successfully obtaining good results in sas certification exam..\\nhttps://scholarspro.tumblr.com/post/160911147369/the-way-of-successfully-obtaining-good-results-in?is_highlighted_post=1\\n\\n#sascertification #sas #advancedsas #basesas #sascertifiedadvancedprogrammer #sascertificationexam #onlineprofessionalcertificationcourses /n/r a study revealed that 83% of career professionals accounted that receiving an sas certification helped them get a promotion, and 66% reported earning an sas certification assisted them to get a hike in salary.\\n\\ndo you know sas certified professionals are considering the analytics? http://bit.ly/2qfnuxv.\\n\\n#sas #sascertifiedprofessionals #analytics /n/r do you know big data usually plays a big role in uplifting total enactments of a company? \\nhttp://bit.ly/2qow20e\\n\\n#bigdata /n/r don\\'t forget to like our facebook page ?\\n? https://www.facebook.com/couthonconseil/\\n\\njoins us on twitter ?\\n? https://www.twitter.com/couthonconseil \\n\\nfollow us on linkedin ?\\n? https://www.linkedin.com/m/company/10107092/ /n/r don\\'t forget to like our facebook page ?\\n? https://www.facebook.com/couthonconseil/\\n\\njoins us on twitter ?\\n? https://www.twitter.com/couthonconseil \\n\\nfollow us on linkedin ?\\n? https://www.linkedin.com/m/company/10107092/ /n/r shape your data with #logpacker, smart service for log collection and analysis. get the idea of the #startup at https://www.producthunt.com/ /n/r cluster analysis using sas and r\\ncluster analysis is the bread and butter of marketing. it has many aspects like \\n\\x95 hierarchical clustering, \\n\\x95 non-hierarchical cluster (k means clustering) and many lingo associated with the same like dendogram, scree plot, euclidean distance, simple linkage, wards method etc. \\n\\nthe tutorial on clustering  is one of the easiest way to master these concepts. this explains cluster analysis using sas along with various options of sas, working of hierarchical and non-hierarchical algorithms, step by step explanation of sas output, differentiation with objective segmentation techniques etc.\\nit also explains the \\n\\x95 clustering using r\\n\\x95 cluster analysis guideline for data mining scenario\\n\\x95 quiz and assignment (along with model solution for assignment)\\nlink \\x96 https://www.udemy.com/cluster-analysis-motivation-theory-practical-application/?couponcode=sp_ca_01 /n/r ultimate course in basic statistics\\nstatistics has been probably one of the most difficult course to master across the globe for mba students. it is all because it is being explained in a wrong way. across the globe most of the faulty are not been able to simplify it for students, even if they know it in great detail. \\nthe right way of learning statistics is when one can see the demonstration of concepts through simulation of a concept and then go through the theoretical concept. the tutorial statistics by example explains statistics by simulation. the tutorial covers\\n\\x95 probability and expectations\\n\\x95 central tendencies and dispersion\\n\\x95 central limit theorem\\n\\x95 sampling distribution \\n\\x95 hypothesis testing \\n\\x95 linear regression\\n\\x95 categorical data analysis and chi square test of independence\\n\\x95 anova\\n\\x95 non parametric tests\\nlink \\x96  https://www.udemy.com/statistics-by-example/?couponcode=sp_sts_01 /n/r what do you wanna learn? just send your query at info@uplatz.com or call us at +44 7836212635 or visit our page to know more https://www.uplatz.com. /n/r don\\'t forget to like our facebook page ?\\n? https://www.facebook.com/couthonconseil/\\n\\njoins us on twitter ?\\n? https://www.twitter.com/couthonconseil \\n\\nfollow us on linkedin ?\\n? https://www.linkedin.com/m/company/10107092/ /n/r predictive analytics : classification and regression tree (decision tree)  using r\\ndecision tree is one of the easiest technique of machine learning that yields tremendous benefit to the business. it has several aspects like \\n\\x95 how to develop the decision tree, \\n\\x95 how to interpret the output, \\n\\x95 how to design data for modeling?\\n\\x95 algorithm to understand how it\\'s working, \\n\\x95 how to understand it\\'s business benefit?\\n\\x95 assignment, quiz to fortify your learning\\nthe tutorial explains how to develop decision tree using r, what is gini / entropy / cart / chaid / random forest method etc. students across the globe has found it to be complete tutorial for learning decision tree \\nlink \\x96 https://www.udemy.com/decision-tree-theory-application-and-modeling-using-r/?couponcode=sp_dt_01 /n/r don\\'t forget to like our facebook page ?\\n? https://www.facebook.com/couthonconseil/\\n\\njoins us on twitter ?\\n? https://www.twitter.com/couthonconseil /n/r where is a #career in #hadoop headed? hurry! #promotional #offer - use code ap1712jb10 and get 10% #discount http://pos.li/db janbasktraining /n/r hello all,\\ndo you know any expert who can teach on microsoft excel reporting and analytics (little bit of statistics background is added advantage) at a nominal cost during weekends. . . someone living close to frazer town/banasawadi vicinity.\\nthank you for your attention to this request.\\nimportant note: send only personal messages with contact details, address and landmark.\\nthnx /n/r check your basics in #bigdata with this quick free practice test: http://bit.ly/2n9spqi /n/r looking for sr data scientist with  2-3 yrs  exp skills of  machine learning , python , nltk , open nlp , r (optional), weka (optional) ,  hadoop , spark , data mining (mandate), text mining (mandate) , textual anaysis (mandate), should be strong in algo and ds, \\n\\nlocation : bangalore\\n\\nmail resume to veena.r@practical-methods.com /n/r grab the chance because we have lift off! https://www.facebook.com/events/283944272064088/\\n#janbask #studyatjanbask #learn #win #launch #website /n/r i\\'m looking for collaborators to help me with regular data science/journalism projects around a dataset i own. \\n\\ni\\'m the founder of sapio (getsapio.com), a dating app that i launched about 6 months ago. our sapio users answer open-ended questions, and have so far answered nearly 600,000 times. these answers have user metadata attached (age/gender/race/orientation/location/etc.) but are otherwise anonymized. i\\'m looking to partner with someone who would be interested in working with me on telling interesting stories using our data. i have a small budget, so i\\'m not looking for something for free, but it isnt much and was hoping to find someone who would be interested in exclusive access to this data to help me tell some really compelling stories that couldnt otherwise be told. if any of you are interested, please email me at kristin@frac.tl   thanks! /n/r the tutorial below helps you learn r programming with comfort by working on data. it explains all the concepts through data workout rather than explaining you by the methods of lingo like vector, metrics etc. it is being like by students as this is quite concise and gives you confidence that you can easily learn it.\\nlink \\x96 https://www.udemy.com/introduction-to-r-programming-learn-r-syntax-by-example/?couponcode=sp_rp_01\\n /n/r excel though used by many folks but rarely people know that one can do pair t test, anova, linear optimization, linear regression, pareto analysis etc. using excel. \\nmost of the tutorial on advance excel across the globe just explains pivot table. \\nthe tutorial on advance analysis using excel explain data analysis toolpak, solver and goal seek. it is quite concise yet sufficiently good to help one master application of microsoft excel for advance analytics. this enhances one\\'s ability to use excel for many statistical analysis and optimization purpose. \\nlink \\x96 https://www.udemy.com/data-analysis-statistical-optimization-using-ms-excel/?couponcode=sp_adex_01\\n /n/r cluster analysis using sas and r\\ncluster analysis is the bread and butter of marketing. it has many aspects like \\n\\x95\\thierarchical clustering, \\n\\x95\\tnon-hierarchical cluster (k means clustering) and many lingo associated with the same like dendogram, scree plot, euclidean distance, simple linkage, wards method etc. \\n\\nthe tutorial on clustering  is one of the easiest way to master these concepts. this explains cluster analysis using sas along with various options of sas, working of hierarchical and non-hierarchical algorithms, step by step explanation of sas output, differentiation with objective segmentation techniques etc.\\nit also explains the \\n\\x95\\tclustering using r\\n\\x95\\tcluster analysis guideline for data mining scenario\\n\\x95\\tquiz and assignment (along with model solution for assignment)\\nlink \\x96 https://www.udemy.com/cluster-analysis-motivation-theory-practical-application/?couponcode=sp_ca_01\\n /n/r this is an open invitation for everyone looking to specialise in the booming fields of internet-of-things, digital security, mobile computing systems, and/or data science. join this webinar and take the opportunity here: https://goo.gl/eixz37 /n/r learn #bigdata and #hadoop..!!\\n\\nattend the free live webinar on 18th march and learn bigdata & hadoop.\\nwebinar\\'s timing: \\n07:00 am pst \\n07:30 pm ist\\nregister now at https://goo.gl/aty0yj /n/r can i have a solution? /n/r check your basics in #bigdata with this quick free practice test: http://bit.ly/2n9spqi /n/r learn #bigdata and #hadoop..!!\\n\\nattend the demo session for free by our industry expert prashant s.r and take a closer look in the world of hadoop and bigdata. register now at https://goo.gl/jprhbz /n/r #internationalwomensday #iwd2017 /n/r looking for an affordable master in artificial intelligence and computer science in europe? join online open day and learn everything about this unique program at the west university of timisoara ! save your place here: https://goo.gl/iqu2o3 /n/r free live #webinar...!!\\nlearn #bigdata and #hadoop from an industry expert. attend the first session for free. register now at https://goo.gl/abhz3x /n/r big data and wrestling? wwe uses data to guide success www.linkedin.com/hp/update/6241942955503034369 /n/r what is the best laptop for a beginner in data science practice? :) /n/r scholarships available! study data and computer science with a scholarship in italy! join our online open day and find out everything about this 2 year programme from the university of genoa!  get your scholarship here: https://goo.gl/hgkzfp /n/r big data: why nasa can now visualize its lessons learned www.linkedin.com/hp/update/6240114050919952384/ /n/r shri a.k.mittal, dept. of comm. & it (goi)\\nhere he talks about iot and m2m\\nand using it to transform supply chain logistics\\nwatch complete talk: https://goo.gl/rufbq2 /n/r big news! we have released logpacker unity plugin that collects logs in one centralized dashboard for you. https://logpacker.com/registration?key1=fb /n/r get hands on experience with first project based hadoop training! attend the first session for free! click here - https://www.dezyre.com/free-webinar/project-based-hadoop-training/120 /n/r supply chain in business using iot:\\nmr. bhabajit nandi: supply chain security manager, microsoft devices group \\ncol. ashwani sindhwani: national security manager, dhl express india\\nfind out what they have to say: https://goo.gl/l0dwcw /n/r we let you acquire the business intelligence through company profiling. check out our work. #thefinansol #companyprofile #microsoft #windows /n/r are you looking for practice questions for #bigdata? here we have it for you for #free.  http://bit.ly/2kschfm /n/r hey guys! for those of you who want to meet talented, like-minded people and build innovative solutions over the weekend in london, we\\'re hosting a 48-hour hackathon for department for transport from the 24th to the 26th of march! \\n\\nwe really would love for data scientists to attend as our lead of contact for department for transport is a data scientist with his own data science meetups so he has his favorites. :p\\n\\nwe\\'ll have loads of datasets and challenges for you guys to play with for sure! \\n\\nif you\\'re interested in the event, just register at register.dfthacks.com!\\n\\nhave a good day :)\\n\\nhttps://www.facebook.com/events/240781796368113/ /n/r digital health conference, london 2017 - call for papers deadline extended to february 20th!\\nwww.acm-digitalhealth.org\\nin partnership with university college london, irdr /n/r call for papers deadline february 13th!\\ndigital health conference with special focus on big data and digital health in disaster, emergency and humanitarian contexts, london july 2017\\nwww.acm-digitalhealth.org\\nin partnership with university college london, irdr /n/r hello dear members, i am currently a student from frankfurt university of applied science have the task to find out why specially the 21-must-know data science interview questions and answers are important for a data mining interview.\\n\\ni have edit this question \"q15. explain edward tufte\\'s concept of \"chart junk.\" and understood it. \\ndoes anyone have an idea why this question could be important or why greory platsky have choose this question ? i will be very thanksfull for your ideas and informations. /n/r looking for a #datascience training with an industry expert? join the live #online training and get trained by the best trainer. click here to join the first session for #free - https://www.dezyre.com/free-webinar/attending-the-first-session-of-data-science-for-free-/119 /n/r hello guys,\\ni have a question regarding using pca sklearn.\\n                           -\\nif i have a (n, 512) matrix \\nwhere (n) is a number of images, and each 512 is a feature vector\\nwhat happens if i do the following?\\n pca = pca(512 ,whiten=true)\\n pca.fit(feats)\\n\\ni see examples to define pca with 2 components to see the variance in the points such in this figure.\\nbut why to define 512 components? what is the point to do it this way? /n/r  - posting on behalf of friend  -\\nhello sir/madam,\\ndo you know any data analyst/ data scientist who are willing to teach fundamentals of statistics and data analytics using advanced excel or r/ power bi? \\n+ if not, do you know anyone who can coach 1st puc and 2nd puc statistics subject.\\npreferred location in and around frazer town or banasawadi surroundings.\\n -no institute references please -\\ntia /n/r  - posting on behalf of friend  -\\nhello sir/madam,\\ndo you know any data analyst/ data scientist who are willing to teach fundamentals of statistics and data analytics using advanced excel or r/ power bi? \\n+ if not, do you know anyone who can coach 1st puc and 2nd puc statistics subject.\\npreferred location in and around frazer town or banasawadi surroundings.\\n -no institute references please -\\ntia /n/r free #live webinar..!!!\\n\\njoin the webinar and discover everything about #datascience. \\n\\nwebinar\\'s date & time: jan-28-2017 at 07:00 am pst \\n\\nregister now at https://goo.gl/ppxra6 /n/r \"data is the new science. #bigdata holds the answers.\" \\x96 pat gelsinger\\n\\nenroll now to learn the science of #data. register yourself at https://goo.gl/z4xvw1\\n\\np.s. - attend the first session for #free.. hurry up..!!! /n/r #live webinar on #hadoop\\nattend the first two live sessions for #free! \\nregister now at https://goo.gl/mm7ag7 /n/r - seattle data scientists - \\nif there are any apache spark enthusiasts here, galvanize now gives you 50% off intro to spark for ds weekend workshop. ? ? \\ncheck it out or share with a friend.\\nthanks! :) /n/r live #hadoop project based #training. attend the first session for free. register now at https://goo.gl/nnrllu /n/r #hackerday - live online session\\n\\nwork on an online interactive #hadoop project. register for this free live #webinar now. webinar\\'s date & time: jan-14-2017 at 07:00 am (pst), jan-14-2017 at 08:30 pm (ist)\\n\\nregistration link : https://goo.gl/kccrjj /n/r plz statistics pg project title tell me no /n/r we invite you to participate in the 3rd international conference \"interdisciplinary cultural group research: youth subcultures, worldviews and lifestyles\" . furthermore, members of subcultures express themselves and communicate with each other not only in the \"real\" spaces such as streets, bars, concerts and festivals but also in \"virtual\" space: social networks (like facebook, twitter, instagram and others), blogs, websites and etc.therefore, this conference aims to bring together scholars from diverse disciplines to share their achievements in applying not only traditional research methods but also computer tools in their work.\\nhttps://www.facebook.com/events/1254715614612506/ /n/r keep up with the big data trends: http://buff.ly/2ius2gc /n/r anyone from the eu here is interested in job in germany? big data, hadoop /n/r i am tcs - hr, join to my group and know recent recruitment news as well as carrier enhancement info on regular basis. request you to share this link in your friend circle.\\nlink - https://www.facebook.com/groups/1866862883544761/ /n/r dai viet group is recruting :\\njob title : data mining analyst description \\njob description\\n- provide expertise in data analytics, data cluster and data classification;\\n- collect data from different sources and create datasets and databases;\\n- research and build models for data mining, text mining, recommendation system and machine learning;\\n- establish and optimize data structure;\\nrequirements\\n- minimum 1 year experience in data mining, statistic programing and modeling at large scales;\\n- being proficient in any of the following languages: r, python, java, c/c++, scala.\\n- solid background in recommendation algorithms, experience in machine learning and text mining\\n- proven experience in large scale data analytics;\\n- fluent english, toeic above 600 (or equivalent);\\n- background in computer science, statistics, mathematics or a related quantitative discipline.\\n- \\npreference\\n- master degree in computer science, applied statistics, mathematics\\n- experience using language processing systems, such as word segmentation, automatic summarization, pos tagging, ner, topic modeling, sentiment analysis\\x85\\n- exposure to search, analytics base\\nexperience working with systems such as hadoop eco system, spark, cassandra, neo4j\\x85\\nlet contact via 0902.360.586 (ms.th??ng) to know more information about this position. /n/r #webinar on #data science\\nattend the first session for #free on #datascience by our expert pradeepta mishra. register now at https://goo.gl/a1yjyt /n/r job description\\n\\nskill sets : solid knowledge of several modeling and learning techniques including statistical predictive and prescriptive analytics, optimization, machine learning, data mining, ai, simulation, social network analysis, churn prediction, segmentation analysis, text analysis and streaming data analytics. being able to work with big data using tools such as python.\\nresponsibilities : hands on responsibility as part of the team engaged in data analytics and solving critical business problems using data. build demonstrable high performing models which can be taken to larger audience within the organization.\\npersonal attributes : leadership capability, self-motivated, exceptional analytical and problem solving skills and ability to present convincingly to technical and non-technical audience.\\n\\nskills/competencies required\\n\\nmust  have a deep understanding of  python , excel , sql queries, big data, (matlab, spss or sas - advantage)  \\nstrong english,reading, writing and speaking.\\nmust have a masters degree in operations research, applied statistics, machine learning or a related quantitative discipline\\nmust have a deep understanding of statistical and predictive modeling concepts, machine-learning approaches, clustering and classification techniques, and recommendation and optimization algorithms.\\n3 years of experience delivering world-class data science outcomes.\\nhave a keen desire to solve business problems, and live to find patterns and insights within structured and unstructured data\\nable to propose analytics strategies and solutions that challenge and expand the thinking of everyone around you\\n\\n**please mention your salary expectations /n/r \"combinations of data\" \\x96 that\\'s a thing now? get ahead of the top bi trends of 2017 on jan 19: http://bit.ly/2hsecql /n/r i am tcs - hr, join to my group and know recent recruitment news as well as carrier enhancement info on regular basis. request you to share this link in your friend circle.\\nlink - https://www.facebook.com/groups/1866862883544761/ /n/r i\\'m waiting for an amd manager approval so i can send an offer letter to neural network architect /n/r manage your purchase #orders more efficiently and find new ways to #save and handle procurement http://bit.ly/2dr7kw2 /n/r learn job oriented  data science training from real-time expects with 100% job assurance.... https://goo.gl/dqqwo4 \\n\\nenroll now. special offer begins... hurry up /n/r operate with #speed, #agility and responsiveness with a complete #retail #software suite http://bit.ly/29f5yw6 /n/r are you tired of managing your #machinery instead of your #business? http://bit.ly/2atkcof /n/r reasons to adopt a #pos #software http://bit.ly/29f5yw6 /n/r multi-channel #retail #erp solution starts here http://bit.ly/29f5yw6 /n/r looking for analytics insights for your sales teams? join lenovo and qlik webinar on dec 14 to see how scaling self-service analytics impacts sales decision-making http://bit.ly/2h8onfk /n/r for beginners how easy to learn #big #data & #hadoop #training\\n\\nvisit @ https://goo.gl/ccpyxf /n/r say goodbye to the headaches of maintaining #on-premise #erp http://bit.ly/29q4ee1 #growthhacking /n/r the only #business #intelligence #software embedded in #erp http://bit.ly/2bssngl /n/r asia\\'s leading get-together of big data & analytics professionals across industries. confirmed speakers are from organisations such as dbs bank, jll, credit suisse, spotify, gsk, a*star, unilever, bp & many others. /n/r #tuesdaytips #socialmedia #usercomments #socialinteraction http://bit.ly/29q4ee1 /n/r do you really want to know about data science and big data? \\nsave your date and visit http://datascienceweekend.id/\\n\\ndon\\'t miss it!!\\nso many data scientist ??? /n/r my son is seeking an entry level data analytics position. his resume is attached. he would prefer to work in florida or the southeastern us. does anybody know of entry level positions available? /n/r don\\'t overlook the need for #ecommerce #erp integration http://bit.ly/29f5yw6 /n/r the only #software that integrates and supports #complex processes in #real-time in the #logistics industry http://bit.ly/2gjjb21 /n/r wish to learn big data & hadoop? hurry!\\n\\ncheckout offers specially crafted for you. new batches starting soon. the course curriculum is in line with industry requirements & curated by industry experts themselves!\\nenroll for the live & interactive online course today from industry experts.\\ninfo@itobj.com\\n408-63-itobj\\n:) :) :d :d /n/r just checking if the group members would be interested in the following data projects for auction: \\n\\nproject list :\\n1. sf bay area homelessness - project #2\\n2. health record and sequenced genome correlation  analysis\\n3. care in old age - long term care insurance keep or drop?\\n4. san francisco real estate value forecasting\\n5. sf bay area homelessness - project 3: analysis strategies to evaluate point in time homeless counts and homeless projects\\n\\nwe are actually on an early launch of a big data marketplace based in san francisco. these projects are not a recruitment for an office-based job, but rather for a community-based collaboration projects. \\n\\nyou can check the project details here: https://goo.gl/jktk1a /n/r great opportunity for you to change your domain to #bigdata the booming technology to boost your career.\\ndsri has launched weekend classes for big data analytics.\\nwe train from scratch to the advance technology. \\nget a chance to work on live projects use cases with industrial experts.\\n*******limited seat\\ntraining on : hadoop, mapreduce, hive, pig, sqoop, flume, hbase, rhadoop, data analytics and data modeling using r programming.\\neach topic is covered with real-time use cases\\nfor more details please give us call mob:9620622125/9620622390/9538093274\\nemail-id: info@dsresearch.in, admissions@dsresearch.in\\nvenue details:\\n#14, 2nd floor srinidhi building( above apollo pharmacy)\\njambu savari dinne /n/r great opportunity for you to change your domain to #bigdata the booming technology to boost your career.\\ndsri has launched weekend classes for big data analytics.\\nwe train from scratch to the advance technology. \\nget a chance to work on live projects use cases with industrial experts.\\n*******limited seat\\ntraining on : hadoop, mapreduce, hive, pig, sqoop, flume, hbase, rhadoop, data analytics and data modeling using r programming.\\neach topic is covered with real-time use cases\\nfor more details please give us call mob:9620622125/9620622390/9538093274\\nemail-id: info@dsresearch.in, admissions@dsresearch.in\\nvenue details:\\n#14, 2nd floor srinidhi building( above apollo pharmacy)\\njambu savari dinne circle, jp nagar 8th phase\\nbangalore-560076 /n/r great opportunity for you to change your domain to #bigdata the booming technology to boost your career.\\ndsri has launched weekend classes for big data analytics.\\nwe train from scratch to the advance technology. \\nget a chance to work on live projects use cases with industrial experts.\\n*******limited seat\\ntraining on : hadoop, mapreduce, hive, pig, sqoop, flume, hbase, rhadoop, data analytics and data modeling using r programming.\\neach topic is covered with real-time use cases\\nfor more details please give us call mob:9620622125/9620622390/9538093274\\nemail-id: info@dsresearch.in, admissions@dsresearch.in\\nvenue details:\\n#14, 2nd floor srinidhi building( above apollo pharmacy)\\njambu savari dinne circle, jp nagar 8th phase\\nbangalore-560076 /n/r great opportunity for you to change your domain to #bigdata the booming technology to boost your career.\\ndsri has launched weekend classes for big data analytics.\\nwe train from scratch to the advance technology. \\nget a chance to work on live projects use cases with industrial experts.\\n*******limited seat\\ntraining on : hadoop, mapreduce, hive, pig, sqoop, flume, hbase, rhadoop, data analytics and data modeling using r programming.\\neach topic is covered with real-time use cases\\nfor more details please give us call mob:9620622125/9620622390/9538093274\\nemail-id: info@dsresearch.in, admissions@dsresearch.in\\nvenue details:\\n#14, 2nd floor srinidhi building( above apollo pharmacy)\\njambu savari dinne circle, jp nagar 8th phase\\nbangalore-560076 /n/r great opportunity for you to change your domain to #bigdata the booming technology to boost your career.\\ndsri has launched weekend classes for big data analytics.\\nwe train from scratch to the advance technology. \\nget a chance to work on live projects use cases with industrial experts.\\n*******limited seat\\ntraining on : hadoop, mapreduce, hive, pig, sqoop, flume, hbase, rhadoop, data analytics and data modeling using r programming.\\neach topic is covered with real-time use cases\\nfor more details please give us call mob:9620622125/9620622390/9538093274\\nemail-id: info@dsresearch.in, admissions@dsresearch.in\\nwebsite: www.dsresearch.in\\nvenue details:\\n#14, 2nd floor srinidhi building( above apollo pharmacy)\\njambu savari dinne circle, jp nagar 8th phase\\nbangalore-560076 /n/r great opportunity for you to change your domain to #bigdata the booming technology to boost your career.\\ndsri has launched weekend classes for big data analytics.\\nwe train from scratch to the advance technology. \\nget a chance to work on live projects use cases with industrial experts.\\n*******limited seat\\ntraining on : hadoop, mapreduce, hive, pig, sqoop, flume, hbase, rhadoop, data analytics and data modeling using r programming.\\neach topic is covered with real-time use cases\\nfor more details please give us call mob:9620622125/9620622390/9538093274\\nemail-id: info@dsresearch.in, admissions@dsresearch.in\\nvenue details:\\n#14, 2nd floor srinidhi building( above apollo pharmacy)\\njambu savari dinne circle, jp nagar 8th phase\\nbangalore-560076 /n/r almost 200 registrations! don\\'t miss out on the hottest monthly event in town! #ai & the #city in #amsterdam #cityhall tomorrow! free tickets amsai1.eventbrite.com #smartcity #techcity #amstechcity #govtech #artificialintelligence #techtrends /n/r are you looking for:\\n- comprehensive data science training?\\n- data science & analytic project in region?\\n- data and tech professional and talent for your big data project?\\n\\ndata science indonesia (dsi) have invited > 30 data&tech professional to train-advocate you on big data tech-analytic:\\n- cto cloudera : amr awadallah\\n- head of google analytic - solution enggineer lead, google cloud\\n- adrianus hitijahubessy\\n- william tjhi phd\\n- dr. karl ng : director of innovation capital (mdec)\\n- nadia alatas (cybertrend)\\n\\nget early bird ticket now: datascienceweekend.id/ticketing/ /n/r problems using data mining  to build regression models,part 2\\nby jim frost (penn state university) \\nplease share with students and researchers\\n\\nhttp://blog.minitab.com/blog/adventures-in-statistics/problems-using-data-mining-to-build-regression-models-part-two /n/r we are looking for a 4+ year experienced freelancer candidate to conduct online training on hadoop. if anybody interested please. contact us : email at jyoti.rajput425@gmail.com /n/r i would love your feedback!! in exchange, i\\'m offering a chance to win 1 of 2 $50 amazon gift cards :) http://www.data-mania.com/blog/data-science-training-paradise-beyond/ /n/r hi! this is the official account of #datascienceweekend, a festive weekend of data science brought to you by datascience.or.id and invisio statistika uii which will be held on december 3rd - 5th 2016 in yogyakarta .\\n\\nat this great #datascienceweekend,\\nwe want you to see the linkage between data science and actionable insight.\\n\\n at this great #datascienceweekend,\\nwe want you to listen more about the story about how a data grow into an invaluable decision.\\n\\nat this great #datascienceweekend,\\nyou will find imagination, cognitive approaches, creative process as connectors between data & insight.\\n\\nthe great #datascienceweekend is the time when data meets creativity! \\n\\nit\\'s creativitime!!\\nwww.datascienceweekend.id /n/r #tagmycollege invites you to a free webinar to help you decide whether you should opt for an #mba or pgdm.\\nanyone interested, comment or left your email id and mobile no /n/r ?#?tagmycollege? invites you to a free webinar to help you decide whether you should opt for an ?#?mba? or ?#?pgdm?.\\ninterested to join, then comment or left your email id /n/r does anyone know a good visualizer for postgresql (like periscopedata or modeanalytics?) /n/r understanding analysis of variance (anova) and the f-test\\nby jim frost (penn state university)\\n \\njim publishes new statistics articles every 2 weeks for minitab\\nplease share with students and colleagues:\\n \\nhttp://blog.minitab.com/blog/adventures-in-statistics/understanding-analysis-of-variance-anova-and-the-f-te /n/r join free webinar on scaling & sharding of data\\nregister now- http://goo.gl/extm4c /n/r join few hours left to join this free webinar to learn \"implementing #machinelearning algorithm on #twitterdata (with r).\" \\nregister now- http://goo.gl/fehnsl /n/r top 10 don\\'ts of #bigdata projects: http://bit.ly/1qcjhkm /n/r join live webinar of face detection using hadoop mapreduce framework\\nregister now-http://goo.gl/ckrmjo\\ntoday 20th april 2016 at 6:30pm /n/r best way to analyze likert item data: two sample t-test versus mann-whitney   by jim frost (penn state university)\\n\\njim publishes new statistics articles every 2 weeks for minitab\\nplease share with students and colleagues:\\n\\nhttp://blog.minitab.com/blog/adventures-in-statistics/best-way-to-analyze-likert-item-data%3a-two-sample-t-test-versus-mann-whitney /n/r big data has already become the big way of staying agile with a strong competitive edge. read the article to be prepared for big data\\'s tomorrow by understanding all new regulations & predictions:  http://bit.ly/216kliu /n/r as per gartner\\'s report, \\'for the fourth year, tableau is a proven leader in the magic quadrant for business intelligence and analytics platforms report\\'. read the full report below. \\n\\ngartner\\'s report on tableau - http://tabsoft.co/1ofkbkv\\n\\ntableau is recognized for the expansion of the range of data source connectivity, an increase in the analytical depth of the tool, the flexible choice between server and online data interaction, and the high variety of use-cases it can be deployed against. below are some of the remarkable tutorials which teaches and explains tableau, its features and benefits in detail, enough to make you a tableau ninja\\n\\nlevel of detail expressions                 - http://bit.ly/2183o0p\\namazing features in tableau 9.0             - http://bit.ly/1tnas2a\\ncreating square choropleth map in tableau   - http://bit.ly/1onkgai\\nhow to create calendar heat maps in tableau - http://bit.ly/1kp1trr\\nnetwork diagram using path shelf in tableau - http://bit.ly/1kvlofw\\nconnecting to ms access database in tableau - http://bit.ly/1kvlt2u /n/r is #bigdata still a thing? the big data landscape in 2016. full image: http://bit.ly/1kftm7w /n/r it\\'s forecasted that big data will touch the whopping $88 billion mark by 2021. read how #bdaas will play a role: http://bit.ly/1nxbldk /n/r do u know how u can turn big data initiatives into large-sized profits? here are 5 tips that can help your market growth:  http://bit.ly/1ktir55 /n/r do you know that big data can become central to your digital strategy? see how big data can deliver massive business outcomes for a wide range for corporate goals:  http://bit.ly/20avx5u /n/r there\\'s a lot that has been written with regards to hadoop1.0 & #hadoop 2.0. take a quick look at their main features and the differences that exist between the two: http://bit.ly/1sgmlkl /n/r did you know that 53% of 1,217 companies had undertaken at least one big data initiative? check out why you should get certified: http://bit.ly/1t8r5nb /n/r the checklist of big data best practices requires small-scale investments in specialized skills. read more: http://bit.ly/1nfm9du /n/r how big is big data project? now how do big data problems look like? read more at: http://bit.ly/1spv8ji /n/r check out this #infographic about how to become a #bigdata developer.  http://bit.ly/1nd2mez /n/r india ideathon 3.0 promotes the spirit of entrepreneurship. funds upto rs 1,00,000, for student\\'s/ professional\\'s ideas/projects. \\nhave a brilliant idea? wanna make it real? take it to the next level with us.\\nregister now: http://bit.ly/1qzvktx\\ndeadline: 10th january, 2016 /n/r need advice-for my son,,\\ndeciding between actuaries and data science analyst. he is bsc in maths from mumbai university-jaihind college. not astrong computerscience background. can u suggest??\\ni feel actuaries is a great line however tough and very long -15 exams .\\nif he considers data analytics, can u suggest any great where he can doe the course from if u think he is eligible ? /n/r hello! i work for a media platform in berlin that focuses on data science and i am looking to talk with anyone from the big data community that knows the scene in tel aviv or they are based in tel aviv, israel - please, pm.:) thanks! /n/r just think of social media messages going viral in minutes, the speed at which credit card transactions are checked for\\nhttps://www.linkedin.com/pulse/when-comes-monetization-all-data-created-equal-vivek-upadhyay-1?trk=pulse_spock-articles /n/r i need synthetic data streams are\\ngenerated by using ibm synthetic data generator proposed\\nby agrawal and srikant (1994). two synthetic data\\nstreams, denoted by t5.i4.d1000k and t15.i6.d1000k.\\ncan you help me?? /n/r to celebrate the first day of data natives, on 19th of november we are throwing a kick ass party for all you data lovers out there, at platoon kunsthalle in mitte, from 9.30 till the early morning. and it\\'s free of charge!\\n\\nso make sure you reserve a slot on eventbrite and come along to party like a data native! whoop whoop...or should i say 0010101101?!\\n\\ncheck the event link for more info! /n/r a note on data visualisation:\\nhttp://anjusthoughts.blogspot.in/2015/10/data-visualization.html /n/r introduction of database and database management system (dbms)\\n\\nhttp://dwbimaster.com/data-warehouse/introduction-of-database-and-database-management-system-dbms/ /n/r modern data warehouse webinar - turning the corner: what it takes to build a modern data warehouse\\n\\nmodern data warehouse roundtable webcast - 15th october\\n\\nregister: http://tinyurl.com/o2lvc87\\n\\nif necessity is the mother of all invention, then it\\'s no wonder the technology space moves so fast. from the consumerization of it to the steady emergence of innovative solutions, organizations often struggle to stay afloat in the sea of options. as we march steady to the pace of moore\\'s law, analysts and business users expect quicker and easier access to enterprise data, and the data warehouse of yore must modernize to meet those demands. \\n\\nregister for this roundtable webcast with analyst david loshin, dwaine snow of ibm and heine krog iversen of timextender. this expert panel will discuss the market forces that brought data warehousing to its current state and the key factors driving today\\'s innovation. they will explore how new data types and sources are raising analytic expectations and changing the way enterprises design information architectures. /n/r how to become a #datascientist for free https://lnkd.in/bxdktms #bigdata #java /n/r what does it means this code (with scala on apache spark)\\n\\ndef hash(rdd: rdd[string]): rdd[vector] = {\\nval doc=rdd.map(line => line.split(\",\").toseq)\\n      val hashingtf = new hashingtf()\\n      val tf = hashingtf.transform(doc)\\n      tf.foreach(println) \\n      val idf = new idf().fit(tf)\\n      idf.transform(tf)\\n} /n/r #bigdata as a service #market worth 7.0 billion usd by 2020! \\ncheckout #article at: http://goo.gl/mi5ao9\\n#datascience #datascientist /n/r what are the key traits of a #datascientist?\\nclick here to know the answer: http://goo.gl/15xfnz \\n#datascience #bigdata /n/r what are the types of #virtulization ? \\ncheckout complete #infographic at: http://goo.gl/vckqo7 \\n#datacenter #bigdata /n/r the term #bigdata and #apachehadoop goes hand in hand. aimed at storing and processing big data, hadoop has core components that have individual functions. furthermore, hadoop can also be combined with multiple applications for beneficial outcomes. \\n http://www.greycampus.com/blog/big-data/all-you-need-to-know-about-hadoop /n/r who are the most notable and influential #datascientists? \\nto join this #discussion visit here: http://goo.gl/bvp4e1 \\n#bigdata #datascience /n/r c?mo analizar grandes vol?menes de datos en las investigaciones en salud y biomedicina?\\n\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\\nestimados  profesionales e investigadores, los invitamos a participar en el programa de especializaci?n \"ciencia de datos para la investigaci?n en salud y biomedicina\" en sus dos modalidades presencial  y virtual. \\n* se pueden inscribir m?nimo con el 40% del costo.\\n* expositor: dr. erwin kraenau espinal\\n* inicio modalidad presencial: 03 octubre al 12 diciembre 2015\\n* inicio modalidad virtual: 05 octubre al 11 diciembre 2015.\\n* modalidades: presencial y virtual\\nhttp://www.urp.edu.pe/programasespecializacion/portal/index.php?idprogramas=190\\n========quedan pocas vacantes!!!\\n\\n****************************************************\\ninscripciones:\\n****************************************************\\npara inscribirse por favor escribirnos a destadistico@gmail.com, para enviarle los formatos de inscripci?n.\\nrpc 993 477 990; rpm #968 248 582\\nweb urp\\nhttp://www.urp.edu.pe/programasespecializacion/portal/index.php?idprogramas=190\\nvisite: https://www.facebook.com/events/577595239081058/ /n/r what are the best #blogs for #datascientists to read?\\nto join this discussion visit here: http://goo.gl/chqo5j #datascience /n/r what are the best #blogs for #datascientists to read?\\nto join this discussion visit here: http://goo.gl/chqo5j #datascience /n/r more about sentiment analysis:\\nhttp://anjusthoughts.blogspot.in/2015/08/more-about-sentiment-analysis.html /n/r what are the #downsides of being a #datascientist?\\nto join this #discussion visit here: http://goo.gl/ljzbcb \\n#bigdata #datascience /n/r what can a #datascientist create in 1 hour, 1 day, 1 week, or 1 month?\\nto join this #discussion visit here: http://goo.gl/ht3fbm\\n#datascience #bigdata /n/r what are some good #thesis topics in #datascience?\\nto join this #discussion at: http://goo.gl/kntdc4 \\n#datascientist /n/r what can #economists learn from #datascientists?\\nto join this #discussion visit here: http://goo.gl/resxsl\\n#bigdata #datascience /n/r \"save money with #bigdata\"! to checkout complete #infographic visit here: http://goo.gl/sqietf #datascientist #datascience /n/r what are the most influential #papers in the #world of #bigdata? why?\\nto join this #discussion visit here: http://goo.gl/svzelr \\n#datascientist #datascience /n/r how can the #bankingsector benefit from #bigdataanalysis?\\nto join this #discussion visit here: http://goo.gl/nmktxf #bigdata #bigdataanalytics /n/r why do companies like uber & lyft need #datascientists and #dataengineers?\\nto join this #discussion visit here: http://goo.gl/neawws \\n#bigdata /n/r what qualities should a budding #datascientist have?\\nto join this #discussion visit here: http://goo.gl/clfebv\\n#bigdata #datascience /n/r innoplexus is hiring... do you wanna be a part of innoplexus family?\\n\\ninbox me your cvs... before 22nd sept. 2015...\\n\\nhurry up... the clock is ticking!!! /n/r is #datascientist shortage overhyped?\\nto join this #discussion at: http://goo.gl/fliizf \\n#datascience #bigdata /n/r 9 nosql pioneers who modernized data management bit.ly/1jg915b /n/r what are the best #datascience masters programs in the us? \\nto join this #discussion visit here: http://goo.gl/e989nx \\n#bigdata #datascientist /n/r cto of lyft, ceo & founder of kaggle and head of data at reddit are just some of the speakers coming along to extract conf in sf. awesome keynotes, workshops and data daiquiris all day. \\n\\nearly bird tickets end this week so grab one and save a few bucks: extractconf.com /n/r what statistics courses should i take to become a #datascientist?\\nto join this #discussion visit here: http://goo.gl/0qgydp \\n#bigdata #datascience /n/r do you want to learn how to master the #hadoop cluster? become a successful hadoop architect by attending this interesting webinar for free on #bigdata and #hadoopdeveloper @greycampus.register here for free  http://bit.ly/1osbtqa /n/r can anyone let me know , in the context of #bigdata which of the following is correct with respect to this statement : hdfs data blocks can be read in parallel \\n\\na true \\nb false /n/r are you interested in knowing about #bigdata use cases ? here is the way to  know all about it at opencampus here : http://bit.ly/1ev7wu2 /n/r what are some actual #projects #datascientists have worked on?\\nto join this #discussion visit here: http://goo.gl/m8hvnv \\n#datascience #bigdata /n/r what are some actual #projects #datascientists have worked on?\\nto join this #discussion visit here: http://goo.gl/m8hvnv \\n#datascience #bigdata /n/r what are some actual #projects #datascientists have worked on?\\nto join this #discussion visit here: http://goo.gl/m8hvnv \\n#datascience #bigdata /n/r big data is no more a hush hush thing. we know that enterprises are leaving no stone unturned to churn this big data and harness some actionable insights. so, what are their particular want. what enterprises do with big data? know more here. - https://www.promptcloud.com/blog/what-enterprises-do-with-big-data/ /n/r can anyone let me know which of the following is true with respect to this statement: #hadoop is open source  \\n\\na true only for apache hadoop\\nb always true\\nc true only for apache and cloudera hadoop \\nd always false /n/r once again in #bangalore, registrations open for #cloudera data analyst training.\\n\\ndate:- 24 - 27 sep\\'15\\ncost:- inr 74,400+ st\\n\\nbook your seat.\\ncontact us: sbagga@xebia .com | 0124 - 4700265 /n/r what are some good resources to practice #sql at the level of a #facebook #datascience interview? \\nto join this #discussion visit here: http://goo.gl/sekw49 \\n#datascientist #bigdata /n/r do you want to learn how master the hadoop cluster? become a successful hadoop architect by attending this interesting webinar for free on #bigdataand #hadoopdeveloper @greycampus.register here for free \\nhttp://bit.ly/1osbtqa /n/r web 2.0 engages its users in a more interactive approach. simply, users can publish their thoughts, create social contents, participate in live social debates and can express their views as comments. nowadays, we tend to share almost every little happening on the social channels. at times, published product reviews and usage experiences on these social surfaces help us pick an informed purchasing decision. so, in every way web 2.0 is more integrated with the world of the internet of things (iot) which own our daily lives.- https://www.promptcloud.com/blog/web-crawling-web-2.0-web-spider-view /n/r what is the relationship between #statisticians and #datascientists? \\nto join this #discussion visit here: http://goo.gl/h0nlkj \\n#bigdata #datascience /n/r do you want to learn how master the hadoop cluster? become a successful hadoop architect by attending this interesting webinar for free on #bigdata and #hadoopdeveloper @greycampus.register here for free : http://bit.ly/1osbtqa /n/r i\\'m looking for technology writers and bloggers. pls inbox me at preeti.juneja@loginworks.com /n/r what are the key traits of a #datascientist?\\nto join this #discussion visit here: http://goo.gl/ma61yb \\n#bigdata #datascience /n/r #alibaba unveils, \" #china\\'s first artificial intelligence platform\"!\\ncheckout #news at: http://goo.gl/r4fkth \\n#datacenter #datascientist /n/r can anyone let me know , which of the following are not metadata items in the context of #bigdata ? \\n\\na hdfs block locations\\nb list of hdfs files\\nc replication factor of files\\nd access rights \\ne file records distributio /n/r do you want to learn how master the hadoop cluster? become a successful hadoop architect by attending this interesting webinar for free on #bigdata and #hadoopdeveloper @greycampus.register here for free : http://bit.ly/1osbtqa /n/r do you want to learn how master the hadoop cluster? become a successful hadoop architect by attending this interesting webinar for free on #bigdata and #hadoopdeveloper @greycampus.register here for free :http://bit.ly/1hheuno /n/r what are the top algorithms that every #datascientist should have in their toolbox?\\nto join this #discussion visit here: http://goo.gl/hm1p6t \\n#datascience #bigdata /n/r join folks like andrew ng & anthony goldbloom at extract conf, 30th october in sf (extractconf.com)\\n\\nreddit, tableau, lyft, twitter, bigml & more gathering for a day of fun and data-learning! /n/r do you want to know about the current #bigdata market in terms of job opportunities? join us for a free #webinar on #bigdata and#hadoopdeveloper @greycampus . register here for free :http://bit.ly/1osbtqa /n/r big data is snowballing and with every passing day, cloud storages across the planet and other alike services, generally branded as \\'cloud as a service\\'(caas) or infrastructure as a service(iaas) are getting burdened with colossal amounts of complex data processing requests. now, supporting this fact, data-intensive applications which exist far away from its connected data center, slog to get their requests done. - https://www.promptcloud.com/blog/big-data-processing-edge-computing /n/r are you looking for practice questions for #bigdata? here we have it for you. get free practice questions for bigdata here :http://www.greycampus.com/opencampus/big-data-developer /n/r it seems #ola, #uber and #taxiforsure forgot the basics of supply and demand analytics! ivk, the co-founder and cto of crayon data, backs this hypothesis with hard facts \\nhttp://ow.ly/r50eq /n/r what are some actual #projects #datascientists have worked on? \\nto join this #discussion visit here: http://goo.gl/6ffule \\n#bigdata #datascience /n/r become a successful hadoop architect by attending an interesting webinar for free on #bigdata and #hadoopadministration @greycampus.register here for free :http://bit.ly/1hheuno /n/r can anyone let me know , what are the stages to the ibm #bigdata & analytics maturity model? \\n\\na novice, builder. leader, master\\nb ad hoc, foundational, competitive, differentiating. breakaway\\nc descriptive analytics, diagnostic analytics, predictive analytics, prescriptive analytics\\nd initial, repeatable, defined, managed, optimizing /n/r do you want to learn how master the hadoop cluster? become a successful hadoop architect by attending this interesting webinar for free on #bigdata and #hadoopadministration @greycampus.register here for free :http://bit.ly/1hheuno /n/r which startups in india are focusing on #machinelearning, #bigdata and #datascience?\\nto join this #discussion visit here: http://goo.gl/m13qfe /n/r do you have the skill to master the hadoop cluster? become a successful hadoop architect by attending this interesting webinar for free on #bigdata and #hadoopadministration @greycampus.register here for free :http://bit.ly/1hheuno /n/r what are the fields where #bigdataanalysis is used?\\nto join this discussion visit here: http://goo.gl/zd1apk \\n#bigdata #datascience #datascientist /n/r can anyone let me which is the option suitable to this sentence : #hadoop is designed to scale up from a single server to thousands of machines, but with ? \\n\\na no fault tolerance  \\nb a very low degree of fault tolerance \\nc a very high degree of fault tolerance \\nd none of the above /n/r celebrations start with the freedom month ! grab your seat in the cloudera apache hadoop training scheduled at different location in india. contact us :- swati bagga 0124- 4700265 | sbagga@xebia.com /n/r #independenceday offer from h2kinfosys\\n\\nregister today get best discounts on all courses.\\n\\n#qatesting training\\n\\n#businessanalyst training\\n\\n#java #j2ee training\\n\\n#hadoop #bigdata training\\n\\n#selenium training\\n\\n#informatica training\\n\\n#qtp #hpuft training                                     \\n\\n#manualtesting training\\n\\n#mobileappstesting training\\n\\n#saptesting training\\n\\n#dotnet training\\n\\n#soapuitesting training\\n\\n#databasetesting training\\n\\n#etltesting training\\n\\ncontact us: \\nwww.h2kinfosys.com\\ncall: +1-770-777-1269\\nemail: h2kinfosys@gmail.com\\n\\n#independencedayoffers   #specialdiscounts /n/r what are the most marketable skills in the field of #data, #analysis, and #datascience?\\nto join this #discussion visit here: http://goo.gl/qgiccq \\n#bigdata #datascientist #bigdataanalysis /n/r big data congress \\nplease join us and share this event\\n#daily registration is available for the #medical #speciality sessions, plaese visit the web site #medicres http://t.co/jruduyos8y #gmr2015 #biostatistics conference #bioethics conference /n/r what #skills are needed for #machinelearning #jobs?\\nto join this #discussion visit here: http://goo.gl/oin9eq \\n#bigdata #datascience /n/r why the current obsession with #bigdata?\\nto join this #discussion at: http://goo.gl/qgfuac \\n#datascience #datascientist /n/r why the current obsession with #bigdata?\\nto join this #discussion at: http://goo.gl/qgfuac \\n#datascience #datascientist /n/r can anyone let me know , which of the following is the outer most part of hbase data model in the context of #bigdata ?\\n\\na row key\\nb column family\\nc database\\nd table /n/r what is the difference between #dataanalytics, #dataanalysis, #datamining, #datascience, #machinelearning, and #bigdata ?\\nto join this #discussion visit here: http://goo.gl/irtsuk /n/r can anyone let me know , which of the following operations can\\'t use reducer as combiner also in the context of #bigdata ? \\n\\na group by maximum\\nb group by minimum\\nc group by average\\nd group by count /n/r hey all. hosting extract data conf in sf, 30th october. love to have you join. got awesome folks like andrew ng from baidu speaking :-)\\n\\nhttp://extractconf.com/ /n/r what is big data hadoop? is this right time to switch your domain to big data hadoop? what is needed to learn it and will you be able to learn it? get answers to many more similar questions which you have in your mind from industry expert in first free online session on monday 10th aug-2015 at 9.30 pm - 11 pm ist.\\n\\nregister at below link:\\nhttp://data-flair.com/big-data-hadoop-training-first-free-session/\\n\\nfeatures of our training:\\n- learn from trainer who has trained thousands of candidates and helped them in boosting their career in this technology\\n- gain handson knowledge through practicals, workshops, pocs and live project\\n- get yourself prepared for interviews and cloudera certification\\n- get course completion certificate from dataflair /n/r what are the most common mistakes made by aspirational #datascientists?\\nto join this #discussion at: http://goo.gl/ihhsyh \\n#bigdata #datascience /n/r 5 ways in which #bigdata will help your #business!\\ncheckout complete #infographic at: http://goo.gl/yvs9cs /n/r what #classes should i take if i want to become a #datascientist ?\\nto join this #discussion visit here: http://goo.gl/pgmuh2 \\n#bigdata #datascience /n/r when you are developing a combiner that takes as input text keys, intwritable values, and emits text keys, intwritable values. in your opinion , which interface should your class implement in the context of  #bigdata? \\n \\na.  combiner <text, intwritable,text, intwritable>\\nb  reducer <text, intwritable,text, intwritable>\\nc  combiner <text,text, intwritable, intwritable>\\nd  combiner <text, text, intwritable, intwritable> /n/r sentiment analysis tools\\nhttp://anjusthoughts.blogspot.in/2015/08/tools-used-for-sentiment-analysis.html?m=0 /n/r what are the best methods for testing #bigdata applications?\\nto join this discussion visit here: http://goo.gl/eigd8j\\n#datascience /n/r how #cloud & #smbs can help each other! \\ncheckout #blog at: http://goo.gl/jycu0x \\n#cloudcomputing #smes /n/r vacancies available at blue kangaroo, amman: php developer, front end engineer, ios developer. to apply, send your cv to careers@bluekangaroo.com. /n/r willing to know what is big data hadoop and why whole world is behind learning it?learn basics of big data hadoop in first free online session on thursday 6 aug-15 at 7.30 am - 9 am ist.\\n\\nregister at below link:\\nhttp://data-flair.com/big-data-hadoop-training-first-free-session/\\n\\nfeatures of our training:\\n- learn from industry expert having 18 years professional experience and more than 5 years of training experience\\n- gain handson knowledge through practicals, workshops, pocs and live project\\n- get yourself prepared for interviews and cloudera certification\\n- get course completion certificate from dataflair /n/r what \" #bigdata \" opportunities will be most interesting and profitable for #startups?\\nto join this #discussion visit here: http://goo.gl/c5gpmo \\n#datascience /n/r can anyone let me know what does hive data models represent in the context of #bigdata ?\\n\\na table in hdfs\\nb table in metastore db \\nc none of the options \\nd directories in hdfs /n/r do you want to know about the current #bigdata market in terms of job opportunities? join us for a free #webinar on #bigdata and #hadoopdeveloper @greycampus . register here for free : http://bit.ly/1osbtqa /n/r can anyone let me know in which of the following categories does the sliding window operations fall in the context of #bigdata ? \\n\\na big data batch processing\\nb oltp transactions\\nc small batch processing\\nd big data real time processing /n/r boost your career with free video tutorials of upcoming technology big data:\\n\\nhttp://data-flair.com/big-data-and-hadoop-training-videos/\\n\\nwhy learn hadoop?\\nwe create 2.5 quintillion bytes of data every day. so much that 90% of the data in the world today has been created in the last two years alone (source: ibm). these extremely large datasets are hard to deal with using legacy systems such as rdbms as data exceed the storage and processing capacity of database. the legacy systems are becoming obsolete.\\n\\naccording to gartner: ?big data is new oil?. big data is all about finding the needle of value in a haystack of structured, semi-structured and un-structured data. hadoop (the solution of all big data problems) has become the most important component in the data stack, which enables rapid processing of data at petabyte scale. hadoop is expected to be at the core of more than half of all analytics software within the next two years. /n/r #nec, #andhra govt. to collaborate on #smartcitiesproject! \\ncheckout #news at: http://goo.gl/ty4cxf /n/r are you interested in knowing about the current #bigdata market in terms of job opportunities? join us for a free #webinar on #bigdata and #hadoop developer @greycampus .register here for free : http://bit.ly/1osbtqa /n/r why is it so hard to recruit top #datascientists? \\nto join this #discussion visit here: http://goo.gl/ikwbnv \\n#bigdata #datascience /n/r why is #python a language of choice for #datascientists? \\nto join this #discussion at: http://goo.gl/d0bgnm #bigdata #datascience /n/r hadoop online training by h2kinfosys is known for their excellent and professional training. we have designed our big data course as per the demand in the present it industry. based on this demand, we started providing hadoop online training course. \\n\\nhadoop online training course explains the purpose of hadoop technology, how to setup hadoop cluster, how to store big data using hadoop (hdfs) and how to process/analyze the bigdata using map-reduce programming or by using other hadoop ecosystems.\\n\\nwe have trained many students on apache hadoop technology and provided efficient big data tutorials during the course.\\n\\n\"core java \"  is offered free for those people who opt for hadoop online training course.\\n\\nenroll for our hadoop online training demo now!\\n\\nfor more details:\\nhttp://www.h2kinfosys.us/courses/hadoop-big-data-online-training/\\nemail: h2kinfosys@gmail.com\\ncall us:\\nusa: +1 770-777-1269\\nuk: (020) 3371 7615. /n/r what are some important questions to ask a #recruiter when #interviewing for a #datascience job?\\nto join this #discussion visit here: http://goo.gl/gncdbc \\n#bigdata #datascientist /n/r enroll for our free webinar on #bigdata and #hadoop on 3 rd august, 2015 8:00pm to 11:00pm ist. register now .\\nhttp://www.greycampus.com/training/big-data-hadoop-webinar /n/r what are the best #bigdataanalysis #companies?\\nto join this #discussion visit here: http://goo.gl/ofpuwg \\n#bigdata #datascience /n/r enroll for our free webinar on #bigdata and #hadoop on 3 rd august , 2015 8:00pm to 11:00pm ist. register now .\\nhttp://www.greycampus.com/training/big-data-hadoop-webinar /n/r can anyone let me know which of the following is true for #hive? \\n\\na hive supports schema checking\\nb hive is the database of hadoop \\nc hive can replace an oltp system\\nd hive doesn\\'t allow row level updates /n/r guys are you willing to become successful hadoop developer which requires java knowledge along with in depth knowledge of map reduce that is the heart of hadoop, that too without affecting much on your pocket?\\n\\nattend first free online class on 29 july at 9.30 pm - 11 pm ist and check the level of training yourself.register at below link:\\nhttp://data-flair.com/big-data-hadoop-training-first-free-session/\\n\\nfeatures of our training:\\n- 40 hours of live instructor led session\\n- instructor having 18 years of experience with 5 years of training experience\\n- more than 15 hours on map reduce concepts to provide complete hands on knowledge\\n- batches as per your time convenience\\n- complementary self paced java course to provide java essential for hadoop /n/r what is the future of #datascience?\\nto join this #discussion visit here: http://goo.gl/gfxb7n #bigdata #datascientist /n/r can anyone let me know when is the earliest point at which the reduce method of a given reducer be called in the context of #bigdata?\\n\\na not untill all mappers have finished processing all records \\nb as soon as a mapper has emitted atleast one record\\nc it depends on the input format used for the job \\nd as soon as atleast one mapper has finished processing its input split /n/r we are proud to announce that xebia has been awarded the \"best training company in cutting-edge technologies\" at world education congress summit /n/r willing to know what is big data hadoop and why whole world is behind learning it?learn basics of big data hadoop free on 26 july at 8.00 am - 10 am ist!!!\\n\\nregister at below link:\\nhttp://data-flair.com/big-data-hadoop-training-first-free-session/\\n\\nfeatures of our training:\\n- learn from industry expert having 18 years professional experience and more than 5 years of training experience\\n- gain handson knowledge through practicals, workshops, pocs and live project\\n- get yourself prepared for interviews and cloudera certification\\n- get course completion certificate from dataflair /n/r how do i learn #datamining?\\ncheckout #discussion at: http://goo.gl/wdr727 \\n#bigdata #datascience /n/r \"modern #datascientist\" \\ncheckout #infographic at: http://goo.gl/zxavgj \\n#bigdata #datascience /n/r coming to a computer screen near you! cracking code - a contest to reward some o the coding folk! \\nthey have 2 rounds. if you make it through, you get to take your pick from an exciting list of rewards! so gear up, save the date and get ready to crack some code! \\nto know more about bigdata-madesimple, visit www.bigdata-madesimple.com /n/r can anyone let me know which of the following is true for #hadoop pseudo distributed mode ?\\n\\na runs on single machines with all daemons \\nb it runs on multiple machines\\nc runs on single machines without all daemons\\nd runs on multiple machines without all daemons /n/r can anyone let me know , which of the following decides number of mappers for a #mapreduce job ? \\n\\na file location\\nb mapred.map.tasks parameter\\nc input file size \\nd input splits /n/r what are some interesting beginner level projects that can be built using apache #hadoop?\\nto join this #discussion visit here: http://goo.gl/7z3hhk #bigdata #datascience /n/r big data, big security: defense in depth bit.ly/1oyk7zx /n/r what kinds of large #datasets open to the public do you analyze the mostly? \\nto join this #discussion visit here: http://goo.gl/xevvvg #bigdata /n/r can anyone let me know , what should be an upper limit for counters of a #mapreduce job? \\n\\na ~5s\\nb ~15\\nc ~150\\nd ~50 /n/r how can i improve my profile to get a #datascientist job in #google/ #facebook/ #linkedin? \\nto join this #discussion visit here: http://goo.gl/8zozas /n/r guys are you willing to become successful hadoop developer which requires java knowledge along with in depth knowledge of map reduce that is the heart of hadoop, that too without affecting much on your pocket?\\n\\nattend first free class on sunday 26th july-15 at 8 am-10 am ist and check the level of training yourself. register at below link:\\nhttp://data-flair.com/big-data-hadoop-training-first-free-session/\\n\\nfeatures of our training:\\n- 40 hours of live instructor led session\\n- instructor having 18 years of experience with 5 years of training experience\\n- more than 15 hours on map reduce concepts to provide complete hands on knowledge\\n- batches as per your time convenience\\n- complementary self paced java course to provide java essential for hadoop /n/r did you miss our #webinar on #bigdata and #hadoopdeveloper ? we have a recorded video of it .here is the link where you can watch it http://certification.greycampus.com/big-data-and-hadoop-webinar/ /n/r can anyone let me know , which of the following #hadoop config files is used to define the heap size?\\n\\na hdfs-site.xml\\nb core-site.xml\\nc hadoop-env.sh\\nd slaves /n/r can anyone let me know , what does commodity hardware in #hadoop mean ?\\n\\na industrial standard hardware\\nb discarded hardware \\nc very cheap hardware\\nd low specifications industry grade hardware /n/r an overview of sentiment analysis:\\nhttp://anjusthoughts.blogspot.in/2015/07/sentiment-analysis_18.html?m=0 /n/r #datascience 2015: what\\'s hot and what\\'s not! checkout #infographic at: http://goo.gl/h524kp #bigdata /n/r what is big data hadoop? is this right time to switch your domain to big data hadoop? what is needed to learn it and will you be able to learn it? get answers to many more similar questions which you have in your mind from industry expert free on 14 july at 9.30 pm-11 pm ist.\\n\\nregister at below link:\\nhttp://data-flair.com/big-data-hadoop-training-first-free-session/\\n\\nfeatures of our training:\\n- learn from trainer who has trained thousands of candidates and helped them in boosting their career in this technology\\n- gain handson knowledge through practicals, workshops, pocs and live project\\n- get yourself prepared for interviews and cloudera certification\\n- get course completion certificate from dataflair /n/r can anyone let me know what is  correct the order of the following phases of a #mapreduce program in the order that they execute? \\npartitioner \\nmapper \\ncombiner \\nshuffle/sort\\na. mapper combiner partitioner shuffle/sort\\nb. mapper combiner shuffle/sort partitioner\\nc. mapper partitioner shuffle/sort combiner\\nd. mapper shuffle/sort combiner partitioner\\ne. partitioner mapper shuffle/sort combiner\\nf. combiner mapper shuffle/sort partitioner /n/r what are some important questions to ask a recruiter when interviewing for a #datascience job? \\nto join this #discussion at: http://goo.gl/84dhor #bigdata /n/r willing to know what is big data hadoop and why whole world is behind learning it?learn basics of big data hadoop free on 17 july at 9.30 pm-11.30 pm ist!!!\\n\\nregister at below link:\\nhttp://data-flair.com/big-data-hadoop-training-first-free-session/\\n\\nfeatures of our training:\\n- learn from industry expert having 17 years professional experience and more than 3 years of training experience\\n- gain handson knowledge through practicals, workshops, pocs and live project\\n- get yourself prepared for interviews and cloudera certification\\n- learn java essentials for hadoop as complementary\\n- get lifetime access to complete study material and recorded sessions\\n- get course completion certificate from dataflair /n/r 4 #bigdata challenges in #healthcare! checkout #slideshare at: http://goo.gl/eez34d /n/r do i need a masters/phd to become a #datascientist?\\nto join this #discussion visit our site: http://goo.gl/mttjgr /n/r enroll for our free webinar on #bigdata and hadoop developer. july 15, 2015 , 8:00pm to 11:00pm ist. training conducted by nag bhushan\\nhttp://www.greycampus.com/training/big-data-hadoop-webinar /n/r enroll for our free webinar on #bigdata&hadoopdeveloper. july 15, 2015 , 8:00pm to 11:00pm ist. training conducted by nag bhushan\\nhttp://www.greycampus.com/training/big-data-hadoop-webinar /n/r what are the best hosted #hadoop providers?\\nto join this #discussion visit our site: http://goo.gl/7fzyp1 \\n#bigdata /n/r with the right analytics in place ,  in your opinion , for what purposes can #bigdata be used  ?  you can refer : http://bit.ly/1jrhzsf \\n\\na correcting consumer behaviour\\nb analyzing driving patterns for accurate insurance premium estimations\\nc including semantics driven strategy for clearbottom line figures /n/r willing to know about biggest buzz word - big data ?\\n\\nhttp://goo.gl/05ceyo\\n\\ntraining objectives:\\n- lead you on the path to become a certified hadoop professional\\n- prepare to lead hadoop movement and become employable in big data industry\\n- learn how to develop game-changing hadoop applications\\n- prepare for interviews and cloudera certification /n/r can anyone let me know the correct choice for this sentence .mapreduce can best be described as a programming model used to develop #hadoop based applications that can process massive amounts of unstructured data  \\n\\na true\\nb false /n/r what are some examples of bad #datascience leading to bad decisions?\\nto join this #discussion visit here: http://goo.gl/5thrkg /n/r enroll for our free webinar on #bigdata&hadoopdeveloper. july 15, 2015 , 8:00pm to 11:00pm ist. training conducted by nag bhushan\\nhttp://www.greycampus.com/training/big-data-hadoop-webinar /n/r are you still confused whether to learn latest technology big data hadoop when your friends have already started working in it? attend free session on big data hadoop on 14 july at 9.30 pm-11.30 pm ist and get answers to your qeueries.\\n\\nregister at below link:\\nhttp://data-flair.com/big-data-hadoop-training-first-free-session/\\n\\nfeatures of our training:\\n- learn from trainer who has trained thousands of candidates and helped them in boosting their career in this technology\\n- gain handson knowledge through practicals, workshops, pocs and live project\\n- get yourself prepared for interviews and cloudera certification\\n- get course completion certificate from dataflair /n/r enroll for our free webinar on #bigdata&hadoopdeveloper. july 15, 2015 , 8:00pm to 11:00pm ist. training conducted by nag bhushan\\nhttp://www.greycampus.com/training/big-data-hadoop-webinar /n/r enroll for our free webinar on #bigdata&hadoopdeveloper. july 15, 2015 , 8:00pm to 11:00pm ist. training conducted by nag bhushan\\nhttp://www.greycampus.com/training/big-data-hadoop-webinar /n/r how do #datascientists use statistics?\\nto join this discussion visit here: http://goo.gl/mo0i16 #bigdata #hpcasia /n/r #cloudcomputing for hospitality! \\ncheckout #infographic at: http://goo.gl/0w8y4q /n/r according to you ,which of the following is included in additional capabilities as companies move past the experimental phase with #hadoop ?\\n\\na improved data storage and information retrieval\\nb improved extract, transform and load features for data integration\\nc improved data warehousing functionality\\nd improved security, workload management and sql support /n/r enroll for our free webinar on #bigdata and hadoop developer. july 15, 2015 , 8:00pm to 11:00pm ist. training conducted by nag bhushan\\nhttp://www.greycampus.com/training/big-data-hadoop-webinar /n/r is it right time to learn big data?? should i learn big data ??\\nget answer to all your questions, watch the exclusive recording:\\n\\nhttps://www.youtube.com/watch?v=vyka4-eajiw\\n\\nbig data\\nbig data is now part of agenda of most of the fortune 500 companies. big data technology is maturing fast and leading corporations in banking and financial services, utilities, telco and retail have embraced big data as foundation technology for information management.\\nacross the world, big data is on boom. companies especially in the finance, retail and e-commerce sectors are relying more heavily on big data to make decisions that impact their sales, operations and workforce. there is such a dearth of big data talent that these companies are willing to pay high salaries for the right skill set. /n/r what are some interesting #bigdata and analytics startups? \\nto join this #discussion visit here: http://goo.gl/tiiij8 /n/r 4 insights for #bigdata tool makers! \\ncheckout #slideshare at: http://goo.gl/l551ua /n/r according to you, for what can traditional it systems provide a foundation when they\\'re integrated with big data technologies like #hadoop? \\n\\n a big data management and data mining\\n b data warehousing and business intelligence\\n c management of hadoop clusters\\n d collecting and storing unstructured data /n/r what should be ideal size, skill set and composition of team for a successful #bigdata implementation in an #organization? \\nto join this #discussion visit here: http://goo.gl/vgcgm4 #hpc /n/r carve your career with big data..!! learn from industry experts:\\n\\nhttp://data-flair.com/course/big-data-and-hadoop-training/\\n\\ntraining features:\\n- preparation for interview and cloudera certification\\n- instructor led live online sessions\\n- 24x7x365 lifetime access\\n- 40hrs classes, 40hrs lab & live project\\n- complementary java training /n/r how do i setup a basic \\'#bigdata\\' infrastructure to sort & manage a 300-500gb #dataset? \\njoin this #discussion at: http://goo.gl/0hicps /n/r install cloudera hadoop cdh5 on ubuntu - a step by step tutorial for beginners: http://data-flair.com/blog/install-cloudera-hadoop-cdh5-on-ubuntu/ alternatively watch video tutorial: https://www.youtube.com/watch?v=489elbwzwce /n/r what \"useful\" data products can i build as a aspiring #datascientist ? \\njoin this #discussion at: http://goo.gl/t2bbok #bigdata /n/r people. stop cracking the \"everyone talks about it no one knows it\" joke about big data and figure out what it is!\\nhere\\'s something to help you out. \\na complete glossary of big data terms.http://goo.gl/rofkdz /n/r can anyone let me know , how can #bigdata help in getting better insights about customers? you can refer : http://bit.ly/1ax5z1i /n/r how do i learn #datascience in a time-efficient manner? \\nto join this #discussion at: http://goo.gl/nm9vn9 #bigdata /n/r searching for some books to get acquainted with the basics of ai? i personally don\\'t know a thing about ai but number 14 on here is about the history of ai! that i can understand! this list pretty much covers it all.\\nhttp://goo.gl/pf2un8 /n/r can anyone let me know , how does getting on top of the challenges of #bigdata enable sophisticated approaches for product marketing, development, and sales ? you can refer : http://bit.ly/1jbeaju /n/r #datascience 2015: what\\'s hot and what\\'s not! \\ncheckout complete #infographic at: http://goo.gl/jysk0k #hpc /n/r can anyone let me know , in which directory #hadoop is installed ? /n/r what are the fields where #bigdata analysis is used?\\njoin this #discussion at: http://goo.gl/1vfwm0 #hpc /n/r peter coveney to keynote #isc cloud & #bigdata conference! checkout complete story at: http://goo.gl/mdy78l #hpc #cloudcomputing /n/r what are the best #bigdata analysis companies?\\nto join this #discussion : http://goo.gl/jh6d6e #hpc /n/r #bigdataanalytics, #internetofeverything and advanced ai add momentum to healthcare innovation! \\ncheckout #guestarticle at: http://goo.gl/knsxjw #hpc /n/r 5 advantages and disadvantages of cloud storage\\nhttp://goo.gl/6nhxt0 /n/r can anyone let me know , which of the following can the workflows expressed in #oozie contain ?\\n\\na.iterative repetition of mapreduce jobs until a desired answer or state is reached.\\nb. sequences of mapreduce and pig jobs. these are limited to linear sequences of actions with exception handlers but no forks.\\nc. sequences of mapreduce jobs only; no pig or hive tasks or jobs. these mapreduce sequences can be combined with forks and path joins.\\nd. sequences of mapreduce and pig. these sequences can be combined with other actions including forks, decision points, and path joins /n/r data science is being touted as the hottest career option in the 21st century. time to get your hands dirty with data! \\nhttp://bigdata-madesimple.com/data-scientist-hacker-programmer-analyst-story-teller-artist/ /n/r can  anyone  let  me  know  in  the  context  of #bigdata, how  is  the  volume  measured  by  data  warehouses  ? you  can  refer : http://goo.gl/pb8lvx \\n\\na gigabytes\\nb gigabytes per hour\\nc terabytes \\nd petabytes /n/r can anyone let me know , what is the largest data source used by organizations in the context of #bigdata? \\n\\na emails \\nb business transactions \\nc log data\\nd social media /n/r watch exclusive recordings of big data & hadoop training:\\nhttp://data-flair.com/big-data-and-hadoop-training-videos/\\n\\ntraining features:\\n- instructor led live session\\n\\n- preparation for interview and cloudera certification\\n- become employable in big data industry\\n- complementary java training /n/r free session on big data on 1st july 2015 @ 9.30pm ist\\nhttp://goo.gl/ts6p06 /n/r what tools do #datascientists use for professional presentations? to join this #discussion visit here: http://goo.gl/eiioom #bigdata /n/r can anyone let me know , in the context of #bigdata which is faster : map-side join or reduce-side join ? why? \\n\\na. both techniques have about the the same performance expectations.\\nb. reduce-side join because join operation is done on hdfs.\\nc. map-side join is faster because join operation is done in memory.\\nd. reduce-side join because it is executed on a the namenode which will have faster cpu and more memory. /n/r #smartcities- the indian perspective! \\nto have a look of complete #infographic visit here: http://goo.gl/dgt0tp #smartcitiesmission /n/r how does working as a #facebook #datascientist (or similar job) change your basic beliefs about people and behavior?\\njoin this #discussion at: http://goo.gl/blnnes /n/r a data analyst knows everything about sql. he wants to run adhoc analysis on data . in your opinion , which of the following is a data warehousing software built on top of #apachehadoop that defines a simple sql-like query language well-suited for this kind of user? \\n\\na pig\\nb hue\\nc hive\\nd sqoop /n/r the internet of things: tracking the \\'cross-everywhere\\' consumer bit.ly/1ldtya8 /n/r can anyone  let  me know ,  which #bigdata tool  will  be  used  to  generate java classes which will  process  data  imported  into hdfs  from relational database ?\\n\\na  pig\\nb  hue\\nc  hive\\nd  sqoop /n/r what are the downsides of being a #datascientist? to join this #discussion group visit here: http://goo.gl/m3oesa #hpc #supercomputing /n/r proven #datascience for competitive and digital marketing intelligence! checkout #blog at: http://goo.gl/5p8k32 #hpc #supercomputing /n/r how should i prepare for statistics questions for a #datascience interview?\\nto join this #discussion visit here: http://goo.gl/nkkyie /n/r can anyone let me know , how are pig and #mapreduce related ?  \\n\\na pig provides additional capabilities that allow certain types of data manipulation not possible with mapreduce.\\nb pig provides no additional capabilities to mapreduce. pig programs are executed as mapreduce jobs via the pig interpreter.\\nc pig programs rely on mapreduce but are extensible, allowing developers to do special-purpose processing not provided by mapreduce.\\nd pig provides the additional capability of allowing you to control the flow of multiple mapreduce jobs. /n/r the #hadoop framework provides a mechanism for coping with machine issues such as faulty configuration or impending hardware failure. mapreduce detects that one or a number of machines are performing poorly and starts more copies of a map or reduce task. all the tasks run simultaneously and the task finish first are used. in your opinion , what is this called ? \\n\\n a identitymapper\\n b combine\\n c default partitioner\\n d identityreducer\\n e speculative execution /n/r are too many people training to become #datascientists? \\nto join this #discussion visit here: http://goo.gl/g8aobd #hpc /n/r can anyone let me know, what is the most widely used #bigdata store ? \\n\\na hadoop hdfs \\nb relational databases\\nc mongo db\\nd analytic databases /n/r what are some better sites for #datascientists to freelance on other than elance? \\nto join this discussion visit here: http://goo.gl/fva2nv #hpc #bigdata /n/r can anyone let me know , what is included in the common cohorts in #hadoop , which is a framework that works with a variety of related tools?  \\n\\na mapreduce, mysql and google apps\\nb mapreduce, hummer and iguana\\nc mapreduce, heron and trumpet\\nd mapreduce, hive and hbase /n/r how good is r for #datavisualization? \\nto join this #discussion visit here: http://goo.gl/vwiaw9 #hpc #bigdata /n/r few more hours to go . join the free webinar from greycampus on #bigdata and hadoop developer course. calling all aspirants. see you all on 18 june at 8 p.m. enroll now!\\nhttp://www.greycampus.com/training/big-data-hadoop-webinar /n/r what are the best books for self-studying statistics and #datascience? \\nto join this #discussion click here: http://goo.gl/lsgklr \\n#hpc /n/r reminding all about our free webinar  from greycampus on #bigdata and hadoop developer course. calling all aspirants. see you all on 18 june at 8 p.m. enroll now!\\nhttp://www.greycampus.com/training/big-data-hadoop-webinar /n/r #bigdata transforming gaming! checkout complete #blog at: http://goo.gl/ejm9on #hpc /n/r can anyone let me know , which organization is currently using maximum number of #hadoop clusters  ? \\n\\na microsoft\\nb ibm\\nc yahoo\\nd hcl /n/r can anyone let me know , which of the following statements most accurately describes the relationship between #mapreduce and pig? \\n\\na. pig provides additional capabilities that allow certain types of data manipulation not possible with mapreduce.\\nb. pig provides no additional capabilities to mapreduce. pig programs are executed as mapreduce jobs via the pig interpreter.\\nc. pig programs rely on mapreduce but are extensible, allowing developers to do special-purpose processing not provided by mapreduce.\\nd. pig provides the additional capability of allowing you to control the flow of multiple mapreduce jobs. /n/r join the free webinar from greycampus on #bigdata and hadoop developer course. calling all aspirants. see you all on 18 june at 8 p.m. enroll now!\\nhttp://www.greycampus.com/training/big-data-hadoop-webinar /n/r tcs generated over $500 million in annual revenue from cloud platforms! checkout #news at: http://goo.gl/ouhb9i #cloudcomputing /n/r can anyone let me know , what are map files and why are they important in #bigdata ?  \\n\\na.  map files are stored on the namenode and capture the metadata for all blocks on a particular rack. this is how hadoop is \"rack aware\" \\nb.  map files are the files that show how the data is distributed in the hadoop cluster. \\nc.  map files are generated by map-reduce after the reduce step. they show the task distribution during job execution \\nd.  map files are sorted sequence files that also have an index. the index allows fast data look up. /n/r what are some #dataanalysis projects i can do as a #datascience beginner?\\nto join this discussion visit here: http://goo.gl/dbghbj \\n#hpc #supercomputing /n/r join the free webinar from greycampus on #bigdata and hadoop developer course. calling all aspirants. see you all on 18 june at 8 p.m. enroll now!\\nhttp://www.greycampus.com/training/big-data-hadoop-webinar /n/r join the free webinar from greycampus on #bigdata and hadoop developer course. calling all aspirants. see you all on 18 june at 8 p.m. enroll now!\\n http://www.greycampus.com/training/big-data-hadoop-webinar /n/r #hadoop api uses basic java types such as longwritable, text, intwritable. they have mostly same features as default java classes. in your opinion , what are these writable data types optimized for? \\n\\na. writable data types are specifically optimized for network transmissions \\nb. writable data types are specifically optimized for file system storage \\nc. writable data types are specifically optimized for map-reduce processing \\nd. writable data types are specifically optimized for data retrieval /n/r old statistics: is it what we are talking about? checkout complete #blog at: http://ow.ly/o74ps #hpc #bigdata /n/r what is the role big data will play in materials science research? \\nto join this discussion visit here: http://goo.gl/tvnd5i \\n#hpc #supercomputing /n/r can anyone let me know , why would a developer create a #mapreduce without the reduce step?  \\n\\na.  developers should design map-reduce jobs without reducers only if no reduce slots are available on the cluster. \\nb.  developers should never design map-reduce jobs without reducers. an error will occur upon compile. \\nc.  there is a cpu intensive step that occurs between the map and reduce steps. disabling the reduce step speeds up data processing. \\nd.  it is not possible to create a map-reduce job without at least one reduce step. a developer may decide to limit to one reducer for debugging purposes. /n/r can anyone let me know , which #mapreduce phase is theoretically able to utilize features of the underlying file system in order to optimize parallel execution? \\n\\na. split\\nb. map\\nc. combine /n/r what are the best blogs for #dataminers and #datascientists to read?\\njoin this discussion at: http://goo.gl/ggkcqp #hpc /n/r #cloud can help increase #gdp for india! checkout #blog at: http://goo.gl/bo5r9k #hpc #supercomputing /n/r which type of data scientist are you? to analyze and work upon your strengths and weakness as a data scientist, take this survey\\nhttp://goo.gl/gz2tq8 /n/r which companies are currently hiring for #bigdata in india? join this discussion at: http://goo.gl/j8gkuj #hpc /n/r stereotypes in the age of #bigdata! checkout complete #blog at: http://goo.gl/vs9sjs #hpc #supercomputing /n/r can anyone let me know , what are the steps in the lifecycle of mapper in the context of #bigdata ? \\n\\na) jobtracker calls the map method of the mapper\\nb) jobtracker calls the task tracker and task tracker calls the map method\\nc) task tracker calls the job tracker.which inturn calls the map method /n/r what are some good projects that can be made in 1 to 2 months in \"machine learning\" or \"big data\" or \"cloud computing\"? to join this discussion visit here: http://ow.ly/ns12g /n/r how to design and implement hive table with partitions? learn here with the help of an example.\\n\\nhttp://bit.ly/1cfuoox /n/r what are some big data companies in bangalore? \\nto join this discussion visit here: http://goo.gl/zyahwn \\n#hpc #supercomputing /n/r #alibaba focusing on international investment to grow its business by building a financial data platform! checkout #news at: http://goo.gl/psc9tx #bigdata #hpc /n/r can anyone let me know , which of the following #mapreduce execution frameworks focus on execution in shared-memory environments? \\n \\na. hadoop\\nb. twister\\nc. phoenix /n/r how to become our agent?\\nhow much commission you will get?\\nwe mainly offer mobile crushers, stationary crushers, sand-making machines, grinding mills and complete plants.\\nhttp://www.crushers-mills.com/quotation-for-new-type-crawler-mobile-crusher/\\nhttp://www.jaw-crushers.org/infonews/agent-distributor-needed.html /n/r can anyone let me know , which of the following utilities allows to create and run #mapreduce jobs with any executable or script as the mapper or the reducer? \\n\\na. oozie\\nb. sqoop\\nc. flume\\nd. hadoop streaming /n/r can india compete with china in #bigdata, #cloudcomputing and the #internetofthings?\\nto join this discussion click here: http://goo.gl/gerv5h /n/r can anyone let me know , which of the following responsibility of the job tracker have been split into separate daemons in the #mapreduce v2 ? \\n\\na. managing files\\nb. job scheduling\\nc. resource management\\nd. mr job statistics /n/r brazilian #cloudcomputing market to reach $1.1bn by 2017! checkout #news at: http://goo.gl/irj6cz #hpc #supercomputing /n/r what topics of #bigdata should a #datascientist know and why? \\nto join this discussion visit here: http://goo.gl/i0do0q \\n#hpc #supercomputing /n/r big isn\\'t big anymore! checkout complete #article at: http://goo.gl/7c0fks #hpc #supercomputing #bigdata /n/r why is python a language of choice for #datascientists?\\nto join this #discussion visit our site at: http://goo.gl/st6fbe \\n#hpc #supercomputing /n/r salesforce working out with #bigdata, new analytics tool! checkout complete #news at: http://ow.ly/nh3oo #hpc #supercomputing /n/r in a #mapreduce program, the reducer receives all values associated with the same key. in your opinion ,which statement is most accurate about the ordering of these values? \\n\\na) the values will be ordered in which they were emitted from the mappers\\nb) values are in sorted order\\nc) values are arbitarily ordered, but multiple runs of the same mapreduce job will have the same ordering\\nd) since the values come from mappers o/p, there will be contiguous sections of sorted values.\\ne) the values are arbitarily ordered, and the ordering may vary from run to run of the same mapreduce job /n/r what are the downsides of being a #datascientist? \\nto join this discussion visit here: http://goo.gl/itndmz \\n#hpc #bigdata /n/r \"bhaskara\",a #supercomputer to help meteorologist in weather forecasting! checkout #news at: http://goo.gl/3fm2dl \\n#hpc #supercomputing /n/r is a masters in #datascience worth it? \\nto join this #discussion: http://goo.gl/7vca2m #hpc #supercomputing /n/r #bigdata in financial services! checkout complete #blog at: http://goo.gl/vtvwxx \\n#hpc #supercomputing /n/r #bigdata in financial services! checkout complete #blog at: http://goo.gl/vtvwxx \\n#hpc #supercomputing /n/r can anyone let me know , which tacc resource has support for #hadoop mapreduce?\\n\\na. ranger\\nb. longhorn\\nc. lonestar\\nd. spur /n/r china-guiyang #bigdata expo! checkout complete #news at: http://ow.ly/nu0it #hpc #supercomputing /n/r \"culture of cloud in #asia\" \\nto checkout complete #blog visit here: http://ow.ly/ntezx \\n#hpc #supercomputing #cloudcomputing /n/r according to you , which of the following statement is true when archiving #hadoop files ? \\n\\na.archived files will display with the extension .arc\\nb mapreduce processes the original files names even after files are archived\\nc archived files must be unarchived for hdfs and mapreduce to access the original, small files\\nd archive is intended for files that need to be saved but no longer accessed by hdfs /n/r #bigdata and big brother: do you care to be watched? checkout #blog at: http://goo.gl/quln51   #hpc /n/r is the iot the next hype after #bigdata? \\nto join this #discussion visit here: http://goo.gl/4hhyb9\\n#hpc #supercomputing /n/r can anyone let me know,which of the following is not an input format in #hadoop ? \\n\\na textinputformat\\nb  byteinputformat\\nc sequencefileinputformat\\nd keyvalueinputformat /n/r #bigdata: changing the e-market as we know it! checkout complete #blog at: http://goo.gl/buec9y #hpc /n/r #bigdata: changing the e-market as we know it! checkout complete #blog at: http://goo.gl/buec9y #hpc /n/r #bigdata and the #internetofthings (iot) are expected to give a boost to taiwan\\'s information services industry! checkout complete #news at: http://goo.gl/yomm0x #hpc /n/r can anyone let me know , what should be carefully coordinated by an administrator when decommissioning multiple datanodes in a cluster in the context of #bigdata ?\\n\\na.immediately after decommissioning, the web ui count for \"dead nodes\" should increase.\\nb.tasktracker should be stopped in rapid succession on all the decommissioned datanodes.\\nc.a sufficient number datanodes should remain to provide adequate data replication.\\nd.the jobtracker \"exclude\" file on the datanodes should be read within 1 minute of the namenode reading its \"exclude\" file. /n/r for a newbie what is the best way to start with #bigdata technology?\\nto join this discussion visit here: http://ow.ly/nhbhf \\n#hpc #supercomputing /n/r are #supercomputers hidden power center of silicon valley? checkout #news at: http://goo.gl/mwt3tj \\n#hpc #supercomputing /n/r don\\'t miss our free webinar today . learn #bigdata and hadoop developer course for better job opportunities with bigger organisations on the technology front . click here to register \\nhttp://www.greycampus.com/training/big-data-hadoop-webinar /n/r #datascience and #bigdata \\nare you interested in the internet of things (iot)?\\ncome and join our free live webinar where cisco funded mobstac will talk to you about how they use hadoop in iot beacon technology.\\nclick here to register - https://attendee.gotowebinar.com/register/3758402689617729025 /n/r do large companies use r for data analysis? or is it #sas or #hadoop, etc.? \\njoin discussion at: http://goo.gl/tsoqvj #bigdata #hpc /n/r which is better, an ms in data science or a #datacience certification from coursera? \\nto join discussion on this topic visit here: http://goo.gl/6dqjzc \\n#hpc #supercomputing /n/r cloud bigtable launched by google to store #bigdata ! \\ncheckout complete #news at: http://goo.gl/nwy2gw #hpc #supercomputing /n/r stay up to date on what\\'s happening in #hpc, #cloudcomputing and #bigdata by subscribing to our #newsletter at: http://hpc-asia.com/subscribe/ /n/r stay up to date on what\\'s happening in #hpc, #cloudcomputing and #bigdata by subscribing to our #newsletter at: http://hpc-asia.com/subscribe/ /n/r what are some #dataanalysis projects i can do as a #datascience beginner?\\nto join this discussion visit here: http://goo.gl/ftmqx2 \\n#hpc #supercomputing /n/r join #discussion on this topic: \"how is #bigdata stored? do they use #databases like oracle? how do they extract information from them?\" at: http://goo.gl/cbczvn /n/r big data has become a growing phenomenon over a period of time but what you need to know is the nature of \\'unstructured data\\' and how to deal with it. still confused what it is? get to know here.\\n\\nhttp://bit.ly/1e8ds6v /n/r checkout latest #infographic on: \"save money with big data\"! \\nfor complete infographic visit here: http://goo.gl/na8qb2 \\n#bigdata #hpc /n/r understanding dimensionality reduction- principal component analysis and singular value decomposition! \\ncheckout #blog at: http://goo.gl/rjdqzs #bigdata #hpc /n/r big data is helping the government. research optimus tells you how - http://www.researchoptimus.com/blog/6-ways-big-data-is-improving-government-effectiveness/ /n/r stay up to date on what\\'s happening in #hpc, #cloudcomputing and #bigdata by subscribing to our #newsletter at: http://hpc-asia.com/subscribe/ /n/r did you know that 80% of data collected by companies is wasted?\\nhttp://www.domo.com/assets/downloads/15_bi-guide.pdf /n/r still time to register for today\\'s free webinar on datameer\\'s big data analytics platform on bigsteps full metal cloud. \\n\\ntoday,  3:00pm gmt . register here: http://bit.ly/datameer-on-fullmetal /n/r hey #bigdata fans, if big data is your forte, then write an article on big data & sports and participate in the big blogger #contest to get a chance to win inr 10,000 gift voucher\\nclick here to participate: cwc15.hpc-asia.com/contest/ #cwc15 #sports /n/r join tomorrow a free webinar about datameer\\'s big data analytics platform on bigsteps full metal cloud. \\n\\nregister here: http://bit.ly/datameer-on-fullmetal /n/r i\\'ve been trying to learning more about business intelligence lately and found these great memes about it. i also found this new bi guide, which was super helpful. enjoy! http://www.domo.com/assets/downloads/15_bi-guide.pdf? /n/r hi, is there anyone here that knows something about xpath? please help me for some easy query, thanks /n/r hi all, let me know if you anybody need training on hadoop / apache spark. you can reach me at datamaking.consultancy@gmail.com for more details. /n/r please register for hadoop online demo class-march-18-2015 on mar 18, 2015 7:30 pm ist at:\\n\\nhttps://attendee.gotowebinar.com/register/1860620330025381890\\n\\nagenda:\\n        \\nwhat is data?\\nmeasuring data?\\nwhat is big data?\\nhow big data is generated?\\nhadoop is the solution to big data?\\nwhy so demand for big data?\\nbig data jobs?\\nfuture of big data?\\nbig data characteristics ? (5v\\'s)\\nwhat is hadoop ?\\npre- requisites to learn hadoop?\\ndifference between hadoop vs rdbms?\\ncourse content? /n/r register link:\\nhttps://docs.google.com/forms/d/19dwj9hslm_upsnilnjpngv5xrltshg3pfmvpbxltjnu/viewform /n/r hey #cricket fans, if big data is your forte, then write an article on big data & sports and participate in the big blogger #contest to get a chance to win inr 10,000 gift voucher\\nclick here to participate: cwc15.hpc-asia.com/contest/ #cwc15 #sports /n/r \"6 rising #bigdata stars in asia & us\" read more at: http://goo.gl/uniklc  #hpc #supercomputing /n/r hey #bigdata fans, if big data is your forte, then write an article on big data & sports and participate in the big blogger #contest to get a chance to win inr 10,000 gift voucher\\nclick here to participate: cwc15.hpc-asia.com/contest/ #cwc15 #sports /n/r start-up analytics: what should your first data hire do for you?\\n\\ni\\'m speaking about instaedu / chegg data science at innovation enterprise\\'s predictive analytics summit in san diego. registration is now open! #bigdata #analytics #datascience /n/r top downloaded papers by #sas software on #bigdata in 2014 https://lnkd.in/eupajrf #hadoop #nosql /n/r where can i find statistics exercises with answers ? /n/r sign up now for b.telligent\\'s free webinar \"better forecasting\" in january! more info\\'s here: http://ow.ly/d9fbx /n/r interesting breakdown of the key data scientist skills. \\n\\nwhich archetype are you? db, dc, dd or dr? i\\'d love to get some small data from the comments below, and see what the trend is. \\n\\nhttp://dataconomy.com/the-22-skills-of-a-data-scientist/ /n/r send resumes to: oacacademy@gmail.com\\nopenings for bca or b.sc graduates\\n\\neducation: bca or b.sc (computer science/ electronics/ mathematics/ physics/ statistics / information technology /information science only)\\njob role: operations executive / testing executive\\neligibility criteria:\\n\\x96 bca or b.sc graduates (computer science/ electronics/ mathematics/ physics/ statistics / information technology / information science\\nonly)\\n\\x96 candidates who have graduated in 2008, 2009 & 2010 with experience of up to 24 months are eligible to apply\\nsimple average aggregate of 60% throughout class x, xii & graduation.\\nshould not have participated in the infosys selection process in the last 9 months.\\nshould have good communication skills.\\nshould be willing to relocate and work in a 247 environment.\\nsend resumes to: oacacademy@gmail.com or call at 9717244459\\nhttps://www.facebook.com/groups/288324188045174/ /n/r the #doag today with a presentation by b.telligent. the topic: bitemporal data modeling as supreme discipline! http://ow.ly/i/7dzoa /n/r today starts the 3-day #doag user conference - also on board: b.telligent! #oracle /n/r send resumes to: oacacademy@gmail.com\\nopenings for bca or b.sc graduates\\ncompany: infosys\\neducation: bca or b.sc (computer science/ electronics/ mathematics/ physics/ statistics / information technology /information science only)\\njob role: operations executive / testing executive\\neligibility criteria:\\n\\x96 bca or b.sc graduates (computer science/ electronics/ mathematics/ physics/ statistics / information technology / information science\\nonly)\\n\\x96 candidates who have graduated in 2008, 2009 & 2010 with experience of up to 24 months are eligible to apply\\nsimple average aggregate of 60% throughout class x, xii & graduation.\\nshould not have participated in the infosys selection process in the last 9 months.\\nshould have good communication skills.\\nshould be willing to relocate and work in a 247 environment.\\nsend resumes to: oacacademy@gmail.com or call at 9717244459\\nhttps://www.facebook.com/groups/288324188045174/ /n/r send resumes to: oacacademy@gmail.com\\nopenings for mba finance- freshers\\ncompany: infosys, hcl, genpact\\neducation: mba-finance\\nother optional requirements:\\n\\x96 ncfm (nse certifications in financial markets.)\\n\\x96 icwai certification\\\\ cfa level one certification \\njob role: data management services and client support\\neligibility criteria:\\n\\x96 mba-finance\\n\\x96 10th and 12th from cbse board preferably\\n\\x96 cutoff: 60 percent marks in 10th and 12th. are eligible to apply\\n\\x96 should have good communication skills.\\n\\x96 should be willing to relocate and work in a 247 environment.\\n\\x96 immediate joining required\\nsend resumes to: oacacademy@gmail.com or call at 9717244459\\nhttps://www.facebook.com/groups/288324188045174/ /n/r hi all,\\n                i am trying to achieve the following requirement in pig and struck up in the middle, can you someone through inputs(tips) on this to proceed further.\\n\\nmy sample input data set:\\ncol1,col2,col3,col4\\nindia,chennai,1,5\\nindia,mumbai,1,3\\nindia,bangalore,1,2\\nusa,california,1,4\\nusa,boston,1,6\\n\\nmy output should be:(sum up rows value for col3 & col4 based on col1)\\ncol1,col2,col3,col4\\nindia,chennai,3,10\\nindia,mumbai,3,10\\nindia,bangalore,3,10\\nusa,california,1,4\\nusa,boston,1,6\\n\\ni have tried the following steps to achieve it, i am in the middle:\\n\\ndb_dataset = load \\'/data/testdata.csv\\' using pigstorage(\\',\\');\\n\\ndb_dataset_grp = group db_dataset by $0;\\n\\ndb_dataset_new = foreach db_dataset_grp generate group as country, bagtotuple(db_dataset.$1) as city, sum(db_dataset.$2) as count, sum(db_dataset.$3) as sales_amt;\\n(usa,(california,boston),2.0,10.0)\\n(india,(chennai,mumbai,bangalore),3.0,10.0)\\n\\ndb_dataset_new_split = foreach db_dataset_new generate country, tobag(city) as city, count, sales_amt;\\n(usa,{(california,boston)},2.0,10.0)\\n(india,{(chennai,mumbai,bangalore)},3.0,10.0)\\n \\ndb_dataset_new_split_act = foreach db_dataset_new_split generate country, flatten(city) as city; this is not transforming tuple or bag into rows.\\n\\nthanks. /n/r send resumes to: oacacademy@gmail.com\\nopenings for bca or b.sc graduates\\ncompany: infosys\\neducation: bca or b.sc (computer science/ electronics/ mathematics/ physics/ statistics / information technology /information science only)\\njob role: operations executive / testing executive\\neligibility criteria:\\n\\x96 bca or b.sc graduates (computer science/ electronics/ mathematics/ physics/ statistics / information technology / information science\\nonly)\\n\\x96 candidates who have graduated in 2008, 2009 & 2010 with experience of up to 24 months are eligible to apply\\nsimple average aggregate of 60% throughout class x, xii & graduation.\\nshould not have participated in the infosys selection process in the last 9 months.\\nshould have good communication skills.\\nshould be willing to relocate and work in a 247 environment.\\nsend resumes to: oacacademy@gmail.com or call at 9717244459\\nhttps://www.facebook.com/groups/288324188045174/ /n/r our b.telligent experts obviously had fun at the #swisscom halloween party! here are some impressions: /n/r the #predictiveanalytics world starts today-live with b.telligent and our #bi expert michael allg?wer! /n/r send resumes to: oacacademy@gmail.com\\nopenings for bca or b.sc graduates\\ncompany: infosys\\neducation: bca or b.sc (computer science/ electronics/ mathematics/ physics/ statistics / information technology /information science only)\\njob role: operations executive / testing executive\\neligibility criteria:\\n\\x96 bca or b.sc graduates (computer science/ electronics/ mathematics/ physics/ statistics / information technology / information science\\nonly)\\n\\x96 candidates who have graduated in 2008, 2009 & 2010 with experience of up to 24 months are eligible to apply\\nsimple average aggregate of 60% throughout class x, xii & graduation.\\nshould not have participated in the infosys selection process in the last 9 months.\\nshould have good communication skills.\\nshould be willing to relocate and work in a 247 environment.\\nsend resumes to: oacacademy@gmail.com or call at 9717244459\\n\\nhttps://www.facebook.com/groups/288324188045174/ /n/r send resumes to: oacacademy@gmail.com\\nopenings for bca or b.sc graduates\\ncompany: infosys\\neducation: bca or b.sc (computer science/ electronics/ mathematics/ physics/ statistics / information technology /information science only)\\njob role: operations executive / testing executive\\neligibility criteria:\\n\\x96 bca or b.sc graduates (computer science/ electronics/ mathematics/ physics/ statistics / information technology / information science\\nonly)\\n\\x96 candidates who have graduated in 2008, 2009 & 2010 with experience of up to 24 months are eligible to apply\\nsimple average aggregate of 60% throughout class x, xii & graduation.\\nshould not have participated in the infosys selection process in the last 9 months.\\nshould have good communication skills.\\nshould be willing to relocate and work in a 247 environment.\\nsend resumes to: oacacademy@gmail.com or call at 9717244459\\n\\nhttps://www.facebook.com/groups/288324188045174/ /n/r please join my group for current opening for bpo & kpo\\n\\noff campus placement cell - ncr ( delhi ) bpo - kpo \\n\\nhttps://www.facebook.com/groups/288324188045174/ /n/r what do you think if ols does not fulfil assumption of normality of residuals in large sample data with n=600, does it give acceptable results in the field of finance and accounting? /n/r now the top job company b.telligent also receives the certificate #berufundfamilie. voices can be found here: /n/r ????? ????? ???? ????? ????????? ??????? ?????\\n????? ????????\\nhttps://www.youtube.com/watch?v=9rxzrtucgxw /n/r anyone here interested in working on economic graphs ? /n/r rackspace adds #bigdata-capable servers to onmetal service - http://bit.ly/1waotch /n/r platfora\\'s #bigdata iceberg and other stories - http://bit.ly/101iylk /n/r the internal b.telligent university has started again. today with the theme presentation training. we are excited! http://ow.ly/i/7ersw /n/r i\\'m giving a data science talk at occidental college on friday.  if you know people who are interested in transitioning from academia to tech send them my way:\\n\\nwhat: physicist to data scientist\\ntime: october 17th, 4:00pm\\nlocation: hameetman 124, occidental college /n/r #bigdata can guess who you are based on your #zipcode - http://bit.ly/1spphyc /n/r #salesforce makes its #bigdata move - http://bit.ly/1yxh4qa /n/r #bigdata needs big brains - http://bit.ly/1vpycxx /n/r intel execs on #bigdata and privacy: it\\'s a balancing act - http://bit.ly/1czpf9v /n/r peter thiel: #mobilecomputing, #bigdata, and edu software nothing but buzzwords - http://bit.ly/1vm6vw7 /n/r fortune-tellers, step aside: #bigdata looks for future entrepreneurs - http://bit.ly/1vl1nsu /n/r how #bigdata is fueling a new age in space exploration - http://bit.ly/1viaxqa /n/r join free live demo on big data and hadoop\\nenroll now - http://goo.gl/dgpzdb /n/r join free live demo session on \"big data and hadoop\"\\nregister now! -  http://goo.gl/ah9omj /n/r how the #bigdatarevolution can help #designidealcities - http://bit.ly/1vfyu78 /n/r register your self for \"big data and hadoop training\"   \\nenroll now - http://goo.gl/d5yjwl /n/r #bigdata in marketing: how to gain the advantage - http://bit.ly/zb2skh /n/r haha ;)\\ndata analytics server is less complicated than girls?\\n\\nlike this page- www.facebook.com/letstalkanalytics /n/r #hadoopers come on...answer this simple question..\\n\\nlike this interesting #analytics page- www.facebook.com/letstalkanalytics /n/r become big data and hadoop expert\\nenroll now for free live demo - goo.gl/2mnacf /n/r fighting discrimination- with #bigdata - http://bit.ly/1sdqly1 /n/r #bigdata breakthrough to boost #childprotectionservices - http://bit.ly/1sufuzo /n/r live webinar on big data and hadoop today 6:30 ist\\n\\nenroll now - http://goo.gl/7usv75 /n/r hello frnds... pls... like & share dis broucher photo ..on this #vvpiste2014 event page... :-) /n/r the breakthrough technology that will turbocharge #bigdata and #cloudcomputing\\nhttp://bit.ly/1sfo8ss /n/r how clearstory offers #bigdata blending for easy insights - http://bit.ly/wqon72\\n#bigdatablendingforeasyinsights /n/r free webinar on \"big data and hadoop\" today \\nregister now - http://goo.gl/uu3t4e /n/r #bigdata \\x97 the aggregation and analysis of #datatooptimize performance \\x97 is transforming modern society. sport, which has always served as a reflection of society, is no exception. while calculations have been employed since the dawn of athletics, the latest innovations in data are poised to trigger a revolution - http://bit.ly/1lvitc9 /n/r join big data and #hadoop live webinar at easylearning guru\\nenroll now - http://goo.gl/dbw23p /n/r what do you do when a girl kindly recommends you a free o\\'reilly  nosql webcast which will be live tomorrow?:) you sign up:)\\nyou will get the chance to listen to ben lorica and find interesting things about how impala, elasticsearch & couchbase performs when scaled vertically and horizontally.\\njoin for free: http://oreil.ly/vh1vvk /n/r what do you do when a girl kindly recommends you an o\\'reilly free nosql webcast which will be live tomorrow?:) you sign up:)\\n\\nyou will get the chance to listen to ben lorica and find interesting things about how impala, elasticsearch & couchbase performs when scaled vertically and horizontally.\\n\\njoin for free: http://oreil.ly/vh1vvk /n/r about ebm? /n/r unicom learning - globally renowned leading knowledge sharing and conference organizers in emerging technology and processes (agile, project management, testing, programming, big data, hadoop, cloud, mobility, internet of things, uxui design, quality, social, analytics and security) is organizing a conference on big data dated 13th august in sydney and 15th august in melbourne, australia. this is a unique opportunity for all executives and professionals involved in data storage, data management and data analysis to gather and discuss how companies can effectively manage, protect and leverage the growing amounts of data in the enterprise. with a focus on best practices, our events allow attendees to explore strategies and technologies surrounding real-time data processing, data protection and privacy, meeting industry regulations and compliance, and data storage and provide you the opportunity to meet big data experts, attend keynote presentation, and participate in workshop and interactive sessions and network with experienced business and technical practitioners.\\n\\nfor any more queries kindly contact us on contact@unicomlearning.com\\nfor registrations kindly visit the link:\\nsydney->http://bit.ly/1nwjh0f\\nmelbourne->http://bit.ly/1p527me /n/r online sas course\\n\\nfree demo on:- 01.07.2014\\n\\nregister now at \\x96pragnaeduvercity@gmail.com                 \\n\\nwe offers high grade online courses in base sas certification,advance sas certification,clinical sas,finance sas,sas bi and di with projects.\\n\\nlearning the courses online saves time as well as students from different geographical locations can participate in the training session.\\n\\nwe provide assistance for fresher to enter into the professional field of clinical sas, finance sas and sas bi as well to the professionals who wish to achieve higher goals in their career.\\n\\nfor more details please contact -  neha.eduvercity@gmail.com\\n                                                      pragnaeduvercity@gmail.com                                  \\n\\nphone- 08050922145 /n/r hi, here is an awesome course to nail r programming. subscribe to the channel for future lesson updates.\\n#rstats #datascience #statistics\\n\\nhttp://goo.gl/oy0ccb /n/r my company has some great big data positions to fill here in austin. please let me know if you are interested! :) /n/r #oracle unveils #mysqlfabric http://lnkd.in/bm5hejs #noql #bigdata #hadoop /n/r get a free ipad mini! /n/r nouveau dossier special :\\n\"big data et ressources humaines\"\\n\\n-> http://www.myrhline.com/dossier-rh/big-data-et-ressources-humaines /n/r nouveau dossier special :\\n\"big data et ressources humaines\"\\n\\n-> http://www.myrhline.com/dossier-rh/big-data-et-ressources-humaines /n/r i am compiling a list of free resources for data science, data viz, and big data... here is what i have so far. do you have anything to add to the list? if so, will you please add it to the comments section of this note? :) i would so appreciate it and i am sure others would too. <3 /n/r #splunk\\'s hunk 6.1 enables faster #analytics for #hadoop and #nosql data stores   http://lnkd.in/bkpkpxt #bigdata /n/r get the insider scoop on big data and data science in the government sector...\\n\\na brazen expose on big data in the government sector\\nhttp://www.data-mania.com/index.php/easyblog/170-a-brazen-expose-on-big-data-in-the-government-sector /n/r the program is directed by prof. dr. ram?n reichert, whose new book will appear soon: http://www.amazon.de/big-data-analysen.../dp/3837625923. website data studies: http://www.donau-uni.ac.at/de/studium/data-studies/index.php /n/r are you a us data scientist, advanced #data analyst or statistician? \\n\\nif so, you can get paid $150 for completing a 2 hr survey. \\n\\nping me for details #bigdata /n/r using #scala to work with #hadoop http://lnkd.in/bqnned5 #cloudera #bigdata #nosql #hdfs #mapr /n/r data exhaust created today is humongous and the need to understand its potential has become more of a necessity than just an interest ! lets explore /n/r privacy preserving in big data, has been major issue for now, please discuss regarding dat too /n/r #mongodb #nosql database interview questions http://lnkd.in/b7sjy7d #hadoop #mapreduce #hive #cassandra, #couchdb /n/r *making pretty data visualizations with python*\\n\\nmy latest post for astrobetter shows you how to make a 2d density plot with histograms projected along each axis.\\n\\nread more at astrobetter:\\nhttp://www.astrobetter.com/visualization-fun-with-python-2d-histogram-with-1d-histograms-on-axes/\\n\\n#bigdata #visualizations #python /n/r i\\'m giving an #astronomy to #datascience #career #talk today at uc berkeley.  come on down:  \\n\\nastronomy career development seminar\\ntuesday (12/17) at 5pm in hfa b1. /n/r 10 days executive sas base certification training in bangalore (1st step towards big data & business intelligence carrier)\\n\\nobjective of training: \\n\\nbase sas certification training \\nequipped with hands on experience with sas\\nclear and strong concepts for big data and bi \\n\\nduration & format of the course: \\nfast track (10 days (30 hrs)) \\nfrom 15th dec to 25th dec \\n\\ntake away \\n* practice software \\x96 limited edition \\n* guide for sas certification \\x96 hard copy \\n* assignment copy \\x96 hard copy \\n* course material \\x96 soft copy \\n* training notes \\x96 soft copy \\n* exam preparation questions \\x96 soft copy \\n\\nfees for training program \\n* 12,000 + 12.36% taxes \\n\\nwho can pursue sas training & certification course. \\nas sas is an analytical tool this can be pursuit by any graduates & pgs students from any discipline. (mbas, commerce, engineering life sciences, arts, medicine, mcas statistics etc.). \\nsas is also for the professionals who are already working and wanted to make a good carrier move in their respective domain. \\n\\naddress:\\n e2e projects pvt. ltd\\n no: 182, 1st floor, 13th main, 7th cross\\n sector 5, hsr layout, bangalore \\nlandmark: behind devi eye hospital, beside prakash dental hospital, near bda bridge.\\n\\n contact person :neha (08050922145)\\n\\n mail id : neha.j@eduvercity.com\\n\\n time : 10am to 1 pm /n/r ijournals\\ncall for papers\\nvolume 1 issue 4\\ninternational journal of software & hardware research in engineering(issn: 2347-4890)\\nsubmission deadline: 10 december\\npublication date: 15 december\\nwww.ijshre.com\\n\\nvolume 1 issue 1\\ninternational journal of social relevance & concern(issn: applied)\\nsubmission deadline: 30 november\\npublication date: 1 december\\nsubmit your mauscript at: ijshre@gmail.com or editor@ijournals.in\\nwebsite: www.ijournals.in /n/r is your boss talking about big data? here\\'s two impressive bits of information related to analytics and processing that you can share (written by yours truly): http://ar.gy/4wtl /n/r xplenty\\'s code-free hadoop as a service platform enables companies of all sizes to utilize their big data without prior programming knowledge.\\n\\nhttp://online.wsj.com/article/pr-co-20130417-911716.html /n/r anyone interested in working with osm to make the better place today? /n/r are you on twitter?? if so, show the love!! please vote for me by tweeting the following :) - i would really appreciate it!!\\n\\n\"i nominate @bigdatagal for the #bigdata100 most influential #bigdatatwitter accounts...\" #opendata #bigdata /n/r thank you all so much for supporting my facebook page!! i am working on turning it into a forum or portal by which data lovers can meet, greet & collaborate....  i just need a little custom development work to get the features built. hopefully it would not take long. /n/r i just invited mandi bishop to this group - she will be perf here.  hey mandi, should we invite the other mythical unicorns... probably, huh...\\n\\ni am not on fb with them though /n/r thank you for the invite and add, lillian pierson! look forward to joining in the conversation. /n/r thanks for the invite and for the add!!! /n/r '"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_start_token = \"SENTENCESTART\"\n",
    "sentence_end_token = \"SENTENCEEND\"\n",
    "\n",
    "female_text = female_text.replace(r'\\[.*?\\]|\\(.*http.+\\)|\\(.*https.+\\)|\\<.*http.+\\>', '')\n",
    "female_text = female_text.replace(r'Rado([^\\s]+)|Skarp([^\\s]+)', '')\n",
    "female_text = female_text.replace(r'\\=[A-Z|0-9][A-Z|0-9]|\\=', '')\n",
    "#female_text = female_text.replace('\\n',' '+ line_break + ' ')\n",
    "female_text = female_text.replace('\\r','')\n",
    "female_text = female_text.replace('--',' ')\n",
    "female_text = female_text.lower()\n",
    "female_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we split the main text into individual words and create a bag of words with repeats. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import text_to_word_sequence\n",
    "female_text2 = text_to_word_sequence(female_text, lower=False, split=\" \") #using only 10000 first words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['mohammed',\n",
       " 'nazili',\n",
       " \"suddicqui's\",\n",
       " 'post',\n",
       " 'advertising',\n",
       " 'a',\n",
       " 'payment',\n",
       " 'gateway',\n",
       " 'is',\n",
       " 'removed',\n",
       " 'for',\n",
       " 'a',\n",
       " 'second',\n",
       " 'time',\n",
       " 'third',\n",
       " 'instance',\n",
       " 'will',\n",
       " 'result',\n",
       " 'in',\n",
       " 'the',\n",
       " 'member',\n",
       " 'being',\n",
       " 'removed',\n",
       " 'from',\n",
       " 'the',\n",
       " 'group',\n",
       " 'n',\n",
       " 'r',\n",
       " 'new',\n",
       " 'member',\n",
       " 'post',\n",
       " 'advertising',\n",
       " 'a',\n",
       " 'payment',\n",
       " 'gateway',\n",
       " 'has',\n",
       " 'been',\n",
       " 'removed',\n",
       " 'n',\n",
       " 'r',\n",
       " 'this',\n",
       " 'is',\n",
       " 'amazing',\n",
       " 'n',\n",
       " 'r',\n",
       " 'we',\n",
       " 'need',\n",
       " '20',\n",
       " 'volunteers',\n",
       " 'to']"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "female_text2[0:50]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, initialize the tokenizer to create the sequences and fit the text onto it, with nb_words=900 representing top 900 words in the text.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "token = Tokenizer(nb_words=900,char_level=False)\n",
    "token.fit_on_texts(female_text2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text_mtx = token.texts_to_matrix(female_text2, mode='binary')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each word will be represented by a vector of size 900, and the the row will show 1 where the row word matched the word column if its in the top 900 words.\n",
    "\n",
    "for that, we use text_to_matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(23170, 900)"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_mtx.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23170"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(female_text2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vocab = pd.DataFrame({'word':female_text2,'code':np.argmax(text_mtx,axis=1)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vocab=vocab.drop_duplicates()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>code</th>\n",
       "      <th>word</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>mohammed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11064</th>\n",
       "      <td>0</td>\n",
       "      <td>kaggle</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11070</th>\n",
       "      <td>0</td>\n",
       "      <td>reddit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11077</th>\n",
       "      <td>0</td>\n",
       "      <td>coming</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11081</th>\n",
       "      <td>0</td>\n",
       "      <td>conf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11085</th>\n",
       "      <td>0</td>\n",
       "      <td>keynotes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11089</th>\n",
       "      <td>0</td>\n",
       "      <td>daiquiris</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11095</th>\n",
       "      <td>0</td>\n",
       "      <td>end</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11105</th>\n",
       "      <td>0</td>\n",
       "      <td>bucks</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11106</th>\n",
       "      <td>0</td>\n",
       "      <td>extractconf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11129</th>\n",
       "      <td>0</td>\n",
       "      <td>0qgydp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11187</th>\n",
       "      <td>0</td>\n",
       "      <td>correct</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11053</th>\n",
       "      <td>0</td>\n",
       "      <td>e989nx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11195</th>\n",
       "      <td>0</td>\n",
       "      <td>blocks</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11204</th>\n",
       "      <td>0</td>\n",
       "      <td>false</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11211</th>\n",
       "      <td>0</td>\n",
       "      <td>knowing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11226</th>\n",
       "      <td>0</td>\n",
       "      <td>opencampus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11231</th>\n",
       "      <td>0</td>\n",
       "      <td>1ev7wu2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11252</th>\n",
       "      <td>0</td>\n",
       "      <td>m8hvnv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11309</th>\n",
       "      <td>0</td>\n",
       "      <td>hush</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11317</th>\n",
       "      <td>0</td>\n",
       "      <td>leaving</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11319</th>\n",
       "      <td>0</td>\n",
       "      <td>stone</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11320</th>\n",
       "      <td>0</td>\n",
       "      <td>unturned</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11327</th>\n",
       "      <td>0</td>\n",
       "      <td>harness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11335</th>\n",
       "      <td>0</td>\n",
       "      <td>particular</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11200</th>\n",
       "      <td>0</td>\n",
       "      <td>parallel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11031</th>\n",
       "      <td>0</td>\n",
       "      <td>1jg915b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11026</th>\n",
       "      <td>0</td>\n",
       "      <td>modernized</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11024</th>\n",
       "      <td>0</td>\n",
       "      <td>pioneers</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10863</th>\n",
       "      <td>0</td>\n",
       "      <td>resxsl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13917</th>\n",
       "      <td>870</td>\n",
       "      <td>led</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15399</th>\n",
       "      <td>871</td>\n",
       "      <td>latest</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9988</th>\n",
       "      <td>872</td>\n",
       "      <td>knows</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>683</th>\n",
       "      <td>873</td>\n",
       "      <td>show</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16991</th>\n",
       "      <td>874</td>\n",
       "      <td>allowing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3186</th>\n",
       "      <td>875</td>\n",
       "      <td>format</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11237</th>\n",
       "      <td>876</td>\n",
       "      <td>actual</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10416</th>\n",
       "      <td>877</td>\n",
       "      <td>tf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10361</th>\n",
       "      <td>878</td>\n",
       "      <td>changing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5293</th>\n",
       "      <td>879</td>\n",
       "      <td>age</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13038</th>\n",
       "      <td>880</td>\n",
       "      <td>behind</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5869</th>\n",
       "      <td>881</td>\n",
       "      <td>everything</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7050</th>\n",
       "      <td>882</td>\n",
       "      <td>carrier</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11904</th>\n",
       "      <td>883</td>\n",
       "      <td>terms</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2956</th>\n",
       "      <td>884</td>\n",
       "      <td>scree</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19030</th>\n",
       "      <td>885</td>\n",
       "      <td>datanodes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13830</th>\n",
       "      <td>886</td>\n",
       "      <td>allow</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11241</th>\n",
       "      <td>887</td>\n",
       "      <td>worked</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>468</th>\n",
       "      <td>888</td>\n",
       "      <td>book</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2344</th>\n",
       "      <td>889</td>\n",
       "      <td>quick</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>543</th>\n",
       "      <td>890</td>\n",
       "      <td>trying</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10170</th>\n",
       "      <td>891</td>\n",
       "      <td>blogspot</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8898</th>\n",
       "      <td>892</td>\n",
       "      <td>innovation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2735</th>\n",
       "      <td>893</td>\n",
       "      <td>registrations</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3209</th>\n",
       "      <td>894</td>\n",
       "      <td>variance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2370</th>\n",
       "      <td>895</td>\n",
       "      <td>streaming</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>389</th>\n",
       "      <td>896</td>\n",
       "      <td>considered</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6861</th>\n",
       "      <td>897</td>\n",
       "      <td>interactive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5941</th>\n",
       "      <td>898</td>\n",
       "      <td>beginner</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7245</th>\n",
       "      <td>899</td>\n",
       "      <td>ng</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3937 rows  2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       code           word\n",
       "0         0       mohammed\n",
       "11064     0         kaggle\n",
       "11070     0         reddit\n",
       "11077     0         coming\n",
       "11081     0           conf\n",
       "11085     0       keynotes\n",
       "11089     0      daiquiris\n",
       "11095     0            end\n",
       "11105     0          bucks\n",
       "11106     0    extractconf\n",
       "11129     0         0qgydp\n",
       "11187     0        correct\n",
       "11053     0         e989nx\n",
       "11195     0         blocks\n",
       "11204     0          false\n",
       "11211     0        knowing\n",
       "11226     0     opencampus\n",
       "11231     0        1ev7wu2\n",
       "11252     0         m8hvnv\n",
       "11309     0           hush\n",
       "11317     0        leaving\n",
       "11319     0          stone\n",
       "11320     0       unturned\n",
       "11327     0        harness\n",
       "11335     0     particular\n",
       "11200     0       parallel\n",
       "11031     0        1jg915b\n",
       "11026     0     modernized\n",
       "11024     0       pioneers\n",
       "10863     0         resxsl\n",
       "...     ...            ...\n",
       "13917   870            led\n",
       "15399   871         latest\n",
       "9988    872          knows\n",
       "683     873           show\n",
       "16991   874       allowing\n",
       "3186    875         format\n",
       "11237   876         actual\n",
       "10416   877             tf\n",
       "10361   878       changing\n",
       "5293    879            age\n",
       "13038   880         behind\n",
       "5869    881     everything\n",
       "7050    882        carrier\n",
       "11904   883          terms\n",
       "2956    884          scree\n",
       "19030   885      datanodes\n",
       "13830   886          allow\n",
       "11241   887         worked\n",
       "468     888           book\n",
       "2344    889          quick\n",
       "543     890         trying\n",
       "10170   891       blogspot\n",
       "8898    892     innovation\n",
       "2735    893  registrations\n",
       "3209    894       variance\n",
       "2370    895      streaming\n",
       "389     896     considered\n",
       "6861    897    interactive\n",
       "5941    898       beginner\n",
       "7245    899             ng\n",
       "\n",
       "[3937 rows x 2 columns]"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab.sort_values(by=\"code\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Shift to predict the next word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((23169, 900), (23169, 900))"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ = text_mtx[:-1]\n",
    "output_ = text_mtx[1:]\n",
    "\n",
    "input_.shape, output_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Activation, Flatten\n",
    "from keras.layers.wrappers import TimeDistributed\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers.recurrent import LSTM\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers.recurrent import SimpleRNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we create a sequential model format, which is a linear stack of neural network layers. This is one of the formats we learned in class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Sequential()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by adding an embedding layer, that turns positive integersinto dense vectors of fixed size.\n",
    "\n",
    "This layer can only be used as the first layer in a model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.add(Embedding(input_dim=input_.shape[1],output_dim= 42, input_length=input_.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Then, we flatten the results to the dense output layer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.add(Flatten())\n",
    "model.add(Dense(output_.shape[1], activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer='rmsprop',metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we fit model with the words. We chose 10 iterations because each epoch takes over 2 minutes to run and we were short on time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 18535 samples, validate on 4634 samples\n",
      "Epoch 1/10\n",
      "18535/18535 [==============================] - 147s - loss: 4.8267 - acc: 0.0238 - val_loss: 5.2373 - val_acc: 0.0220\n",
      "Epoch 2/10\n",
      "18535/18535 [==============================] - 152s - loss: 4.6053 - acc: 0.0245 - val_loss: 5.0633 - val_acc: 0.0615\n",
      "Epoch 3/10\n",
      "18535/18535 [==============================] - 148s - loss: 4.2276 - acc: 0.1280 - val_loss: 4.8404 - val_acc: 0.1237\n",
      "Epoch 4/10\n",
      "18535/18535 [==============================] - 138s - loss: 3.7995 - acc: 0.1780 - val_loss: 4.6576 - val_acc: 0.1325\n",
      "Epoch 5/10\n",
      "18535/18535 [==============================] - 148s - loss: 3.4621 - acc: 0.2017 - val_loss: 4.6524 - val_acc: 0.1470\n",
      "Epoch 6/10\n",
      "18535/18535 [==============================] - 152s - loss: 3.1937 - acc: 0.2220 - val_loss: 4.5497 - val_acc: 0.1521\n",
      "Epoch 7/10\n",
      "18535/18535 [==============================] - 146s - loss: 2.9725 - acc: 0.2413 - val_loss: 4.8968 - val_acc: 0.1463\n",
      "Epoch 8/10\n",
      "18535/18535 [==============================] - 139s - loss: 2.8208 - acc: 0.2533 - val_loss: 4.7989 - val_acc: 0.1511\n",
      "Epoch 9/10\n",
      "18535/18535 [==============================] - 136s - loss: 2.6910 - acc: 0.2615 - val_loss: 4.7811 - val_acc: 0.1511\n",
      "Epoch 10/10\n",
      "18535/18535 [==============================] - 128s - loss: 2.6038 - acc: 0.2638 - val_loss: 5.1957 - val_acc: 0.1491\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1c213080>"
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(input_, y=output_, batch_size=200, nb_epoch=10, verbose=1, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3.1148451200151661, 0.23820622383357071]"
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score = model.evaluate(input_,output_, verbose=0)\n",
    "score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "We recieved a 23% accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'n'"
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_next(\"hello\",token,model,vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vocab = vocab2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>code</th>\n",
       "      <th>word</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Mohammed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>Nazili</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>Suddicqui's</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>228</td>\n",
       "      <td>post</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>advertising</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>14</td>\n",
       "      <td>a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>824</td>\n",
       "      <td>payment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>gateway</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>16</td>\n",
       "      <td>is</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>631</td>\n",
       "      <td>removed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>13</td>\n",
       "      <td>for</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>14</td>\n",
       "      <td>a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0</td>\n",
       "      <td>second</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>102</td>\n",
       "      <td>time</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>8</td>\n",
       "      <td>SENTENCEEND</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>7</td>\n",
       "      <td>SENTENCESTART</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0</td>\n",
       "      <td>Third</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0</td>\n",
       "      <td>instance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>81</td>\n",
       "      <td>will</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0</td>\n",
       "      <td>result</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>12</td>\n",
       "      <td>in</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>445</td>\n",
       "      <td>member</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>269</td>\n",
       "      <td>being</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>631</td>\n",
       "      <td>removed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>47</td>\n",
       "      <td>from</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>138</td>\n",
       "      <td>group</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>8</td>\n",
       "      <td>SENTENCEEND</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>7</td>\n",
       "      <td>SENTENCESTART</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>4</td>\n",
       "      <td>n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>3</td>\n",
       "      <td>r</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26829</th>\n",
       "      <td>23</td>\n",
       "      <td>with</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26830</th>\n",
       "      <td>435</td>\n",
       "      <td>them</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26831</th>\n",
       "      <td>0</td>\n",
       "      <td>though</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26832</th>\n",
       "      <td>4</td>\n",
       "      <td>n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26833</th>\n",
       "      <td>3</td>\n",
       "      <td>r</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26834</th>\n",
       "      <td>0</td>\n",
       "      <td>Thank</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26835</th>\n",
       "      <td>24</td>\n",
       "      <td>you</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26836</th>\n",
       "      <td>13</td>\n",
       "      <td>for</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26838</th>\n",
       "      <td>733</td>\n",
       "      <td>invite</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26839</th>\n",
       "      <td>9</td>\n",
       "      <td>and</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26840</th>\n",
       "      <td>669</td>\n",
       "      <td>add</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26841</th>\n",
       "      <td>0</td>\n",
       "      <td>Lillian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26842</th>\n",
       "      <td>0</td>\n",
       "      <td>Pierson</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26843</th>\n",
       "      <td>517</td>\n",
       "      <td>Look</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26844</th>\n",
       "      <td>0</td>\n",
       "      <td>forward</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26846</th>\n",
       "      <td>0</td>\n",
       "      <td>joining</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26847</th>\n",
       "      <td>12</td>\n",
       "      <td>in</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26849</th>\n",
       "      <td>0</td>\n",
       "      <td>conversation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26850</th>\n",
       "      <td>8</td>\n",
       "      <td>SENTENCEEND</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26851</th>\n",
       "      <td>7</td>\n",
       "      <td>SENTENCESTART</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26852</th>\n",
       "      <td>4</td>\n",
       "      <td>n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26853</th>\n",
       "      <td>3</td>\n",
       "      <td>r</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26854</th>\n",
       "      <td>425</td>\n",
       "      <td>Thanks</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26855</th>\n",
       "      <td>13</td>\n",
       "      <td>for</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26857</th>\n",
       "      <td>733</td>\n",
       "      <td>invite</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26858</th>\n",
       "      <td>9</td>\n",
       "      <td>and</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26859</th>\n",
       "      <td>13</td>\n",
       "      <td>for</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26861</th>\n",
       "      <td>669</td>\n",
       "      <td>add</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26862</th>\n",
       "      <td>4</td>\n",
       "      <td>n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26863</th>\n",
       "      <td>3</td>\n",
       "      <td>r</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>23898 rows  2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       code           word\n",
       "0         0       Mohammed\n",
       "1         0         Nazili\n",
       "2         0    Suddicqui's\n",
       "3       228           post\n",
       "4         0    advertising\n",
       "5        14              a\n",
       "6       824        payment\n",
       "7         0        gateway\n",
       "8        16             is\n",
       "9       631        removed\n",
       "10       13            for\n",
       "11       14              a\n",
       "12        0         second\n",
       "13      102           time\n",
       "14        8    SENTENCEEND\n",
       "15        7  SENTENCESTART\n",
       "16        0          Third\n",
       "17        0       instance\n",
       "18       81           will\n",
       "19        0         result\n",
       "20       12             in\n",
       "22      445         member\n",
       "23      269          being\n",
       "24      631        removed\n",
       "25       47           from\n",
       "27      138          group\n",
       "28        8    SENTENCEEND\n",
       "29        7  SENTENCESTART\n",
       "30        4              n\n",
       "31        3              r\n",
       "...     ...            ...\n",
       "26829    23           with\n",
       "26830   435           them\n",
       "26831     0         though\n",
       "26832     4              n\n",
       "26833     3              r\n",
       "26834     0          Thank\n",
       "26835    24            you\n",
       "26836    13            for\n",
       "26838   733         invite\n",
       "26839     9            and\n",
       "26840   669            add\n",
       "26841     0        Lillian\n",
       "26842     0        Pierson\n",
       "26843   517           Look\n",
       "26844     0        forward\n",
       "26846     0        joining\n",
       "26847    12             in\n",
       "26849     0   conversation\n",
       "26850     8    SENTENCEEND\n",
       "26851     7  SENTENCESTART\n",
       "26852     4              n\n",
       "26853     3              r\n",
       "26854   425         Thanks\n",
       "26855    13            for\n",
       "26857   733         invite\n",
       "26858     9            and\n",
       "26859    13            for\n",
       "26861   669            add\n",
       "26862     4              n\n",
       "26863     3              r\n",
       "\n",
       "[23898 rows x 2 columns]"
      ]
     },
     "execution_count": 228,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#vocab.shape\n",
    "vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_next(text,token,model,vocabulary):\n",
    "    \n",
    "    #converting the word to 1-hot matrix represenation\n",
    "    tmp = text_to_word_sequence(text, lower=False, split=\" \")\n",
    "    tmp = token.texts_to_matrix(tmp, mode='binary')\n",
    "    #predicting next word\n",
    "    bestMatch=model.predict_classes(tmp)[0]\n",
    "    return vocabulary[vocabulary['code']==bestMatch]['word'].values[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function returns a list of generated texts. First we generate FEMALE texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['SENTENCESTART \\r \\r \\r \\r \\r \\r \\r \\r \\r \\r \\r \\r \\r \\r \\r \\r \\r \\r \\r \\r \\r \\r \\r \\r \\r \\r \\r \\r \\r \\r \\r \\r \\r \\r \\r \\r \\r \\r \\r \\r',\n",
       " '500GB \\r \\r \\r \\r \\r \\r \\r \\r \\r \\r \\r \\r \\r \\r \\r \\r \\r \\r \\r \\r \\r \\r \\r \\r \\r \\r \\r \\r \\r \\r \\r \\r \\r \\r \\r \\r \\r \\r \\r \\r',\n",
       " '\\r \\r \\r \\r \\r \\r \\r \\r \\r \\r \\r \\r \\r \\r \\r \\r \\r \\r \\r \\r \\r \\r \\r \\r \\r \\r \\r \\r \\r \\r \\r \\r \\r \\r \\r \\r \\r \\r \\r \\r \\r',\n",
       " 'ly split \\r \\r \\r \\r \\r \\r \\r \\r \\r \\r \\r \\r \\r \\r \\r \\r \\r \\r \\r \\r \\r \\r \\r \\r \\r \\r \\r \\r \\r \\r \\r \\r \\r \\r \\r \\r \\r \\r \\r',\n",
       " 'from r Join BIGDATA \\r \\r \\r \\r \\r \\r \\r \\r \\r \\r \\r \\r \\r \\r \\r \\r \\r \\r \\r \\r \\r \\r \\r \\r \\r \\r \\r \\r \\r \\r \\r \\r \\r \\r \\r \\r \\r',\n",
       " 'Two \\r \\r \\r \\r \\r \\r \\r \\r \\r \\r \\r \\r \\r \\r \\r \\r \\r \\r \\r \\r \\r \\r \\r \\r \\r \\r \\r \\r \\r \\r \\r \\r \\r \\r \\r \\r \\r \\r \\r \\r',\n",
       " 'Calling hpc \\r \\r \\r \\r \\r \\r \\r \\r \\r \\r \\r \\r \\r \\r \\r \\r \\r \\r \\r \\r \\r \\r \\r \\r \\r \\r \\r \\r \\r \\r \\r \\r \\r \\r \\r \\r \\r \\r \\r',\n",
       " 'SENTENCEEND \\r \\r \\r \\r \\r \\r \\r \\r \\r \\r \\r \\r \\r \\r \\r \\r \\r \\r \\r \\r \\r \\r \\r \\r \\r \\r \\r \\r \\r \\r \\r \\r \\r \\r \\r \\r \\r \\r \\r \\r',\n",
       " '\\r \\r \\r \\r \\r \\r \\r \\r \\r \\r \\r \\r \\r \\r \\r \\r \\r \\r \\r \\r \\r \\r \\r \\r \\r \\r \\r \\r \\r \\r \\r \\r \\r \\r \\r \\r \\r \\r \\r \\r \\r',\n",
       " 'and To goo with r Join BIGDATA \\r \\r \\r \\r \\r \\r \\r \\r \\r \\r \\r \\r \\r \\r \\r \\r \\r \\r \\r \\r \\r \\r \\r \\r \\r \\r \\r \\r \\r \\r \\r \\r \\r \\r']"
      ]
     },
     "execution_count": 242,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def generate_text(num_message,length,model,token,vocab):\n",
    "   \n",
    "    lst=[]\n",
    "    for j in range(0,num_message):\n",
    "           # pick a random seed\n",
    "            start = np.random.randint(0, len(vocab)-1)\n",
    "            pattern = vocab.iloc[start].word\n",
    "            message=''+ pattern\n",
    "            # generate characters\n",
    "            for i in range(length):\n",
    "               \n",
    "                #predict\n",
    "                prediction = get_next(pattern,token,model,vocab)\n",
    "               \n",
    "                message=message+' '+prediction\n",
    "                pattern = prediction\n",
    "            lst.append(message)             \n",
    "    return lst\n",
    "lst_generate=generate_text(10,40,model,token,vocab)\n",
    "lst_generate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We convert the list to a dataframe and export the dataframe to a file. That file now contains all the texts generated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SENTENCESTART \\r \\r \\r \\r \\r \\r \\r \\r \\r \\r \\r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>500GB \\r \\r \\r \\r \\r \\r \\r \\r \\r \\r \\r \\r \\r \\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\\r \\r \\r \\r \\r \\r \\r \\r \\r \\r \\r \\r \\r \\r \\r \\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ly split \\r \\r \\r \\r \\r \\r \\r \\r \\r \\r \\r \\r \\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>from r Join BIGDATA \\r \\r \\r \\r \\r \\r \\r \\r \\r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Two \\r \\r \\r \\r \\r \\r \\r \\r \\r \\r \\r \\r \\r \\r ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Calling hpc \\r \\r \\r \\r \\r \\r \\r \\r \\r \\r \\r \\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>SENTENCEEND \\r \\r \\r \\r \\r \\r \\r \\r \\r \\r \\r \\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>\\r \\r \\r \\r \\r \\r \\r \\r \\r \\r \\r \\r \\r \\r \\r \\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>and To goo with r Join BIGDATA \\r \\r \\r \\r \\r ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   0\n",
       "0  SENTENCESTART \\r \\r \\r \\r \\r \\r \\r \\r \\r \\r \\r...\n",
       "1  500GB \\r \\r \\r \\r \\r \\r \\r \\r \\r \\r \\r \\r \\r \\...\n",
       "2  \\r \\r \\r \\r \\r \\r \\r \\r \\r \\r \\r \\r \\r \\r \\r \\...\n",
       "3  ly split \\r \\r \\r \\r \\r \\r \\r \\r \\r \\r \\r \\r \\...\n",
       "4  from r Join BIGDATA \\r \\r \\r \\r \\r \\r \\r \\r \\r...\n",
       "5  Two \\r \\r \\r \\r \\r \\r \\r \\r \\r \\r \\r \\r \\r \\r ...\n",
       "6  Calling hpc \\r \\r \\r \\r \\r \\r \\r \\r \\r \\r \\r \\...\n",
       "7  SENTENCEEND \\r \\r \\r \\r \\r \\r \\r \\r \\r \\r \\r \\...\n",
       "8  \\r \\r \\r \\r \\r \\r \\r \\r \\r \\r \\r \\r \\r \\r \\r \\...\n",
       "9  and To goo with r Join BIGDATA \\r \\r \\r \\r \\r ..."
      ]
     },
     "execution_count": 245,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "female_df = pd.DataFrame(lst_generate)\n",
    "\n",
    "female_df.to_csv('generated_female_posts.csv', sep=',', index=False)\n",
    "female_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now repeat the process with the MALE text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Does Gmail sell information is one\\'s private emails. Judging by adverts in my Facebook news feed, I would say \"Yes.\" But perhaps this isn\\'t news for anyone and I have been under a rock for years. Clarification, please. Is private email private in name only? /n/r Hold the applause. The new rocket has two main purposes, neither of them connected with the development of science: to boost the obscene personal wealth of Musk, currently more than $21 billion, and to expand the military arsenal of American imperialism.\\r\\n\\r\\n\\'Falcon Heavy launch marks new stage in the privatization\\x97and perversion\\x97of space exploration\\'\\r\\nWorld Socialist Web Site\\r\\n\\r\\nhttp://www.wsws.org/en/articles/2018/02/09/falc-f09.html /n/r Fake or real followers? /n/r THIS is where science is needed. HERE ON EARTH! /n/r If you bombard the Earth with photons for a while, it will emit a Tesla Roadster-st /n/r Wake Up America, the Original, the Original .. America is \"Fighting The Wrong War\"!  We need a \"War On Killer Diseases\"!! /n/r Check out that lenticular cloud we caught over Mt. Hood, Oregon, USA yesterday, Feb 8, 2018 morning during #sunrise. #omht #StormHour\\r\\n\\r\\nhttps://twitter.com/inMtHood/status/962053037031174145 /n/r how does amazon go works? /n/r God is a prankster, he gives you instincts, he gives you this and he sets the rules. And while he looks, he is laughing his ass off. He set the rules of all time. He\\'s a tight ass, he\\'s a sadist. He\\'s an absentee landlord.\\r\\n\"The Devil\\'s Advocate\"\\r\\nhttps://youtu.be/O4irXQhgMqg?t=2 /n/r My hat off to #teamcanada.\\r\\nBecause these are the ideals I came to the US for. Maybe I aimed a bit low (I am a competitive archer, but not an olympian after all). :) /n/r This reminds me of the first glimpse we got of Darth Vader in the second Star Wars movie, \"The Empire Strikes Back\":\\r\\n-- it was a \\'peek-a-boo\\' shot from behind as his helmet was slowly lowered onto his disfigured and scarred head. /n/r Demonstration of intelligence ? /n/r ++Don\\'t miss the chance++\\r\\n#Oral #presentation #slot #available \\r\\n#Grab your slot: https://goo.gl/uHQvZ1 \\r\\n#Molecular #Immunology #Conference  #London /n/r The Mandelbrot Set, a pathological curve.... /n/r \"Is A Little Less American Evil Really Too Much To Ask For?\"\\r\\n\\r\\nDamn Good Question if you ask me.\\r\\n\\r\\n\"\\r\\n\\r\\nIs it too much to ask for the US not to drone-bomb innocent civilians or torture suspected enemy combatants in Guantanamo?\\r\\n\\r\\nI am frequently criticized of being too idealistic and too judgmental when it comes to politicians, whether Democrat or Republican. They\\'re just human beings, people tell me. They\\'re doing their best. Why am I so hard on them?\\r\\n\\r\\nBut the criticisms that I wage hardly seem like too much to ask. For example, don\\'t destroy the entire countries like Libya and Syria. Don\\'t kill hundreds of thousands of people in Iraq. Don\\'t torture them at Guantanamo. Don\\'t give massive tax cuts to the wealthy. Don\\'t hang out with rich people who want to comprise you. And don\\'t accept their contributions.\\r\\n\\r\\nThis is too much to ask?\\r\\n\\r\\n? Ted Rall /n/r Notice to all members:\\r\\nFirst instance of obviously off-topic posting: \\r\\n-Will be deleted.\\r\\nSecond instance of obviously off-topic posting: \\r\\n-Member will be banned.\\r\\nBLOCKING of group admins is NOT allowed.\\r\\nA member that blocks an admin may be warned to unblock.\\r\\n-If member does NOT UNBLOCK, member will be banned.\\r\\nThe group admins are:\\r\\n-- Jill Perlman.\\r\\n-- M Patricia McLaughlin.\\r\\n-- Mi Robin.\\r\\n-- Kathy Henderson.\\r\\n-- Steve Cooperman.\\r\\n-- Bob Kerns.\\r\\n-- Steve Hicks\\r\\n-- Kevin G. Rhoads, and \\r\\n-- William A. Boyle.\\r\\nNew members:\\r\\n-PLEASE read the group description!\\r\\n(pic thanx to Sargon Agade!) /n/r ACLU Action\\r\\n\\r\\nHi Jerald \\x96\\r\\n\\r\\nBig news for drug policy reform: After California legalized recreational marijuana use, San Francisco followed suit by clearing thousands of marijuana convictions dating back 40 years. This is a step in the right direction \\x96 now people won\\'t be held for crimes that are no longer crimes. \\r\\n\\r\\nThe outdated policies of the War on Drugs have done little but waste federal resources and harm lives forever. But while some of us are fighting back, others \\x96 like Attorney General Jeff Sessions \\x96 are doggedly pursuing the same failed policies. His vendetta against marijuana and people who use it goes against public opinion and completely disregards the human consequences of these unjust laws. \\r\\n\\r\\nBut we can stop him. There\\'s a new bill in Congress to legalize marijuana under federal law \\x96 and remedy some of the injustices of the War on Drugs. We need your help to make sure it becomes law.\\r\\n\\r\\nTell your members of Congress to support the Marijuana Justice Act and reform our broken criminal justice system.\\r\\n\\r\\nHow do we know that reform is needed? The data says it all. In 2013 we found that Black people are almost four times more likely to be arrested for marijuana possession even though Black and white people use marijuana at comparable rates.\\r\\n\\r\\nWhat\\'s more, 64 percent of Americans approve of legalizing marijuana. But Jeff Sessions\\' policies go against the will of the people.\\r\\n\\r\\nLet\\'s tell Congress to pass the Marijuana Justice Act NOW to promote equitable and proportional justice. Antiquated and out-of-touch drug laws \\x96 like Sessions\\' marijuana policies \\x96 have got to go.\\r\\n\\r\\nWith your help, we can make sure Congress listens to us and does the right thing.\\r\\n\\r\\nThanks for standing up with us,\\r\\n\\r\\nJesselyn McCurdy\\r\\n\\r\\nJesselyn McCurdy\\r\\nACLU Deputy Legislative Director, fighting for criminal justice reform /n/r the eye often perceives what the brain has already seen!? /n/r Hey! ...FREE! :O /n/r A potential area of research. /n/r A BRITISH APPEALS COURT on Monday rejected demands from the U.S. Government for the extradition of accused British hacker, Lauri Love, citing the inability of U.S. prisons to humanely and adequately treat his medical and mental health ailments. Extradition to the U.S., the court ruled, would be \"oppressive by reason of his physical and mental condition.\" /n/r Blowing snow with very low ground level visibility made for a perfect winter day for photography. Eastern Manitoba, Canada Feb 1st, 2018 #sundog #Manitoba #Weather #StormHour @ctvwinnipeg @CNN @TheWeatherNetUS\\r\\n\\r\\nhttps://twitter.com/BrentMckean501/status/961052879992905735 /n/r altruism |?altro?o?iz?m|\\r\\nnoun\\r\\n\\x97 the belief in or practice of disinterested and selfless concern for the well-being of others /n/r My son and I.... /n/r One of the perks of being lodged at a #rorbuer, Lofoten Islands, Norway is that you can see the Aurora right from your balcony ~ Thanks to @DavidRocaberti #Auroraborealis #StormHour\\r\\n\\r\\nhttps://twitter.com/StormHour/status/960634125362266118 /n/r How the diet changed the Neanderthals.\\r\\nNeanderthals dominated the territory of modern Europe for over 200,000 years. They were perfectly adapted to the conditions of the ice age, but disappeared somewhere 20,000-30000 years ago. They were replaced by Cro-Magnens, who became the progenitors of modern people.\\r\\nHowever, it is proved that the genes of Neanderthals are in the genome of modern man.\\r\\nThe main part of the Neanderthal diet was meat. These are proteins and fats.\\r\\nOnly in the liver is glycogen in a relatively large amount.\\r\\nProceeding from this, gluconeogenesis was very characteristic for the metabolism of Neanderthals. Vegetable food accounted for a very small percentage of their diet.\\r\\nThe diet of Neanderthals was very similar to the diet of modern brown bears: meat, fish, snails, berries, roots and leaves of plants.\\r\\nWith the melting of glaciers, vast spaces of free land opened up, which were filled with plants, including berries: raspberries, blueberries, acorns. They can be dried and mixed with meat. The Indians still know the recipe for phemikana.\\r\\nHowever, an increase in the percentage of carbohydrates in the diet led Neanderthal women to polycystic ovary syndrome and a decrease in fertility.\\r\\nIt has already been proved that the Neanderthals crossed with the Cro-Magnens.\\r\\nWomen with part of the genes of the Cro-Magnens were more fertile, because the Cro-Magnens lived in a warmer climate and consumed more carbohydrates. Gluconeogenesis in them was less pronounced, fertility is higher, their descendants and replaced net Neanderthals can and pure Kromiens who could not tolerate the cold. There was a highly adaptive race that occupied all the earth and displaced all other hominids.\\r\\n??? ????? ???????? ??????????????.\\r\\n????????????? ?????????????? ?? ?????????? ??????????? ?????? ????? 200000 ???. ??? ???? ????????? ????????????? ? ???????? ??????????? ???????, ?????? ??????? ??? ?? 20000-30000 ??? ?????. ?? ??????? ???????????, ??????? ????? ????????????? ??????????? ?????.\\r\\n?????? ????????, ??? ???? ?????????????? ????????? ? ?????? ???????????? ????????.\\r\\n???????? ????? ????? ????????????? ?????????? ????. ??? ????? ? ????.\\r\\n?????? ? ?????? ???? ???????? ? ???????????? ??????? ??????????.\\r\\n?????? ?? ????? ????????????? ??? ????? ?????????? ??? ??????????? ??????????????. ???????????? ???? ?????????? ????? ????? ??????? ? ?? ???????.\\r\\n?????? ?????????????? ??? ????? ????? ?? ?????? ??????????? ????? ????????: ????,????, ??????, ?????, ????? ? ?????? ????????.\\r\\n??? ?????? ???????? ??????????? ???????? ???????????? ????????? ?????, ??????? ??????????? ??????????, ? ??? ????? ? ????????: ???????, ????????, ?????????. ?? ????? ?????? ? ????????? ? ?????. ??????? ?? ??? ??? ????? ?????? ????????. \\r\\n?????? ?????????? ???????? ????????? ? ??????? ???????? ? ?????? ?????????????? ? ???????? ????????????? ???????? ? ???????? ????????????. \\r\\n??? ???????? ???,????????????? ???????????? ? ?????????????.\\r\\n??????? ??????? ????? ????? ???????????? ???? ????? ?????????, ?????? ??? ??????????? ???? ? ????? ?????? ??????? ? ?????????? ?????? ?????????. ????????????? ? ??? ??? ????? ???????, ???????????? ????,?? ??????? ?  ????????? ?????? ?????????????? ????? ? ?????? ????????????, ??????? ?? ?????????? ?????. ???????? ?????? ?????????? ????, ??????? ?????? ??? ????? ? ????????? ???? ?????? ?????????. /n/r Trump\\'s ascension to the leader. /n/r Here is a short list of items that may contain cochineal-derived colorant: /n/r \"Equifax has said it is under investigation by every state attorney general and faces more than 240 class action lawsuits.\"...\\r\\n\\r\\nOnce upon a time a headline like this would have been unbelievable,\\r\\n-- but w/the bastards \\'running the show\\' today, No Longer.\\r\\n\\r\\n*\\r\\n\\r\\nEquifax (EFX.N) said in September that hackers stole personal data it had collected on some 143 million Americans. Richard Cordray, then the CFPB director, authorized an investigation that month, said former officials familiar with the probe.\\r\\n\\r\\nBut Cordray resigned in November and was replaced by Mulvaney, President Donald Trump\\'s budget chief. The CFPB effort against Equifax has sputtered since then, said several government and industry sources, raising questions about how Mulvaney will police a data-warehousing industry that has enormous sway over how much consumers pay to borrow money.\\r\\n\\r\\nThe CFPB has the tools to examine a data breach like Equifax, said John Czwartacki, a spokesman: \"The bureau has the desire, expertise, and know-how in-house to vigorously pursue hypothetical matters such as these,\" he said... /n/r It looks like a welcoming place! /n/r Using a sophisticated technique called extra galactic micro lensing, astronomers have detected ( not observed ) over 2,000 planets outside our own Milky Way galaxy.\\r\\n\\r\\nThe planets orbit stars in a galaxy some 3.8 billion light years away from the earth, so their discovery is quite a stunning achievement.\\r\\n\\r\\nCommon sense told us these planets existed, but to have evidence is always comforting. /n/r Is This For REAL?! /n/r Here are 77 Facts That Sound Like Huge Lies\\r\\nBut Are\\r\\nActually Completely True.\\r\\n:\\r\\nGet ready to have your mind blown into a\\r\\ndifferent time zone.\\r\\n1. If you put your finger in your ear and\\r\\nscratch, it sounds just like Pac-Man.\\r\\n2. The YKK on your zipper stands for \"Yoshida\\r\\nKogyo Kabushikigaisha.\"\\r\\n3. Maine is the closest U.S. state to Africa.\\r\\n4. Anne Frank, Martin Luther King Jr., and\\r\\nBarbara Walters were born in the same year,\\r\\n1929.\\r\\n5. The name Jessica was created by\\r\\nShakespeare in the play Merchant of Venice.\\r\\n8. Cleopatra lived closer to the invention of the\\r\\niPhone than she did to the building of the\\r\\nGreat Pyramid.\\r\\n9. Russia has a larger surface area than Pluto.\\r\\n10. Saudi Arabia imports camels from\\r\\nAustralia.\\r\\n11. Hippo milk is pink.\\r\\n12. The toy Barbie\\'s full name is Barbara\\r\\nMillicent Roberts.\\r\\n13. Woody from Toy Story has a full name too\\r\\n\\x97 it\\'s Woody Pride.\\r\\n14. And while we\\'re at it, Mr. Clean\\'s full name\\r\\nis Veritably Clean.\\r\\n15. Oh, and Cookie Monster\\'s real name is Sid.\\r\\n16. Carrots were originally purple.\\r\\n17. The heart of a blue whale is so big, a human\\r\\ncan swim through the arteries.\\r\\n18. Vending machines are twice as likely to kill\\r\\nyou than a shark is.\\r\\n19. Home Alone was released closer to the\\r\\nmoon landing than it was to today.\\r\\n20. Oxford University is older than the Aztec\\r\\nEmpire.\\r\\n21. Not once in the Humpty Dumpty nursery\\r\\nrhyme does it mention that he\\'s an egg.\\r\\n22. France was still executing people with a\\r\\nguillotine when the first Star Wars film came\\r\\nout.\\r\\n23. Armadillos nearly always give birth to\\r\\nidentical quadruplets.\\r\\n24. Betty White is actually older than sliced\\r\\nbread.\\r\\n25. The unicorn is the national animal of\\r\\nScotland.\\r\\n26. A strawberry isn\\'t a berry but a banana is.\\r\\n27. So are avocados and watermelon............\\r\\n28............. Visit this page #Vanlex_KoluS to get the complete version \\r\\nAnd also get more interesting topics to discuss & read. Thanks \\r\\n.............Vanlex KoluS............. /n/r From #immunotherapy to the #microbiome, one #scientist\\'s journey to find a #cure for #cancer- Dr. #Jennifer #Wargo  https://goo.gl/uxTT6J /n/r Today\\'s sunrise and stunning sun pillar above Cape Cod National Seashore in Wellfleet, Massachusetts, USA, Fev 5, 2018. Thanks to Dapixara @dapixara #Sunpillar #StormHour\\r\\n\\r\\nhttps://twitter.com/StormHour/status/960301020847988736 /n/r We cease to question. /n/r In Three years the Routine should Stop. /n/r Thank you for adding me to the group /n/r Science is timeless, bitches! ?\\r\\n\\r\\nhttps://m.facebook.com/story.php?story_fbid=1194499744028087&id=773558749455524 /n/r Here\\'s a mind blower... check it out!\\r\\n\\r\\nhttps://www.facebook.com/groups/1846743545583087/permalink/2034887416768698/ /n/r The movie \"Brazil\" /n/r Rest from Work-Van Gogh /n/r Facebook\\x97working closely with intelligence agencies and government\\x97is seeking to leverage its role as a mechanism of communication to become an instrument of censorship and repression. In the process, it is turning one of the most  liberating technological advances of the century, the growth of artificial intelligence, into a mechanism for police control and dictatorship.\\r\\n\\r\\n\\'From Facebook to Policebook\\'\\r\\nWorld Socialist Web Site\\r\\n\\r\\nhttp://www.wsws.org/en/articles/2018/02/02/pers-f02.html /n/r Mathematical Entertainments -- FREE! /n/r Details .... /n/r My fighter pilot buddy, boots, took this pic out the window of his F-18 of Von Karman Vorticity creating an eddy off the coast of San Diego Feb 1st, 2018. Great shot of a cool phenomenon I\\'ve just learned a lot about! @NWSSanDiego can\\'t say I\\'ve seen this in Iowa before\\r\\n\\r\\nhttps://twitter.com/WXMegs/status/959638888993173506 /n/r *Hallelujah!*\\r\\n-- \"God Bless Us, Every One.\"\\r\\n\\r\\nIt\\'s time once again for another *31 Days of Oscar* on Turner Classic Movies (TCM);\\r\\n-- Scores of the greatest films Ever Made 24/7!\\r\\n\\r\\n... How I *Love* TCM...\\r\\n\\r\\nIt\\'s the 3rd day into this film festival and I didn\\'t know. (I\\'ve been catching up on DVDs...)\\r\\n\\r\\nJust saw John Ford\\'s \"She Wore a Yellow Ribbon\" for the first time in a decade or two. WOW...\\r\\n\\r\\n*\\r\\n\\r\\nI never was much a fan of the Academy Awards themselves, but I\\'m expecting many moments of genuine happiness in the 28 Days to come,\\r\\n-- and, as always at this time, my DVR is going to be MIGHTY Busy. /n/r A reminder for Super Bowl weekend that like the church, the NFL doesn\\'t pay taxes. /n/r The United States has become a very machiavellian society. /n/r I captured this glorious lunar halo a couple of days ago at Paranal Observatory, Chile. You can see all our telescopes (except VISTA), including UT4 and its four mighty lasers. #astrophotography #stormhour\\r\\n\\r\\nhttps://twitter.com/astro_jcm/status/959402120054222848 /n/r Is the \"privatised\" part at fault?\\r\\nWhat happens if you provide greedy people the means to exploit people who do not have the means to defend themselves?\\r\\nThe system worked by \"bringing in private contractors to build and\\r\\nmaintain the system and collect the penalties it ascribed,\" :O /n/r An Image from the Spitzer Telescope... /n/r Tireless worker even away from home, will it also be in the future for humans? /n/r THE END OF EVERYTHING, as if you didn\\'t have enough to worry about....OY Vey...\\r\\n\\r\\nEinstein\\'s famous equation, E=MC squared implies that energy and matter (mass) are interchangeable, that one can be converted to the other and that the amount of energy obtainable from the destruction of a given amount of mass equals the amount of mass times the speed of light squared. It also implies that the sum of matter plus energy never changes.\\r\\n\\r\\nIn nuclear reactions, ( both fission, where atoms are split and in fusion, where atoms are combined ) energy is produced in just this very way...by the complete annihilation of matter.\\r\\n\\r\\nAll the stars in the universe, including our own sun, of course, use fusion to produce energy. Deep in the interior of stars, atoms of hydrogen are combined or FUSED into helium. But the mass of the resulting helium is less than the mass of the hydrogen, and it is that \"missing\" mass that is turned into energy.\\r\\n\\r\\nStars may be thought of as islands of intensely concentrated energy in a vast interstellar void. It\\'s a system, though, a universe, whose organization ( pockets of energy here and nothingness there ) is being constantly eroded, dismantled, degraded.\\r\\n\\r\\nIn other words, the degree of organization of the universe is constantly on the decrease, while the overall disorder, called entropy, is inexorably on the the rise.\\r\\n\\r\\nEventually this disorder, entropy, will reach a maximum as all the stars inevitably exhaust their supply of fuel and cease producing energy altogether. \\r\\n\\r\\nThey will shine no more. There will be no more \"islands of energy\"...no concentration of energy here and nothingness there. What scientists call the temperature gradient will cease to exist and that means no more usable energy available for ANYTHING.\\r\\n\\r\\nInstead, there will be a uniform sameness throughout the cosmos. The temperature from one \"end\" of the universe to the other will be the same...just a few degrees above absolute zero. \\r\\n\\r\\nAppropriately enough, this is what scientists call the \"heat death\" of the cosmos. And it will be the end of absolutely EVERYTHING. There is, however, a great unknown in this dooms day scenario...dark energy and its role in the ultimate fate of the universe.\\r\\n\\r\\nWill this enigmatic \"stuff\" change the outcome?\\r\\n\\r\\nNobody knows.\\r\\n\\r\\nSome say, yes, the the cosmos will end with a whimper as in the heat death scenario described above...in other words, the end will come with a quiet \"whimper\". Others say, no the universe will continue expanding forever. Still others say, the universe will collapse into a singularity, the so-called \"Big Crunch\".\\r\\n\\r\\nRegardless, whatever the fate of the universe...the ultimate catastrophe, if it happens at all won\\'t likely occur for billions, if not trillions of years. So no need to worry.\\r\\n\\r\\nAnd if it continues expanding indefinitely, well, we need not worry about that either, cuz that portends nothing catastrophic...as far as we know now. /n/r The only thing that\\'s changed since then is it\\'s gotten worse. /n/r Blue moon over Blackpool, UK, Jan 30, 2018. How it might have looked if it wasn\\'t for clouds lol #StormHour #BluemoonEclipse #supermoon uk #luna #moon\\r\\n\\r\\nhttps://twitter.com/Stephencheatley/status/958837349487693824 /n/r Reported post deleted due to conspiracy theory and non-factual sourcing. Member warned. /n/r 9th #Molecular #Immunology & #Immunogenetics #Congress #March 08-09, 2018, #London, UK\\r\\n10% - 15% #Discount for #Group #Bookings \\r\\n#Contact here \\r\\n#Email: molecularimmunology@immunologyconferences.org \\r\\n#Ph: 0-800-014-8923, 1-888-843-8169\\r\\n#Web: https://goo.gl/vbyS6G /n/r I enjoy seeing/hearing Good News for a change,\\r\\n? and Glennzilla is an ideal deliverer of This news. /n/r What is the best part of your job? For me, it\\'s giving away money to people from different countries who need help. It takes a lot to get there. Not my company but my society. I\\'m happy for that. /n/r Check them out! :) /n/r Ah-rooooo baby, Australia, Jan 31, 2018. #moon #Eclipse2018\\r\\n\\r\\nhttps://twitter.com/MossyGene/status/958673787041464322 /n/r The moon sets over Mount Susitna in this view from Point Woronzof in Anchorage, Alaska, USA on January 30, 2018. (Marc Lester / ADN)\\r\\n\\r\\nhttps://twitter.com/adndotcom/status/958542319774449665 /n/r \"Modern science is the mathematical description of natural phenomena;\\r\\nMathematics is the systematic study of patterns (in time, in space, or in structures).\" --WAB. /n/r Your thoughts...The osculatory resonance of spiritual enhancements portend advances in geometric syllogisms, provided substitutions in existentialism don\\'t preclude duality.\\r\\n\\r\\nDoug Hullander /n/r Have you seen this post? /n/r After his photo op showing him \"working\" on a completely clear desk, while grasping his phone landed with a thud and a laugh, tRump resorted to digital tech to come up with an alternative photo.   https://www.facebook.com/photo.php?fbid=10213245085714438&set=a.10200471450861550.1073741831.1615423959&type=3&theater /n/r Our beliefs, no matter how intimately and fervently they\\'re embraced, never change the substantiated facts\\r\\n\\r\\nBut facts, when confirmed by evidence, should always change our beliefs...IF we\\'re courageous enough to apply logic and reason. /n/r Hi everyone, who has heard about Sophia the AI robot. She\\'s pretty amazing. Lemme know what you think about her in the comment box below. \\r\\nAnd before I forget, she was also given a citizenship @ Saudi-arabia. \\r\\nThat bot is really amazing. /n/r This article is fro PROGRESS REPORT.\\r\\n\\r\\nJanuary 29, 2018\\r\\n \\r\\nINFRASTRUCTURE SCAM.\\r\\nDuring tomorrow\\'s State of the Union address, President Trump is expected to tout his infrastructure plan, which he has talked about since his time on the campaign trail. But a leaked draft of the plan show it\\'s nothing more than a scam. It calls for cutting or significantly changing at least 10 bedrock environmental laws to make it easier for corporations to bypass critical protections for air, water, and wildlife. Instead of putting Americans to work rebuilding crumbling infrastructure, the Trump infrastructure scam is the administration\\'s latest giveaway to the oil and gas industry. The only clear winners from this plan are corporate developers, while communities, workers, and the environment face untold threats. And there\\'s no clear economic benefit from this plan, as it will decimate the Highway Trust Fund, cutting jobs and crucial infrastructure projects.\\r\\nAs far as environmental risks, this scam is one of the worst. It would prioritize polluter profits and cut communities out of the decision-making process for major developments in their neighborhoods. Corporations would be able to sidestep public health, worker safety, and environmental protections for infrastructure projects, including for toll roads, pipelines, drilling projects, and new mines. It allows polluters to skirt landmark public health protections like the Clean Air Act and the Clean Water Act, all while depriving funding for improvements for clean drinking water systems. It even grants authority to Interior Secretary (and friend of the oil and gas industry) Ryan Zinke to unilaterally approve new gas pipelines through National Parks. The Trump administration will attempt to brand these environmental attacks as an effort to improve the infrastructure permitting process. In actuality, they are attempting to steamroll hardworking Americans by silencing their voices in determining where pipelines, highways, and other large projects should be built, all to boost industry profits.\\r\\nBut Americans won\\'t be duped by anything that puts us, our families, and our outdoor places at risk. A recent poll found that 94% of voters reject the idea that we have to put our health and outdoors at risk to improve our nation\\'s infrastructure. So when Trump tries to use the State of the Union speech tomorrow to sell us on an infrastructure plan, we\\'ll see it for what it is: a scam.\\r\\n\\r\\nACTION OF THE DAY\\r\\n#HandsOff Our Medicaid. Earlier this month, the Trump administration ended Medicaid as we know it, allowing states to enact punitive work requirements as part of their Medicaid programs. The majority of working-age Medicaid recipients are already working\\x97and majority of those who are not are either ill or disabled, caring for a loved one, or going to school. This decision has put 6.3 million Americans\\' health insurance at risk, and it kicks struggling workers while they\\'re down. And according to a new report, the state of Kentucky will go a step further\\x97requiring people who have lost their Medicaid to pass a health or financial literacy test to get health care back, harkening back to racist literacy tests in the Jim Crow South. It\\'s the latest in his attacks on crucial programs that help families make ends meet\\x97so take action today! Add your name to our petition urging the Trump administration to reverse their decision and keep their #HandsOff our Medicaid.\\r\\n\\r\\nWHAT\\'S TRENDING\\r\\nAttacking Reproductive Rights. Tonight, the Senate will vote on a 20-week abortion ban. The procedural vote has been scheduled by Senate Majority Leader Mitch McConnell, who promised a vote early this year after the House passed the bill a few months ago. What effects would such a restrictive ban have on women across the country? Twenty weeks is the time when women have an ultrasound that helps determine if the child will survive after birth. Read stories of women that reveal why the ban would be so harmful. In addition to the harmful effects, our country just acknowledged the 45th anniversary of Roe v. Wade one week ago. Such a ban would clearly violate Roe v. Wade, and thus, is unconstitutional. Even if this particular legislation fails, it reminds us to stay vigilant and continue to fight for women\\'s rights to access safe, affordable abortion care.\\r\\n#TimesUp at the Grammys. Last night, some of the biggest music stars in the country gathered for the 60th Grammy Awards. Just like the stars at the Golden Globes, many attendees showed up in support of #TimesUp, the initiative to end sexual misconduct in the workplace, by wearing white roses. Kesha gave a stunning, emotional performance of her anthem \"Praying,\" which reflects on her survival after her own experiences with sexual assault and abuse. But despite the spotlight given to #TimesUp and #MeToo, gender diversity remains a huge problem for the music industry. In fact, a new study from the University of Southern California \"showed that in the past six years, more than 90 percent of Grammy nominees have been men \\x97 a statistic that\\'s come up a lot as the industry begins to grapple with sexual misconduct.\" And, women only won one of the 10 awards broadcast during the show last night. It\\'s clear that the music industry\\x97as well as every other industry across America\\x97needs more than just gestures or roses to bring about real change.\\r\\n\\r\\nOFF-KILTER\\r\\nDog-Whistling Donald. In the aftermath of the blatantly racist \"shithole\" comment from President Trump, the latest episode of the Off-Kilter podcast explores Trump\\'s more subtle racism, and whether that is actually more dangerous. Hear from former NAACP head Cornell Brooks and longtime disability rights advocate Patrick Cokley on these issues and why people are afraid to use the word \"racist.\" And then don\\'t miss a conversation with Inge Fryklund\\x97a former Assistant State\\'s Attorney in Cook County who serves on the board of Law Enforcement Action Partnership, an organization devoted to criminal justice reform and stopping the war on drugs\\x97about the law enforcement case for legalization.\\r\\n\\r\\nUNDER THE RADAR\\r\\nTracking Your Car. The Immigration and Customs Enforcement (ICE) agency has a new, powerful tool in their hands\\x97and it\\'s raising concerns about people\\'s civil liberties. According to reports, ICE now has access \"to billions of license plate records and new powers of real-time location tracking.\" While this is concerning for all Americans, it is especially concerning for immigrants, given ICE\\'s focus on sweeping up anyone it comes in contact with, even long time family and community members. This new tool will make it easier for ICE to expand its enforcement overdrive apparatus; in addition, it could lead to more people avoiding registering their cars, \"a public safety hazard.\" /n/r We all knew this was coming. /n/r Check out my new laboratory!\\r\\n\\r\\nWhich is the \"monster\"?\\r\\nFRANKENSTEIN - Mondays at 7 PM\\r\\nwww.TheFrankensteinMusical.com\\r\\nmusic, book & lyrics by Eric Sirota physicist/composer/playwright /n/r When Trump said we\\'d be back in the lead.... this wasn\\'t what I had in mind  :/ /n/r In perpetuity. /n/r \"Law Enforcement\" (cont.) /n/r Hey! J. Willard Gibbs! :O /n/r Nature Science Lesson.\\r\\n\\r\\n(File under \"Framing\". How people employ words Matters,\\r\\n-- so let\\'s not be \\'Suckers\\'.) /n/r \"Trending.\" (cont.)\\r\\n\\r\\nFrom the Archives (which just makes this more relevant IMO).\\r\\n-- And so it Goes... /n/r I think I heard the line \"America First\" from the mother\\'s shirt in use these days... but... I doubt it, history never repeats itself, does it? /n/r Your computer will read this book aloud to you! :O /n/r ...Linus Pauling! (Y) /n/r FYI...\\r\\n\\r\\nWhy do we get goose bumps when exposed to cold temps and in situations which evoke fear, when we\\'re skeet?\\r\\n\\r\\nThe short answer is that this reaction to threatening situations and frigid conditions is an evolutionary throw back.\\r\\n\\r\\nWe are descended from primates whose body was covered with hair and fur. \\r\\n\\r\\nFor these early creatures, it was a survival advantage if they were able to appear larger and more formidable to enemies. How to do that? Cause each hair on the body to become erect. \\r\\n\\r\\nIn freezing conditions, it was a survival advantage also to erect each hair on the body, because this action trapped body heat and provided additional insulation from the cold.\\r\\n\\r\\nWhat erected body hair? Tiny muscles near the skin\\'s surface. When these muscles are activated they form small \"pimples\" or goose bumps.\\r\\n\\r\\nOf course, we\\'re no longer covered with fur, but the muscles that erected the hair in our ancestors remain with us. /n/r \"Maybe that is a good idea. I will discuss with others.\" /n/r THE KING TUT WRAP? A new technology is coming out to wrap food and produce that doesn\\'t include the toxic waste to dump when its no longer needed.\\r\\nBut surprisingly its based on an old technology- a very old one.\\r\\nIn fact, it was discovered in King Tut\\'s tomb and used for preservation by Egyptian pharaoahs..... and its far more cost effective. In fact, it can be re-used up to 150 times\\r\\n\\r\\n\"Come join the revolution, and say goodbye to plastic and its [unhealthy] chemicals. Help our landfills shed some pounds\", says Etee, a company who is going with the idea.\\r\\nLink to video---->\\r\\n\\r\\nhttps://www.youtube.com/watch?v=ohe64q7c5l0 /n/r A stroll under the #NorthernLights, Canada, Jan 27, 2018. #aurora #exploreCanada\\r\\n\\r\\nhttps://twitter.com/Adamhillstudios/status/957318381178368000 /n/r Interesting image to run maps to study human settlements on our planet. This composite image offers a view of the Americas at night. NASA Earth Observatory\\'s \"Night Lights\" website. /n/r Slower metabolizing of alcohol? :O /n/r #Register now for #Molecular #Immunology #Conference\\r\\n#March 08-09, 2018 #London, UK https://goo.gl/DuTJ9z /n/r \"They have access to the most sophisticated technology the world has ever seen and they bully you with it. They are The Advertisers and they are laughing at you.\" /n/r WOW! Northern Lights seen last night, Jan 24, 2018 from Kval?ya, Norway. Photo credit: Marianne Bergli. https://tinyurl.com/y8rzvyoa  #Aurora #NorthernLights #Norway\\r\\n\\r\\nhttps://twitter.com/mark_tarello/status/956707717254729728 /n/r Not good. /n/r Hendrik Casimir effect of the same name in 1948 was declared on the basis of a description of effects in the vacuum between two parallel mirrors. the small inclinations of the mirrors can drastically change the strength of Casimir\\'s strength making a correct measurement difficult.\\r\\nA flat-ball configuration solves this problem because the inclinations of the plane or lateral movements of the sphere only cause a small change in the distance between the two conductors.\\r\\nThis research was published in Physical Review Letters, Matteo Rini. /n/r QOTD (Christ, I wish someone would tell \\'our\\' politicians!):\\r\\n\\r\\n\"All parts should go together without forcing. You must remember that the parts you are reassembling were disassembled by you. Therefore, if you can\\'t get them together again, there must be a reason. By all means, do not use a hammer.\"\\r\\n\\x97 IBM maintenance manual, 1925 /n/r FROM JOHN PAVLOVITZ\\r\\nDear Friend,\\r\\nThank you for your note to let me know you\\'re worried about me, that you\\'re concerned about my health\\x97that you\\'re not sure that I realize I\\'m coming across as really angry lately.\\r\\nYour assessment is correct.\\r\\nI am angry.\\r\\nI\\'m sorry.\\r\\nI can imagine I\\'m not all that fun to be around right now, and that from time to time my words come across as combative or abrasive. I\\'m probably more than a bit of a downer lately and I apologize.\\r\\nYou\\'re going to have to bear with me, as I haven\\'t been sleeping well for about a year or so. Admittedly I\\'m not at my best these days, so you\\'ll need to forgive me. I\\'m chronically overtired. I\\'m exhausted from having to give all the sh*ts about people that you\\'re supposed to be giving\\x97along with my own.\\r\\nI\\'m worn out from keeping up on legislation and watching hearings and staying on top of details and remembering deadlines and imploring action\\x97while you go about your day as if such things are an annoyance, is if they are a disruption of your plan, as if the expiration date for my outrage has long come and gone.\\r\\nI am absolutely burnt out from trying to make my voice loud enough to counteract not only the bad people\\'s incredible volume\\x97but your deafening silence. Both of these things are doing similar damage right now, sadly.\\r\\nBelieve me, I understand that my activism is a problem for you. Please know that your inactivism is similarly problematic for me. It\\'s part of the reason I am as angry as I am; because I\\'m not only having to fight against those who seem furiously bent on hurting people\\x97I\\'m having to fight against those who don\\'t seem give enough of a damn that they are doing so, to say anything.\\r\\nLook, I get it, I really do. It\\'s difficult to see so much bad news, to fully face the relentless flood of terrible, to try and wrap your brain around seemingly boundless cruelty around you. It\\'s tiresome to spend so much time with a closed fist. I know it\\'s even a pain in the rear end to endure the continual rantings of people like me on your news feed and in your timeline and across the dinner table and in the break room.\\r\\nI\\'m tired of me too.\\r\\nI\\'m sick of the fight too.\\r\\nI\\'m sick of the sound of my own voice.\\r\\nI\\'d rather not be doing this either.\\r\\nI\\'d much rather prefer to just enjoy life, to forget about it all, to only post pictures of puppies and my kids, and to simply ignore all that \"political garbage.\"\\r\\nBut that is what privilege looks like; to even believe I have such an option, to have the great luxury of living without urgency because I can seemingly shield myself from it all.\\r\\nThat is what the bad people are counting on. They\\'re counting on good people who are too tired, too apathetic, too selfish, or to oblivious to sustain their outrage. I am not going to give that gift to them.\\r\\nAs long as they\\'re fully invested in putting people through hell, I\\'m going to be as invested in pushing back against it.\\r\\nI think the people I love are worth it.\\r\\nI think you and the people you love are worth it.\\r\\nI think people I\\'ll never meet are worth it.\\r\\nAnd that\\'s the rub here: love will often look a lot like rage, as it fiercely fights on behalf of those who are being attacked. \\r\\nSo yes, angry is not all that I am, but I am rightly angry.\\r\\nAnd it would be really helpful if we could carry the load of outrage right now. \\r\\nThat would actually be a source of rest and joy and breath.\\r\\nFriend, if you really want me to be less angry, you might try being a little more angry.\\r\\nI am angry, friend.\\r\\nI wish you were angry too. /n/r Davos meeting -- Live now. /n/r Trump\\'s Attorney Lay\\'s Down the Law! /n/r Are you a Scientist who loves Science Fiction?\\r\\n\\r\\nAnd if so, may I ask for help with a Science Fiction Story? /n/r Tiffany Foster-Grant I have a problem with the Ten Commandments. Here it is: Why are there ten? We don\\'t need that many. I think the list of commandments was deliberately and artificially inflated to get it up to ten. It\\'s clearly a padded list.\\r\\n\\r\\nHere\\'s how it happened: About five thousand years ago, a bunch of reli\\xadgious and political hustlers got together to figure out how they could control people and keep them in line. They knew people were basically stupid and would believe anything they were told, so these guys announced that God\\x97 God personally\\x97had given one of them a list of Ten Commandments that he wanted everyone to follow. They claimed the whole thing took place on a mountaintop, when no one else was around.\\r\\n\\r\\nBut let me ask you something: When these guys were sittin\\' around the tent makin\\' all this up, why did they pick ten? Why ten? Why not nine, or eleven? I\\'ll tell you why. Because ten sounds important. Ten sounds official. They knew if they tried eleven, people wouldn\\'t take them seriously. People would say, \"What\\'re you kiddin\\' me? The Eleven Commandments? Get the fuck outta here!\"\\r\\n\\r\\nBut ten! Ten sounds important. Ten is the basis for the decimal system; it\\'s a decade. It\\'s a psychologically satisfying number: the top ten; the ten most wanted; the ten best-dressed. So deciding on Ten Commandments was clearly a marketing decision. And it\\'s obviously a bullshit list. In truth, it\\'s a politic; document, artificially inflated to sell better.\\r\\n\\r\\nI\\'m going to show you how you can reduce the number of commandments and come up with a list that\\'s a bit more logical and realistic. We\\'ll start with the first three, and I\\'ll use the Roman Catholic version because those are the ones I was fed as a little boy.\\r\\n\\r\\n\\x95 I AM THE LORD THY GOD, THOU SHALT NOT HAVE STRANGE\\r\\nGODS BEFORE ME.\\r\\n\\r\\n\\x95 THOU SHALT NOT TAKE THE NAME OF THE LORD THY GOD IN\\r\\nVAIN.\\r\\n\\r\\n\\x95 THOU SHALT KEEP HOLY THE SABBATH.\\r\\n\\r\\nOkay, right off the bat, the first three commandments\\x97pure bullshit \"Sabbath day,\" \"Lord\\'s name,\" \"strange gods.\" Spooky language. Spooky language designed to scare and control primitive people. In no way does superstitious mumbo jumbo like this apply to the lives of intelligent, civilized human in the twenty-first century. You throw out the first three commandments, am you\\'re down to seven.\\r\\n\\r\\n\\x95HONOR THY FATHER AND MOTHER.\\r\\n\\r\\nThis commandment is about obedience and respect for authority; in other words it\\'s simply a device for controlling people. The truth is, obedience and respect should not be granted automatically. They should be earned. They should be based on the parents\\' performance. Some parents deserve respect. Most of them don\\'t. Period. We\\'re down to six.\\r\\n\\r\\nNow, in the interest of logic\\x97something religion has a really hard time with\\x97I\\'m going to skip around the list a little bit:\\r\\n\\r\\n\\x95 THOU SHALT NOT STEAL.\\r\\n\\r\\n\\x95 THOU SHALT NOT BEAR FALSE WITNESS.\\r\\n\\r\\nStealing and lying. Actually, when you think about it, these two com\\xadmandments cover the same sort of behavior: dishonesty. Stealing and lying. So we don\\'t need two of them. Instead, we combine these two and call it \"Thou shalt not be dishonest.\" Suddenly we\\'re down to five.\\r\\n\\r\\nAnd as long as we\\'re combining commandments I have two others that be\\xadlong together:\\r\\n\\r\\n\\x95 THOU SHALT NOT COMMIT ADULTERY.\\r\\n\\r\\n\\x95 THOU SHALT NOT COVET THY NEIGHBOR\\'S WIFE.\\r\\n\\r\\nOnce again, these two prohibit the same sort of behavior; in this case, mar\\xadital infidelity. The difference between them is that coveting takes place in the mind. And I don\\'t think you should outlaw fantasizing about someone else\\'s wife, otherwise what\\'s a guy gonna think about when he\\'s waxing his carrot?\\r\\n\\r\\nBut marital fidelity is a good idea, so I suggest we keep the idea and call this commandment \"Thou shalt not be unfaithful.\" Suddenly we\\'re down to four.\\r\\n\\r\\nAnd when you think about it further, honesty and fidelity are actually parts of the same overall value. So, in truth, we could combine the two honesty commandments with the two fidelity commandments, and, using positive lan\\xadguage instead of negative, call the whole thing \"Thou shalt always be honest and faithful.\" And now we\\'re down to three.\\r\\n\\r\\n\\x95THOU SHALT NOT COVET THY NEIGHBOR\\'S GOODS.\\r\\n\\r\\nThis one is just plain stupid. Coveting your neighbor\\'s goods is what keeps the economy going: Your neighbor gets a vibrator that plays \"O Come All Ye Faithful,\" you want to get one, too. Coveting creates jobs. Leave it alone.\\r\\n\\r\\nYou throw out coveting and you\\'re down to two now: the big, combined honesty/fidelity commandment, and the one we haven\\'t mentioned yet:\\r\\n\\r\\n\\x95THOU SHALT NOT KILL.\\r\\n\\r\\nMurder. The Fifth Commandment. But, if you give it a little thought, you realize that religion has never really had a problem with murder. Not really. More people have been killed in the name of God than for any other reason.\\r\\n\\r\\nTo cite a few examples, just think about Northern Ireland, the Middle East, the Crusades, the Inquisition, our own abortion-doctor killings and, yes, the World Trade Center to see how seriously religious people take Thou Shalt Not Kill. Apparently, to religious folks\\x97especially the truly devout\\x97murder is ne\\xadgotiable. It just depends on who\\'s doing the killing and who\\'s getting killed.\\r\\n\\r\\nAnd so, with all of this in mind, folks, I offer you my revised list of the Two Commandments:\\r\\n\\r\\nFirst:\\r\\n\\r\\n\\x95THOU SHALT ALWAYS BE HONEST AND FAITHFUL, ESPECIALLY\\r\\nTO THE PROVIDER OF THY NOOKIE.\\r\\n\\r\\nAnd second:\\r\\n\\r\\n\\x95THOU SHALT TRY REAL HARD NOT TO KILL ANYONE, UNLESS,\\r\\nOF COURSE, THEY PRAY TO A DIFFERENT INVISIBLE MAN\\r\\nTHAN THE ONE YOU PRAY TO.\\r\\n\\r\\nTwo is all you need, folks. Moses could have carried them down the hill in his pocket. And if we had a list like that, I wouldn\\'t mind that brilliant judge in Alabama displaying it prominently in the courthouse wall. As long he in\\xadcluded one additional commandment:\\r\\n\\r\\n\\x95THOU SHALT KEEP THY RELIGION TO THYSELF! /n/r Josh Ray, 35, was a husband and father of a young daughter. The World Socialist Web Site spoke with Nikki Emmanuel, whose husband worked with Ray in the oil fields for six years. \"He was a devoted father and husband who loved his job,\" Nikki said.\\r\\n\\r\\n\\'Bodies of five workers recovered at site of Oklahoma gas well blast\\' / World Socialist Web Site\\r\\n\\r\\nhttp://www.wsws.org/en/articles/2018/01/24/okla-j24.html\\r\\n\\r\\n[Top row, left to right: Josh Ray, Matt Smith, Cody Risk. Bottom row, left to right: Parker Walbridge and Roger Cunningham] /n/r \\'Our\\' priorities today. <sigh>\\r\\n\\r\\nHow did that song \\'go\\'?\\r\\n\\x97 \"Teach Your Children Well\"?... /n/r A testimonial documentation. /n/r Thanks to our Emperor\\'s decree, I foresee higher prices on solar panels (and washing machines) coming in the U.S. Which will make switching to renewable energy a less attractive option.\\r\\nOn the positive side.... this might be a great bump for people who have a lot of stock in big oil companies and fossil fuel.\\r\\nDo you think Trump thought of that? ;) /n/r Horizontal monopoly in Europe /n/r #AuroaBorealis above the frozen beach at Flakstad in Lofoten, Norway. It was pretty tough to stand up and the tripod was sliding allover the place. #NorthernLights #twanight #stormhour\\r\\n\\r\\nhttps://twitter.com/AlexConu/status/956155967984619520 /n/r THESE articles are from PROGRESS REPORT\\r\\n\\r\\nJanuary 24, 2018\\r\\n\\r\\nDESENSITIZED.\\r\\nYesterday, tragedy struck Marshall County High School in western Kentucky, as a 15-year-old student opened fire, killing 2 and wounding 18 others. The victims of the shooting \"ranged from 14 to 18 years old\"\\x97young people who have (or had) their whole lives in front of them. With such a brutal attack on high school students, you\\'d expect a nation in mourning, with media outlets covering the violence non-stop.\\r\\nBut you\\'d be wrong. The story barely broke through the 24/7 Trump news cycle, and most members of Congress didn\\'t even comment on the tragedy. The President of the United States didn\\'t say anything at all, even though he spent the morning tweeting. It\\'s really no surprise that Americans have become so incredibly desensitized to such violence. In fact, the school shooting in Kentucky was the 11th school shooting of 2018\\x97and we are only 24 days into the new year. And it\\'s not just these mass shootings that are putting young people in the U.S. at such high risk. Teenagers in the U.S. between the ages of 15 to 19 are 82 times more likely to die from gun violence than teenagers in peer countries. And remember: on an average day, 96 Americans are killed with guns.\\r\\nThese statistics are shocking, but once you\\'ve heard them before, they start to become normalized. But we can take action to prevent future gun violence\\x97we just need our elected officials to have the courage to stand up to the National Rifle Association (NRA). Check out this report to see why policies must be enacted to curb gun violence in America\\x97then call your elected representatives and tell them it\\'s time to #HonorWithAction!\\r\\n\\r\\nACTION OF THE DAY\\r\\n#ProtectDreamers. On Monday, Congress ended the #TrumpShutdown without any solution for Dreamers. This is unacceptable. The next deadline is now February 8, and, in the meantime, 122 Dreamers are losing their protection every single day. We must continue to stand with Dreamers, and demand that Congress take action. Senate Majority Leader Mitch McConnell must be held accountable for his promises to bring legislation to the floor on DACA, and this legislation must provide a permanent solution for Dreamers, without compromising our values by allowing Stephen Miller and his allies to attach their anti-immigrant wishlist. Call your members of Congress today at 202-224-3121! Then, share the graphic below.\\r\\n\\r\\nWHAT\\'S TRENDING\\r\\nGlobal Gag Rule. A year ago, President Trump used an executive order to reinstate and expand the Mexico City Policy, also known as the Global Gag Rule. First instituted in 1984 by then-President Ronald Reagan and rescinded most recently by then-President Barack Obama in 2009, the global gag rule restricts all U.S. global health aid from being used to support abortion in any way. Under Trump\\'s policy, U.S.-funded nongovernmental organizations are even restricted from using private, non-U.S. funds to offer abortion care, discuss abortion, or engage in activities to change restrictive abortion laws in the countries in which they work. The reinstatement of the Global Gag Rule will lead to an increase in unsafe abortions and pregnancy-related deaths among women in the poorest countries. Early analysis of the impacts of the policy shows reductions in critical reproductive health services that cannot easily be replaced.\\r\\nDitching Collection Agencies. Twelve Senators sent a letter to the Department of Education demanding it justifies the use of private collection agencies for federal student loans. Last year, the federal government paid these contractors almost a billion dollars to recover debt from about 7 million borrowers. But, a piece published today by the Center for American Progress points out that the government spent almost as much on debt collection as it did servicing accounts for 33 million borrowers who are currently in repayment. The column describes the pitfalls of using private collection agencies, and argues that borrowers would be better served by removing them from the student loan system.\\r\\nWelcoming Communities. Immigrating to the United States can be a challenging journey, but it is made much harder for LGBTQ immigrants. A new report by the Center for American Progress explains the unique challenges that LGBTQ immigrants face, as well as how cities, service providers, and philanthropic organizations can make communities more welcoming to those individuals. Given that LGBTQ unauthorized immigrants make up 17 percent of survivors of anti-LGBTQ hate violence, it is crucial that cities provide a host of services are made available to LGBTQ immigrants to help them feel safe, protected, and assist them in thriving in their new community. As the report details, the types of services necessary include, but are not limited to, legal, health, employment, housing, language access, and education. If the U.S. is to continue to serve as a beacon of hope for LGBTQ immigrants around the world, communities must reform immigration policies and take concrete actions to improve services to ensure they are safe havens.\\r\\n\\r\\nGOOD NEWS\\r\\nSecond Chances. In November, Florida voters are going to have the opportunity to restore voting rights for more than a million people with a felony record. Over 750,000 signatures were gathered, meaning that the proposed constitutional amendment can now appear on the ballot. As one of only four states with a lifetime ban on voting, the state of Florida\\'s harsh laws prohibit those with a prior felony conviction from getting a second chance\\x97but Floridians have a chance to change that. Head to SecondChancesFL.org for more information about the ballot initiative and what it could mean for 1 in 10 Floridians. /n/r #New Hope for #Repairing a Damaged or #Aging #Immune System https://goo.gl/9G2i6h /n/r Hydrogen bonds are what keeps cellulose fibers together to form paper... :O /n/r \"It\\'s not up to you to decide! \\r\\nIt\\'s not up to me, or the pope, or Billy Graham! \\r\\nAnd for sure it\\'s not up to the government. \\r\\nIt\\'s only up to the mother, hopefully with the advice of a physician and the consideration of the father! \\r\\nBut ultimately the mother makes the final decision.\"\\r\\n(link thx to Mi Robin!) /n/r Its created a huge quackery based industry /n/r Montana says: Either the corporations let you see and use the entire Internet, nothing blocked.\\r\\nOr THEY can get out. (y) /n/r STAND WITH DREAMERS. This article is from Progress Report.\\r\\n\\r\\nYesterday, a bipartisan group of senators agreed to end the shutdown, with the promise from Senate Majority Leader Mitch McConnell that he would bring legislation on DACA to the floor. It is unclear what exactly this legislation would look like, although McConnell said it would be \"neutral\" and allow for an open amendment process. This could lead to amendments being offered by some of the most staunch anti-immigrant senators, such as Senator Tom Cotton, who helped write the cruel RAISE Act. Such nativist, racist policies would not only prevent common sense, bipartisan talks on immigration reform, but they would put more than just Dreamers at risk.\\r\\nThis agreement between moderates to end the shutdown without a permanent fix for Dreamers has drawn ire from many, including other senators, who question why McConnell should be trusted. Senator Claire McCaskill, who voted \"yes\" on ending the shutdown, claims that she is not relying on McConnell\\'s promise, but instead, the \"12 moderate Republican[s] who were willing to step forward in this.\" But remaining wary about these reassurances is not only understandable but necessary. Trump has refused to negotiate in good faith and showed that he does not actually want to help Dreamers by walking away from multiple global, bipartisan deals that could have averted this shutdown in the first place. He is more interested in appealing to the members of his base that are rabidly anti-immigrant, and these efforts to appease those individuals are being led by none other than Stephen Miller. It\\'s time for all members of Congress to stand up to such appalling policies and views and recommit to an America that welcomes immigrants with open arms.\\r\\nWe must continue to stand with Dreamers. The next deadline is now February 8th; but, in the meantime, another 2,000+ Dreamers will lose protection, in addition to the 17,000 that already have been exposed to risks of losing their jobs and deportation. If a permanent solution is not reached soon, the White House has already indicated it will start deporting Dreamers on March 5th. Read our \"Action of the Day\" to see how you can take action immediately.\\r\\nACTION OF THE DAY\\r\\n#ProtectDreamers. Yesterday, Congress ended the #TrumpShutdown without any solution for Dreamers. This is unacceptable. The next deadline is now February 8, and, in the meantime, 122 Dreamers are losing their protection every single day. We must continue to stand with Dreamers, and demand that Congress take action. Senate Majority Leader Mitch McConnell must be held accountable for his promises to bring legislation to the floor on DACA, and this legislation must provide a permanent solution for Dreamers. Call your members of Congress today at 202-224-3121! /n/r Alaska, USA. Aurora....Photography by @timthetoothninja. jan 22, 2018 #Auroraborealis #StormHour\\r\\n\\r\\nhttps://twitter.com/StormHour/status/955592797448560643 /n/r Current situation of Mt.Mayon Volcano, Albay,Philippines /n/r From Kevin G. Rhoads... (Y) /n/r From Bruce Jensen... :O /n/r NorthernLights. Alaska, USA, Jan 22, 2018 #alaska #NorthernLights #Aurora #stormhour #Auroraborealis\\r\\n\\r\\nhttps://twitter.com/thetoothninja/status/955294008720351232 /n/r WE EXPERIENCE TIME TRAVEL ALL THE \"TIME\".\\r\\n\\r\\nOver 100 years ago, Albert Einstein discovered that the rate at which time flows is affected by two parameters...gravity and the speed of the traveler. These two principles were foundational to both his Special and his General Theory of Relativity. Although it\\'s counter intuitive...it\\'s nonetheless true...the passage of time is not constant. It\\'s variable.\\r\\n\\r\\nHow so, you ask...The faster one travels relative to a stationary observer, the more SLOWLY time passes for the traveler compared to that observer.\\r\\n\\r\\nBUT, the weaker the gravity is for one individual compared to another, the more RAPIDLY time flows.\\r\\n\\r\\nIn other words, weaker gravity SPEEDS UP time passage and a faster velocity SLOWS DOWN time passage..\\r\\n\\r\\nSo here we have two competing affects. The higher relative speed compared to someone stationary on the ground, means time for the passenger passes more SLOWLY.\\r\\n\\r\\nBUT the higher one is above those on the ground, the more the flow of time speeds UP.\\r\\n\\r\\nJust how then do these two competing phenomena affect an airline passenger flying 500 MPH at an altitude of 35,000 feet.\\r\\n\\r\\nBeing 6 miles above the earth, where gravity is a tiny bit weaker than for those on the ground, means time for our passenger passes a bit faster.\\r\\n\\r\\nBut, since passengers are traveling over 500 MPH, time flows a little more slowly than for people on the ground.\\r\\n\\r\\nSo, which affect wins out?\\r\\n\\r\\nAs it turns out, the weaker gravity at the higher altitude, speeds UP the flow of time, more than the higher velocity of the aircraft slows DOWN the flow of time.\\r\\n\\r\\nThese affects have actually been calculated for a passenger who racked up 10,000,000 frequent flier miles at 500 MPH.\\r\\n\\r\\nOur passenger aged 59 millionths of a second more than his homebody wife. He has traveled that far into the future compared to folks who kept their feet on the ground.\\r\\n\\r\\nDH /n/r #Oncolytic #Virus Therapy Passes Early #Efficacy Tests in #Glioma Subtypes https://goo.gl/d7VzxC /n/r What can we learn from Mary Shelley\\'s Frankenstein?\\r\\nMaybe that doing science without collaboration, mentorship or peer review is a bad thing.\\r\\n\\r\\nOr maybe is is about the human need for love and companionship.\\r\\nFRANKENSTEIN a new Off-Broadway musical written/composed by a physicist. /n/r Ballooning Trump... /n/r It\\'s hard.\\r\\n\\r\\nI don\\'t VIEW \\'our\\' fcked up government through a partisan lens.\\r\\n-- After all, Both parties SUCK.\\r\\n\\r\\nBut damn are they ever making it hard... /n/r Post deleted due to solicitation. Please follow the rules pinned to this forum. /n/r 5 discoveries of science that will change our future?? /n/r The Trump\\'s Presidency is missing many parts, heavily fictionalized and the timeline is frequently questionable. /n/r *Here\\'s TO You!\" /n/r Reasonable question.\\r\\n\\x97 What\\'s *UNREASONABLE* is that only a Tiny SLIVER of the American populace would even think to ask it.\\r\\n\\r\\n(Here\\'s a Hint:\\r\\n\\x97 *Ka-CHING!*) /n/r (This struck me as worth sharing on a page calling itself the \"Science, Technology, and Society Discussion Corner\"):\\r\\n\\r\\nGlenn Greenwald\\r\\n@ggreenwald\\r\\n\\r\\n*\\r\\n\\r\\nTwitter is sending out messages to people telling them that, for their own good, they are documenting that the user has either followed, cited or re-tweeted an account Twitter decided is linked to Russia & its propaganda efforts. That\\'s not creepy at all. /n/r What was Victor Frankenstein\\'s  \"sin\"?\\r\\nThe Frankenstein Musical\\r\\nwww.TheFrankensteinMusical.com /n/r We\\'ve had incredible display of the aurora tonight, Jan 19, 2018 in Finnish Lapland. #StormHour #ThePhotoHour @Aurora_Zone\\r\\n\\r\\nhttps://twitter.com/Astro_Matt27/status/954478734903533569 /n/r A welcome return to some decent solar wind this evening, Jan 19, 2018 at Janiskoski, Finland @TamithaSkov #AuroraBorealis #StormHour\\r\\n\\r\\nhttps://twitter.com/MrAntiatlas/status/954522517984641024 /n/r Just another symptom of the end of democracy in the US.  https://www.facebook.com/photo.php?fbid=10215510831825837&set=a.1118153515947.20266.1290271584&type=3&theater /n/r Beauty in the eye of the microscope /n/r Rocket launch over Japan looks just like California UFO reports\\r\\n\\r\\nAnthony Watts / 40 mins ago January 19, 2018\\r\\n\\r\\nRemember when the pre-dawn SpaceX launch from Vandenberg just before Christmas created a flurry of UFO reports in Southern California? Now it\\'s Japan\\'s turn.\\r\\n\\r\\nOn Jan. 18th, the Japanese space agency JAXA launched a small rocket from the Uchinoura Space Center. It made a big display. Japanese artist and photographer Kagaya captured dramatic images of the rocket\\'s exhaust glowing in the starry pre-dawn sky over the Pacific:\\r\\n\\r\\n\"I watched the launch from Okinawa Island and photographed it using my Sony ?7RIII camera,\" says Kagaya, who has posted a must-see video of the event on Youtube:\\r\\n\\r\\nJapan\\'s new Epsilon rocket is relatively small, designed to launch scientific satellites at a fraction of the cost of its larger predecessors. On this occasion, the Epsilon propelled an Earth observing satellite to orbit, the ASNARO-2. Power by solar cells and carrying a large X-band antenna, ASNARO-2 is a synthetic aperture radar capable of imaging the surface of our planet with 1-meter resolution.\\r\\n\\r\\nShortly after the launch, noctilucent (night shining) clouds were seen over a broad swath of western Japan as ice crystals forming in the rocket\\'s wake caught the rays of the rising sun. These clouds occur naturally around Earth\\'s poles, but they are very rare at lower latitudes such as Japan\\'s. In polar regions, noctilucent clouds are seeded by specks of meteor smoke, which become frosted by naturally occurring water vapor drifting up toward the edge of space. Over Japan, the ingredients were provided by JAXA: water vapor in the rocket\\'s exhaust mixed with solid-booster aerosols to create the display.\\r\\n\\r\\nFrom NASA\\'s Spaceweather.com\\r\\n\\r\\nhttps://wattsupwiththat.com/2018/01/19/rocket-launch-over-japan-looks-just-like-california-ufo-reports/ /n/r \"The cycles of parasites are often diabolically ingenious. It is to the unwilling host that their ends appear mad. Has earth hosted a new disease \\x96 that of the world eaters? Then inevitably the spores must fly. Short-lived as they are, they must fly. Somewhere far outward in the dark, instinct may intuitively inform them, lies the garden of the worlds. We must consider the possibility that we do not know the real nature of our kind. Perhaps Homo sapiens, the wise, is himself only a mechanism in a parasitic cycle, an instrument for the transference, ultimately, of a more invulnerable and heartless version of himself\\x85a biological mutation as potentially virulent in its effects as a new bacterial strain. The fact that its nature appears to be cultural merely enables the disease to be spread with fantastic rapidity. There is no comparable episode in history\\x85 To climb the fiery ladder that the spore bearers have used one must consume the resources of a world\\x85 Basically man\\'s planetary virulence can be ascribed to just one thing: a rapid ascent, particularly in the last three centuries, of an energy ladder so great that the line on the chart representing it would be almost vertical\\x85 Western man\\'s ethic is not directed toward the preservation of the earth that fathered him. A devouring frenzy is mounting as his numbers mount. It is like the final movement in the spore palaces of the slime molds. Man is now only a creature of anticipation feeding upon events.\"\\r\\n\"The World Eaters\"\\r\\nfrom Loren Eiseley, The Invisible Pyramid, 1970 /n/r The Following Is From SOCIAL SECURITY WORKS: \\r\\n\\r\\nAs all eyes are focused on a potential government shutdown, the Senate is attempting to sneak through Alex Azar. He\\'s Donald Trump\\'s new pick for Secretary of Health and Human Services, the cabinet post that oversees Medicare and Medicaid. And he might be even worse than Trump\\'s first HHS Secretary, Tom Price.\\r\\n\\r\\nAzar is the former president of pharmaceutical giant Eli Lilly USA. During his tenure, Eli Lilly more than tripled the price of life-saving insulin medication?protecting drug company profits over patient care.\\r\\n\\r\\nNext week, the Senate will vote on Alex Azar\\'s nomination. Write to your Senators right now and tell them that we don\\'t need a pharmaceutical exec running our health care. /n/r Trump denies saying \"shithole countries\" and now some of the Republicans present support Trump\\'s statement, intimating that Dick Durbin, who made the information public, is a liar. Trump says he\\'s not a racist. Trump says \"there was no collusion\"... almost a dozen times. Trump says that the Dems are preventing his own bill, which would end DACA, from being canceled and permit DACCA to continue. Trump says this, Trump says that. I\\'m sick of Trump and want at least him GONE, but would prefer the WHOLE Trump Administration and associated cronies to also be removed from our government !!! /n/r Imperfect nature or cruel nature? /n/r From the Archives.\\r\\n-- That Carl Sagan was one helluva Human Being. R.I.P. /n/r Phew! \"From the Archives\" again...\\r\\n-- \"I don\\'t need no Stinkin\\' iPod\" edition...\\r\\n\\r\\n(Then again, what with Apple\\'s iPhones, etc., the iPod is now as extinct as the Dodo Bird along w/these turntables, isn\\'t it?) /n/r Would you like to play a game? /n/r Occam\\'s Razor /n/r Speaking the Truth.... /n/r Intelligent design is a hoax. No supernatural input was required for evolution, in fact, supernatural is an oxymoron. It does not exist. Nothing can be outside of nature. All phenomena have an explanation. All phenomena obey the laws of science.\\r\\n\\r\\nYes, we may not understand all natural processes now, but to assign what we don\\'t understand to the workings of a god is irrational. /n/r From Kevin G. Rhoads... :O /n/r Pink and blue..the bizarre ice world of Lake Ontario, Rochester, NY, USA... Thanks to @JamesMontanus #StormHour\\r\\n\\r\\nhttps://twitter.com/StormHour/status/953358338703724544 /n/r Aside from the obvious which would be a sort of Trump KGB-for-hire, I can\\'t see this as an intelligent way to go in just about ANY circumstance :/ /n/r \"Sing It, Baby!\" /n/r Even in the Hospital, I saw more things having less destructive  diseases than Trump... /n/r Reading ???? ????? amin maalouf:\\r\\n\\r\\nEach individual\\'s identity is made up of a number of elements and these are clearly not restricted to the particulars set down in official records. Of course, for the great majority these factors include allegiance to a religious tradition; to a nationality \\x97 sometimes two; to a profession, an institution, or a particular social milieu. But the list is much longer than that; it is virtually unlimited.\\r\\n\\r\\n[\\x85]\\r\\n\\r\\nNot all these allegiances are equally strong, at least at any given moment. But none is entirely insignificant, either. All are components of personality \\x97 we might almost call them \"genes of the soul\" so long as we remember that most of them are not innate.\\r\\n\\r\\nWhile each of these elements may be found separately in many individuals, the same combination of them is never encountered in different people, and it\\'s this that gives every individual richness and value and makes each human being unique and irreplaceable.\\r\\n\\r\\n[\\x85]\\r\\n\\r\\nIt can happen that some incident, a fortunate or unfortunate accident, even a chance encounter, influences our sense of identity more strongly than any ancient affiliation.\\r\\n[\\x85]\\r\\n\\r\\nIn every age there have been people who considered that an individual had one overriding affiliation so much more important in every circumstance to all others that it might legitimately be called his \"identity.\" For some it was the nation, for others religion or class. But one has only to look at the various conflicts being fought out all over the world today to realize that no one allegiance has absolute superiority. /n/r Did you ever wonder how much heat flows from the earth\\'s core to the surface, and how it\\'s distributed?\\r\\n\\r\\nI ran across this map in a paper on Enhanced Geothermal Energy.\\r\\n\\r\\nThe paper is here.  I haven\\'t looked at their economic analysis -- but it\\'s from 12 years ago, so the competing technology picture has changed enormously.\\r\\n\\r\\nPaper here:\\r\\nhttps://energy.mit.edu/wp-content/uploads/2006/11/MITEI-The-Future-of-Geothermal-Energy.pdf\\r\\n\\r\\n(Since I\\'m supplying my own graphic, I don\\'t think Facebook will link the graphic to the paper.) /n/r Light of the Zodiac #Astrophotography Tenerife, Spain of a phenomena that can sometimes be viewed after dusk in spring, and before dawn in autumn, the suns rays lighting up cosmic dust! Venus can also be seen in the centre ~ Thanks to @OllieTPhoto #StormHour\\r\\n\\r\\nhttps://twitter.com/StormHour/status/953061845488881664 /n/r Suttons Law /n/r People often ask how I can take a photo of static radiation and it be of Earth? If I am on Earth...? I do it the same way Prof Alan Guth did... He has even pointed out where Earth is for you, so you know where to look in my photo... ;) #UniversalMechanics  :) https://www.facebook.com/UniversalMechanicsStudyingTheQuantumSpaceRealm/posts/864802400357369 /n/r Finland is the nation that has the highest per capita quantity of metal bands. Finland is also where the creator of Linux was born. I\\'m reasonably certain that this is *not* coincidence! /n/r \\'Society and Technology\\':\\r\\n-- For MLK Day, a painting by Haitian artist Walter Mere. /n/r This would be a interesting ride.  Hold on to your belongings! /n/r Mankind tries to survive between superstition and (little) science. Why? /n/r For MLK Day, a painting by Haitian artist Walter Mere. /n/r WOW! Northern Lights seen tonight, Jan 13, 2018 from Troms?, Norway. Photo credit: Markus Varik. https://tinyurl.com/ybuse43d #Aurora #NorthernLights #Norway\\r\\n\\r\\nhttps://twitter.com/mark_tarello/status/952360140900052993 /n/r Anyone use a Boogie Board? LCD tablet? /n/r If you are in the mood for some reading,,,, /n/r Is your thermometer accurate?  :O /n/r Machine-tool calculations, 1972.\\r\\nHey! ...FREE! :O /n/r Microsoft says security patches slowing down PCs, servers\\r\\n\\r\\nMicrosoft Corp said on Tuesday the patches released to guard against Meltdown and Spectre security threats slowed down some personal computers and servers, with systems running on older Intel Corp processors seeing a noticeable decrease in performance.\\r\\n\\r\\n10 Jan 2018 02:15AM(Updated: 10 Jan 2018 02:55AM)\\r\\n\\r\\nREUTERS: Microsoft Corp said on Tuesday the patches released to guard against Meltdown and Spectre security threats slowed down some personal computers and servers, with systems running on older Intel Corp processors seeing a noticeable decrease in performance.\\r\\n\\r\\nThe security updates also froze some computers running AMD chipsets, Microsoft said in a blog post, citing customer complaints.\\r\\n\\r\\nShares in Intel, which reiterated on Tuesday that it saw no sign of significant slowdown in computers, fell 1.4 percent, while those of AMD fell nearly 4 percent.\\r\\n\\r\\nAMD shares have gained nearly 20 percent in the last week as investors speculated that the chipmaker could wrest market share from Intel, whose chips were most exposed to the security flaws.\\r\\n\\r\\n\"We (and others in the industry) had learned of this vulnerability under nondisclosure agreement several months ago and immediately began developing engineering mitigations and updating our cloud infrastructure,\" Microsoft executive Terry Myerson wrote in a blog post. (http://bit.ly/2mj6f3Q)\\r\\n\\r\\nSecurity researchers disclosed the flaws on Jan. 3 that affected nearly every modern computing device containing chips from Intel, AMD and ARM Holdings.\\r\\n\\r\\nMeltdown and Spectre are two memory corruption flaws that could allow hackers to bypass operating systems and other security software to steal passwords or encryption keys on most types of computers, phones and cloud-based servers.\\r\\n\\r\\nIntel said a typical home and business PC user should not see significant slowdowns in common tasks such as reading email, writing a document or accessing digital photos. (http://intel.ly/2FiL0Hk)\\r\\n\\r\\nThe chipmaker said last week that fixes for security issues in its microchips would not slow down computers, rebuffing concerns that the flaws would significantly reduce performance.\\r\\n\\r\\nRival AMD had also played down the threat, saying its products were at \"zero risk\" from the Meltdown flaw, but that one variant of the Spectre bug could be resolved by software updates from vendors such as Microsoft.\\r\\n\\r\\nBut on Tuesday AMD said it was aware of an issue with some older-generation processors following the installation of a Microsoft security update that was published over the weekend.\\r\\n\\r\\nMicrosoft said it was working with AMD to resolve the issues.\\r\\n\\r\\nApple Inc also released an updated version of its operating system software on Monday to fix the security flaw.\\r\\n\\r\\n(Reporting by Eric Auchard in Frankfurt and Supantha Mukherjee in Bengaluru; Editing by Saumyadeb Chakrabarty and Shounak Dasgupta)\\r\\n\\r\\nSource: Reuters /n/r ...Appropriate reading for our times?\\r\\n\"Tenser, said the tensor!\\r\\nTension, apprehension and dissension have begun!\" :O /n/r Anyone in Boise Idaho? /n/r I\\'m thinking about using \"Antimatter Sails\" in my Science Fiction Story, does anyone know about those? /n/r Add this to the book of lies.\\r\\nDonald J. Trump\\r\\n\\r\\nVerified account\\r\\n \\r\\n@realDonaldTrump\\r\\n 15m15 minutes ago\\r\\nMore\\r\\nThe language used by me at the DACA meeting was tough, but this was not the language used. What was really tough was the outlandish proposal made - a big setback for DACA! /n/r Things you should stop believing or saying about Astrophysics:\\r\\n\\r\\n1) The universe expands: The universe expands, it is true, but it does so at large scales such as galactic clusters, it does not expand in the Solar System and moves us away from the other planets, or that the galaxies of the Local Group move away, because if it were like that, the Milky Way would never collide with Andromeda ever.\\r\\n\\r\\n2) We are stardust: We have ATOMS of stars, but not only of them, other atoms are obtained from other means such as the collision of neutron stars, supernovas of type 1a or 1b, cosmic ray spacing and others.\\r\\n\\r\\n3) When a particle and antiparticle collide annihilate: For that \"annihilation\" must be of some particle with its corresponding antiparticle as an electron and a positron, it can not if it is electron and anti-proton for example. The truth is that they do not annihilate, they become more particles, you can understand this as joining 2 and -2 gives you 0 and from this 0 you can get other particles like 1 or -1, also if you have a certain energy you can create particles that have that same energy, for example if you have 4 and -4, you can get 3 and -3 on one side and 1 and -1 on the other.\\r\\n\\r\\n4) The cosmic microwave background is the echo or the light of the Big Bang: It would really be the light emitted 400,000 years after the Big Bang, since before that important things happened like the time of the great unification, the inflationary era, the Hadron era, neutrino decoupling and much more. /n/r \"The Republican tax bill departs from tradition to specifically target citizens of Democratic states with higher taxes via the elimination of the deductions for real estate taxes and state and local income taxes, both of which are more important in the coastal states where those taxes are high. What else will the Republicans do to make life difficult for Democrats?\"\\r\\n\\r\\n\\x97 Ted Rall /n/r Denny Haldeman https://www.facebook.com/photo.php?fbid=1140106886124829&set=a.165411843594343.36705.100003765861975&type=3&theater&ifg=1 /n/r So -- What is the \"deal\" with Hillary Clinton and the sale of Uranium One to Russia that has the REPUBS so excited?\\r\\n...Can you spell \"FAKE NEWS\"? -- Please SHARE!\\r\\n(link thx to Estel Cooper!)\\r\\nhttps://www.facebook.com/CAFE/videos/1954628538193363/ /n/r HOW MANY TIMES has Trump used the word COLLUSION\" ? He\\'s used it at least a dozen times by my count. To me, that means he and his family are GUILTY of COLLUSION !!! /n/r (Okay. \"Society\".)\\r\\n\\r\\n*\\r\\n\\r\\nUnderstand, you Must ACCEPT This!\\r\\n\\r\\nOur *Overlords* INSIST,\\r\\n\\x97 And they\\'re going to Shovel this SH*T down our throats and Up our BUTTS until We SUBMIT! /n/r Jaguar Pyramid in Belize where you can climb to the summit. Was at the complex in the jungle a few years ago. /n/r Only a few days left to #register for the 9th #Molecular #Immunology & #Immunogenetics #Congress\\r\\nIf you haven\\'t #booked your #slots yet, then now is the time to do. \\r\\n#Book here: https://goo.gl/vbyS6G /n/r \"Dialectic of Nature\"\\r\\n\\r\\nBelow are three fundamental mathematical equations that bring a solution to the interaction in nature.\\r\\n\\r\\nNewton\\'s law of universal gravitation states that two celestial bodies attract each other with a force that is directly proportional to the product of their masses and inversely proportional to the square of the distance between them. This force is independent of the kinetic energies of the masses (static solution). The left wing of the equation below is the interacting force (effect, action or cause) and the right wing is the result as reaction of mass M2. It is apparent that, in this interaction, a reason causes a result but is not affected from it. In a sense, the result fully depends on the reason, but the reason is independent of result it causes.\\r\\n\\r\\nIn Einstein field equations, the force relation works with a rather strange logic. Here too, the reason (the mass that creates the space-time curvature) is not affected by the result (the object following the shortest path along this curvature). There is also another unusual breaking off here: isolation of mathematics (geometry) from physics. The left side of the equation is the curved space-time geometry, while the right side is the physics thought to form this geometry. Physicist John Wheeler describes this situation as: \"Space-time tells the matter how to move, and matter tells the space-time how to curve\". The \"cause\" and the \"result\" guide each other, but any one of them cannot affect the other, because the one is abstract mathematics, and the other is tangible physics.\\r\\n\\r\\nWe can hardly talk about cause-and-result relation in quantum physics. There is even no guarantee that a cause will have an expected consequence. There is complete uncertainty prevailing there and only possibilities are in question. Some basic particles of constant magnitude are responsible from force relations, so the reason (force) is not influenced by its result, such that the interaction process in micro and macrocosm work in totally different way. Therefore, efforts have almost no chance to achieve a unified theory with traditional physics logic.\\r\\n\\r\\nOn the other hand, the cause and result (action and reaction) in the Field-Relative Model of the Universe (UF-ME) are interpreted together and inseparably (spotlight 10 in www.researchgate.net/publication/322317299). As you can see below, the right side of the equation is zero, and in the expression on the left side, the cause and result are considered together in a way that cannot be separated from each other. So, a reason causes a result and is affected by this result. In other words, a result emerges for some reason and becomes a part of the reason which is the source of its entity. Since the \"UF-ME\" is valid in every scale from atomic structure to cosmology, this form of relationship is universal. This way of operation in whole nature makes the dynamism and ceaseless change inevitable.\\r\\n\\r\\nSuch functioning in the nature is known as \"dialectic of nature\". It can also be called as \"dialectic of interaction\" or \"dialectic of action and reaction\". The dialectical method emerged in the ancient Greek philosophy as a \"reasoning method using the antagonisms\". Since 19th Century German philosopher G.W.F. Hegel interpreted the universe as a concrete idea, he added a material content to the dialectical relation. Friedrich Engels another German philosopher defined the dialectic as the scientific theory of totality in the context of the relationship of opponents in his unfinished work \"Dialectic of Nature\". \\r\\n\\r\\nThe dialectical method was used in ancient Greece to reach the right result in idea debates. In Marxist literature this was the fundamental method to explain the historical evolution of societies. The UF-ME shows that the process of interaction in physical nature, based on action-reaction relation, is also a complete dialectical process. At each stage after the interaction process starts, the cause (force) is not the same as the one in previous stage, because it is affected from the reaction it created. The \"result\" in later stage will also change accordingly (continuous dynamism). In this process, cause and effect cannot be separated from each other and should be evaluated together (unity of opposites). In a sense, \"God does not play dice\". In my opinion the relativistic approach to the interaction in nature is the formulation of dialectic of nature and the UF-ME is the mathematical expression of this concept. \\r\\n\\r\\nMost readers know now, how this dialectical method (UF-ME) provides perfect solutions to gravitation and other major mysteries in cosmology and astronomy that cannot be explained by contemporary physics (www.researchgate.net/publication/321474644 and www.researchgate.net/publication/317845079). Please keep in mind that the UF-ME will present its principal revolutionary solutions at atomic and subatomic level. You had better follow me. /n/r Space/time is faster the closer you are to gravity right? (See blackhole scene on interstellar) Doesnt that mean that the age of the universe is different on earth than in intergalactic space? \\r\\n\\r\\nIf that\\'s true than the 13.5 billion earth years age of the universe would be alot less away from the gravitational pull of earth, the sun, and the milky way galaxy.  How do i figure the math?Any physicists here? /n/r Some more of the pyramid complex in Belize. /n/r I can\\'t Help It. I remember all too well that - in many ways\\r\\n\\x97 I grew up in a DIFFERENT Country.\\r\\n\\r\\n(It\\'s not as if \\'my\\' government\\'s actions didn\\'t disgust me \\'way back when\\' Too; but Jeebus\\r\\n\\x97 At Least it was run by GROWN UPS...)\\r\\n\\r\\nOf course the network media conglomerate \\'handled\\' those criticizing \\'our\\' government Then as it does Now:\\r\\n\\x97 \"You\\'re FIRED!\"\\r\\n\\r\\nYears later, alas, MSNBC *Superstar* Phil Donahue didn\\'t get the memo. /n/r I wish to complain about some recent actions by two moderators of this group. I\\'ve sent a Facebook private message one of them and have received no response in 24 hours. What are my options? /n/r Hey! ...FREE! :O /n/r Personal caveat, and as I am on my 2nd Sat. nite cocktail before watching my Sat. Nite Movie I find myself hoping I don\\'t \\'ruffle feathers\\' around here, which is NOT my intent. (How many times need I mention my high regard for this group, it\\'s admin. team - several of whom are valued friends - and Most of its denizens? [I don\\'t like some & they don\\'t like me, LOL. So it goes.]\\r\\n\\x97 I\\'m making \"observations\" here, not expressing \"Criticism\" [as anyone who\\'s been the target of my criticism would likely \\'second\\']).\\r\\n\\r\\nThis is the \"S, T & S\" page. Has anyone performed any kind of analysis re. the ratio of \"S & T\" compared to \"Society\" (as exemplified by Anti-Trump\" articles) lately?\\r\\n\\r\\nPlenty of denizens have complained over the months about the preponderance of what they regard to be *Wholly Political* oriented posts. I\\'ve never been one of them. I learned Long Ago that I can \"skip\" posts I\\'m not interested in and go to those I *Am* interested in. The \\'funny thing\\' (considering I\\'m writing this) is that\\r\\n\\x97 I *like/Read - SHARE* veritable TONS of material pertaining to our Economics and Politics, so this Isn\\'t a \"Complaint\" on my part. (This is no surprise to folks who \\'drop by\\' my home base.)\\r\\n\\r\\n*\\r\\n\\r\\nBut I confess I\\'m a tad surprised at the VOLUME of material here focused Entirely on the HORROR that is \\'Our\\' Government today... *Specifically TRUMP*.\\r\\n\\r\\nTo anyone who\\'d be STUPID enough to suggest I\\'m not as DISGUSTED as *Anyone* else about what\\'s goin\\' on I\\'d respond \"Have Fun PROVING IT (Idiot):\\r\\n\\x97 You\\'re F*CKED!\"\\r\\n\\r\\nBut if we\\'re going to be so acerbic \"in the name of sharing re. *SOCIETY*\" shouldn\\'t we address The ENTIRE CORRUPT SYSTEM instead of just today\\'s Obvious and JUSTIFIABLE Target; like how Both parties \"Say one thing while acting in a way that Proves what they\\'ve Said are LIES\" and \"How the MEDIA is as beholden to the *Powers That BE* as \\'our\\' government representatives are to The Oligarchy*?\\r\\n\\r\\nShould I be sharing the TONS of material I share \\'at home\\' and elsewhere concerning this subject matter? I hold back because of the \"S & T\" aspect of the group. (I just shared - under the aegis of \"Society\" - the HUGE expos? from James Risen\\r\\n\\x97 but That CRITICAL story about the impact On our Society because of Big MEDIA *Excoriates* - JUSTIFIABLY - the Democratic Party [under the Obama administration] as much as it does those of George W. Bush and the GOP.)\\r\\n\\r\\n*\\r\\n\\r\\nJust my Two Cents (being a long time resident here who hopes my shares are usually considered \\'worthwhile\\' [as the responses to such generally suggest]). I\\'ll just wrap with:\\r\\n\\r\\n? I hope that those here who love a lot of the content here excepting all the *Partisan* Politicizing know how to \"Skip\" those posts they regard \"inappropriate\" - those they don\\'t Like - as I do. And:\\r\\n\\r\\n? I participate on a large Multitude of FB groups dealing w/everything from literature to music to economics to, of Course, politics and more...\\r\\n\\x97 and I see more anti-Trump threads here than I do on most sites Solely DEDICATED to ripping that bastard \"a New One\". (Is it just \\'Me\\' - and need I mention that eviscerating that P.O.S. w/out condemning all those Other A-HOLES in his party today is all but Inexcusable? [But then again, there IS *Plenty* on this page vilifying those fiends too, No Doubt - LOL.])\\r\\n\\r\\nAgain: I understand why that \\'bothers\\' some folks even if it Doesn\\'t Bother ME in the Least. (If this isn\\'t the \\'same page\\' I joined ages ago, \"Okay by Me\":\\r\\n\\x97 I subscribe to *Evolution*. [But some \\'things Have changed around here\\' haven\\'t they?])\\r\\n\\r\\nWot It IS... /n/r A friend of mine posted this as her Header Photo for her group page.\\r\\n\\r\\nAwesome. /n/r It has just been revealed that a new FB algorithm is now being used to read the minds of users and, worse, this program is capable of implanting irrational ideas and even anti-WWE propaganda directly into your cerebral cortex.\\r\\n\\r\\nNASA advises users who oppose this intrusion to always wear an aluminum covered hat when on the FB site.\\r\\n\\r\\nTaking this precaution will prevent FB from hijacking your brain 98% of the time.\\r\\n\\r\\nShould you be among the unfortunate 2%, stand at least 9.5 feet from your computer and type and mouse with a 10 ft pole.\\r\\n\\r\\nDoug Hullander /n/r The year 2018 has opened with an international campaign to censor the Internet.\\r\\n\\r\\n\\'Governments and corporations escalate Internet censorship and attacks on free speech\\'\\r\\nWorld Socialist Web Site\\r\\n\\r\\nhttp://www.wsws.org/en/articles/2018/01/06/pers-j06.html\\r\\n\\r\\n[Upcoming video livestream of a discussion on Internet censorship.] /n/r Any comments from actual specialists? /n/r Cheap drones.\\r\\nLocal gas station for 5$ miniflyer? /n/r Post deleted due to lack of credible source. Fox is now considered propaganda. /n/r Almost Full Moon On NWT Highway 3 about 170km from Yellowknife, Canada. The sun had set and the colors in the sky were intense. #moonphoto #moonphotography #skyphoto #skyphotos #sunset #sunsetphoto #sunsetphotography #spectacularnwt http://bit.ly/2lPn0CX \\r\\n\\r\\nhttps://twitter.com/EclecticBlogs/status/948665171899187200 /n/r Snowflake (slang)\\r\\nSnowflake as a slang term involves the derogatory usage of the word snowflake to make reference to people. Its meaning has varied, but may include a person who has an inflated sense of their own uniqueness, has an unwarranted sense of entitlement, or is easily offended and unable to deal with opposing opinions. Generation Snowflake, and snowflake are a politicized insult. /n/r ....even the horses are saying H_ _ _ No to this Winter weather! ?? /n/r YOUR PUBLIC LANDS SOLD ON THE INTERNET. At 9:00 a.m. on February 2, 2018, 2 million acres of Utah\\'s Bears Ears National Monument and Grand Staircase-Escalante National Monument will be immediately available for mining and drilling. Without paying a dime to the federal government, speculators will be able to stake a claim to mine for uranium, potash, and any other mineral that they believe can be extracted from the monuments. The oil and gas resources in the area, meanwhile, will be eligible to be sold off to energy companies through recently established private internet auctions and anonymous bidding systems that are highly susceptible to waste, fraud, and abuse. Fight back at MonumentsForAll.org. This is from the Jan 4th Progress Report. /n/r What is an Expert? An expert is a person with extensive knowledge or ability based on research, experience, or occupation and in a particular area of study and skills through study and practice over the years, in a particular field or subject, to the extent that his or her opinion may be helpful in fact finding, problem solving, or understanding of a situation..... /n/r Reported post deleted and member advised against click bait sources. Yabberz is click bait. /n/r GOP and DNC.... /n/r From Steve Cooperman... :) /n/r Multiple Sclerosis or MS as from the Mayo Clinic. In T1 weighted images, fat is bright and cerebrospinal fluid (CSF) is dark; with T2 weighting, fat is dark and CSF is bright. Proton density (PD) weighting produces image contrast on the basis of the PD of the tissue. Such weighting of images has been exploited to identify inflammatory demyelinating lesions at different stages in their evolution....\\r\\nhttps://www.mayoclinic.org/\\x85/m\\x85/symptoms-causes/syc-20350269 /n/r Flat Earthers.... 3:)\\r\\nhttps://www.facebook.com/photo.php?fbid=1573302639431167&set=gm.10157182908107715&type=3&theater&ifg=1 /n/r A Trusted Trump Confidant.... /n/r Post reported and deleted for spam. Member warned. /n/r A little late... But way cool! 3:)\\r\\nhttps://www.facebook.com/photo.php?fbid=2025423991006090&set=gm.10157177211152715&type=3&theater /n/r From Susan Ludwig... 3:) /n/r What is the PURPOSE of government? /n/r A poison is found on the skin of a species of a genuine poison dart frog. In northern Choc? Department, Phyllobates aurotaenia is used, while P. bicolor is used in Risaralda Department and southern Choc?. In Cauca Department, only P. terribilis is used for dart making. The poison is the batrachotoxins in P. terribilis are powerful enough that it is sufficient to dip the dart in the back of the frog without killing it..... /n/r ...And counting...\\r\\n(link thx to Charlie Kent!) /n/r The U.S. Navy and Army made the first attempts to fly to Hawaii. It was no easy matter. Hawaii lay about 2,400 landless miles from San Francisco\\x96a tiny navigational target in a vast ocean. Few planes could fly that distance without refueling. But Charles Lindbergh\\'s historic transatlantic. The flight in 1927 made many aviators eager to try, and crew wave from their PN-9. The Navy stationed ships every 200 miles to guide the flyers, refuel their planes, or rescue them if trouble arose. /n/r For all you Scientists who believe Time is unreal Happy ???????????? /n/r One of the most accurate weather reports I\\'ve ever seen. /n/r Static Friction keeps a stationary object at rest! Once the Force of Static Friction is overcome, the Force of Kinetic Friction is what slows down a moving object.... /n/r The \"law of physics\" is the Pauli exclusion principle, which states that two identical fermions (particles with half-integer spin) cannot occupy the same quantum state simultaneously.... /n/r Lad Akins, Director of Special Projects at REEF, said in one presentation not long ago that studies have shown that lionfish can live without food for up to 3 months and only lose 10% of their body mass.\\r\\n\\r\\nhttps://lionfish.co/lionfish-faq/ /n/r Remembering 2017...\\r\\n(pic thx to Ben Kirchner & The Washington Post!) /n/r For those interested in technology for the ongoing new area of human healthspan and lifespan, here is the International Longevity Alliance annual report for 2017. http://longevityalliance.org/?q=ila-annual-report-2017\\r\\n\\r\\nPlease don\\'t hesitate to suggest things to add by sending an email to contact_a t_longevityalliance_d o t_com\\r\\n\\r\\nOf note, to respect the statutes the board will undergo some renewal soon, also new applications of non-lucrative associations as ILA members are welcome. http://longevityalliance.org/?q=partners http://longevityalliance.org/?q=ila-membership-application-eligibility /n/r Science was a big part of the dumpster fire of 2017.   https://www.facebook.com/photo.php?fbid=1833325163367762&set=a.1275702055796745.1073741825.100000708879267&type=3&theater /n/r Vladimir Putin is a LOT smarter than Donald Trump... /n/r Have we all been \"buying the Brooklyn Bridge\"? :O /n/r A thought for our New Year... /n/r Interesting analysis! /n/r Wow! (Y) /n/r Which infusion I could drink to see the context from the observation point of a ladybug! A Hamlet doubt or a hidden desire? This is the question! /n/r Love your Mother...\\r\\n...board! :O /n/r Forces, mass -- and acceleration! :O\\r\\n(Hey! ...FREE!) /n/r Solving the World\\'s Problems through Creativity (cont.) /n/r Roy Moore... /n/r You will get A CHARGE OUT OF THIS!! :O\\r\\n(Hey! ...FREE!) /n/r What is universal force-motion equation (UF-ME)?\\r\\nThe UF-ME is an expression of all force and motion relations between all entities in nature with a single and simple mathematical formula. The equation expresses all the entity energies of the masses together with spin and linear motion energies in the cause and effect context.  The figure below shows how this equation is formed in the case of two bodies. For an n-body system the equation can be generalized in the form of matrices. The figure shows the escape and spin field velocities of each mass at the point where the other mass is as well as their own spin and linear velocities. For the UF-ME, it is a decisive factor for how each mass perceives the total velocity fields that the other produces. The relative effective field distribution factor ?R, is obtained by dot product of two relative total field velocities perceived by each mass. The UF-ME is the gradient of (?_R/r). \\r\\nSome basic properties of UF-ME:\\r\\n Owing to the dot product of perceived total field velocities of each mass, the UF-ME both preserves the excellent symmetry of Newton\\'s universal law of gravitation and implements Einstein\\'s relativistic concept in its true nature.\\r\\n The force equation F=-GM_1 M_2 ?(?_R?r) is reduced to Newton\\'s universal law of gravitation if the masses have no kinetic energies (k_i  = 0 and v_i  =0), i.e., Newton\\'s law applies only to systems that are stationary relative to each other. The Newton law will lose its validity as the interacting objects accelerate. \\r\\n The equation uses the relative velocity of the masses to each other, the total mass of interacting objects and mass size ratio owing to the true inertial reference frame.  In this way, it becomes applicable the force relationship between any two masses each may have any size. For example, if the two interacting masses are equal then ? = 1, and if it is the sun-photon relationship or M_1?M_2 then ? = 0. \\r\\n The relative effective field distribution factor ?_R  of UF-ME includes both the causes (field velocities) and the results (kinetic energies of masses). In other words, the UF-ME interprets the cause and effect of an interaction simultaneously in a single and simple mathematical expression. \\r\\n In general, the relative effective field distribution factor ?_R has also polar and azimuthal components due to the relative linear and spin velocities of the masses. Therefore the solution of UF-ME gives lateral force components besides the main radial one. \\r\\n The UF-ME shows that gravitational repulsion is also possible under certain circumstances (www.researchgate.net/publication/317845079). We only familiar with the radial gravitational component (attractive only) from the conventional physics. \\r\\n The polar component of interacting force causes small masses to cluster at the equatorial plane of the main body (Spiral galaxies, planets positions in Solar system, Saturn\\'s rings, most recent observation of positions of Uranus\\' moons on its unusual equator plane etc.)  (Conventional theories require globular clusters only due to their spherically symmetric nature.)\\r\\n Azimuthal component causes all members of a balanced cluster system to revolve in the same direction which is the spin direction of the main body in the system (spiral galaxies and motion of the planets in the Solar system). (The spherical symmetric force relation requires rotating in random directions.)\\r\\nThe UF-ME is a single expression which is valid in every scale in the Universe. Readers can estimate that in the cosmic scale and visible universe with naked eye M_i?1 unit mass and r?1 unit distance while k_i?1, but in atomic and subatomic scale M_i?1 unit mass and r?1 unit distance while  k_i?1. The universal gravitation constant G  in UF-ME serves in every scale because it is a parameter specifying space fabric as indicated in one of previous spotlights. (In contemporary physics  G has no role at all in atomic scale). /n/r Donald Trump\\'s least presidential moments so far... /n/r Donald Trump\\'s least presidential moments so far... /n/r Donald Trump\\'s least presidential moments so far... /n/r Donald Trump\\'s least presidential moments so far... /n/r From M Patricia McLaughlin... (Y) /n/r Immigrants FROM Silicon Valley are taking all your jobs! :O\\r\\n....AND your salaries!\\r\\n(link thx to Patricia Darling!) /n/r For their relentless work against the environment, the poor, and the planet. /n/r They only needed to convince just enough voters in those precise Electoral College districts that would put Trump over the edge...\\r\\n...AND THEY DID. Trump actually won with an advantage of less than 100 thousand votes in just the critical state districts... /n/r *DEEP Science* (cont.) /n/r John Lundin writes: Sorry to share this on Christmas, but a friend shared it today and I found it interesting - and scary. You don\\'t need to be a rocket scientist to crunch the numbers and realize this isn\\'t sustainable... /n/r Society:\\r\\n\\x97 And now, a commercial message for our rapacious would-be \\'overlords\\': /n/r This is now \"everyday technology\" -- it is amazing how it works -- for EVERYONE! :O\\r\\nhttps://www.facebook.com/kgrhoads/posts/10210842465601206 /n/r (I saw and had planned to share the article before I saw my friend\\'s post.\\r\\n\\r\\n\"Net Neutrality: The Coming Battles in the Congress and the Courts\"\\r\\nhttps://www.nakedcapitalism.com/2017/12/net-neutrality-coming-battles-congress-courts.html\\r\\n\\r\\nBut I like her graphic so I\\'m sharing the article as listed in her share...) /n/r 3d printing and the next steps? How much can be \"printed\" at home? /n/r Vr and Ar and virtual reality and augmented reality? The next steps? Show things? Interact with things? And record things and data tracking? /n/r SOMETHING WE ALL NEED TO THINK ABOUT\\r\\n\\r\\nPaleontologists, archeologists, evolutionary biologists and biochemists concur with near unanimity. Upwards of 99% of all species that once walked the earth, that once burrowed in the soil, inhabited the bodies of others, swam in the oceans and rivers or flew in the atmosphere above...are no more. They are extinct. The scientific data ( the fossil record ) indisputably confirms it. Again 99%...gone!\\r\\n\\r\\nIn almost every instance, organisms disappeared because of their failure to adapt to a changing environment: they failed to adapt to drastic weather alterations; failed to find new habitats after theirs was destroyed; fell victim to invasive predators; or they were annihilated by asteroid impacts, etc.\\r\\n\\r\\nFaced with an inability to cope, to change with conditions that threatened their very survival, they succumbed en mass, completely disappearing, never to return again.\\r\\n\\r\\nAnd so humans must face the age old question: will our species follow suit or will we be the exception to this fatal fate of our own species ? \\r\\n\\r\\nWill our superior intelligence enable us to beat those odds? Will we able to adapt...to outwit what has been the inevitable death knell for most every other creature?\\r\\n\\r\\nOr will our willful ignorance, our false sense of superiority and invulnerable result in the exact opposite...will these foibles be our undoing...will they result in the same fate that befell the doomed 99%?\\r\\n\\r\\nFurther, do we have the will to determine which direction our intellect will lead us? Will our failure to mitigate the effects of climate change with all its dire consequences spell the eventual end? Will over use of anti-biotics do us in? Will thermonuclear global warfare be the end of our reign on earth?\\r\\n\\r\\nOur planet has already undergone 5 mass extinction events. We are overdue for the 6th. Will we be its victims?\\r\\n\\r\\nFifty, one hundred, one thousand, ten thousand years from now, will we have become extinct like the thousands of other species. Could we have prevented, but because of sheer stupidity didn\\'t? \\r\\n\\r\\nAs Carl Sagan once said, \"there is no evidence that help will ever come from any outside source to save us from ourselves. It\\'s up to us and us alone\". /n/r WHY are National Parks and Monuments and other Public Lands so important? Why should places like Bear\\'s Ears, Grand Staircase - Escalante, Cascade-Siskyou, Katahdin Woods and Waters and Giant Sequoia National Monuments be protected? \\r\\n\\r\\nHere is just one reason - without them, only the rich would have access to lands.\\r\\n\\r\\nPer the Washington Post, the top 100 private land owners in the U.S. own a combined 40.2 million acres, an amount of land roughly equivalent to Connecticut, Rhode Island, Massachusetts, New Hampshire and Maine combined. That\\'s up from only one New Hampshire and Maine\\'s worth of America (27.2 million acres) in 2007.\\r\\n\\r\\nAnd this argument completely sets aside the local economic, ecological and scientific value of these lands.  There is no valid excuse for removing protected status UNLESS you intend to capitalize the resources and damage the lands. /n/r Whom do you consider as YOUR family? :O /n/r Certainly! -- Of course! (Y) /n/r From Kevin G. Rhoads... /n/r If you were to read this book and absorb even a 10% of it you\\'d fall in love with your mind. /n/r Its a good start towards fixing things.Even with 45 running things, they\\'re doing something right.\\r\\n\\r\\nhttps://www.npr.org/sections/health-shots/2017/12/18/571666553/food-and-drug-administration-plans-crackdown-on-risky-homeopathic-remedies /n/r DLCC Breaking News (via DLCC.org) \\r\\n  \\r\\n\"Last-Minute Tax Break Could Enrich Trump And GOP Leaders\"\\r\\n\\x96International Business Times, December 15\\r\\n\\r\\nNow we know why Donald Trump\\'s tax bill was written entirely behind closed doors.\\r\\n\\r\\nTo win the last few votes necessary to ram their bill through, GOP leaders reportedly inserted a last-minute loophole that will save themselves and Donald Trump\\'s businesses millions of dollars, while the rest of the bill raises taxes on tens of millions of middle-class families.\\r\\n\\r\\nThe two most prominent beneficiaries of this crooked deal? Tennessee Sen. Bob Corker \\x96 who flipped his vote from NO to YES within hours of the final bill\\'s release \\x96 and Donald Trump himself.\\r\\n\\r\\nMeanwhile, 50% of Americans expect this bill to raise their own taxes to pay for Trump and Corker\\'s sweetheart deal. /n/r Congenital Insensitivity to Pain(CIP) /n/r \\'Pai\\'s the Limit\".  Great, I can\\'t wait. :/ /n/r One practical application of my cube theory would be an extremely strong support material. It could be made of very light-weight material and be extremely stable and withstand tremendous weight. /n/r Ended this day....1916 Bataille de Verdun met fin ? la bataille de Verdun, le plus long engagement de la Premi?re Guerre mondiale, s\\'ach?ve en ce jour, apr?s dix mois et pr?s d\\'un million de pertes totales subies par les troupes allemandes et fran?aises. La bataille avait commenc? le 21 f?vrier.... /n/r WOW! Northern Lights seen last night, Dec 17, 2017 from Troms?, Norway. Photo credit: Marianne Bergli. #Aurora #NorthernLights #Norway\\r\\n\\r\\nhttps://twitter.com/mark_tarello/status/942742299867525120 /n/r Personally I would rather discuss science on this page rather than politics. There are plenty of other forums to discuss politics. /n/r Shakespeare - Hamlet /n/r No matter how many doors you lock, someone will find their way in. /n/r This facebook page seems more dedicated to issuing threats to its members and censorship than it does to Science and Technology... I don\\'t even know how I got signed up for this page... I\\'m done with it, bye. /n/r REGENERATION, RENEWAL OF THE NERVUS TISSUE, DIVISION OF NEURONS. SENAD ?e?erbegovi?  25. Mar 2017 The frogs can regenerate neurons and nerve tissue, healing is similar and the same principle as dermatoglyphic findings in humans (genes shape), proteins in this process are in two places, those in the cell wall of a few, over the signal broken off the molecules occur hromatin protein appearance of tissue damage, and then activate the sharing of cells to fill the space tissue damage, when the damage is filled in, the shape of the tissue, the surface station sent a signal molecule that occurs hromatin in protein to stop cell division. In humans they are, and at possible dermatogliphy frog parts of nerves and organs, though this upgrade people you need to add a larger number of designed protein, for now though just AFM research, because we need to understand the Insert protein on the default locations. human neurons, have mechanisms for repairing and replacing hromatin and other proteins, but for now they don\\'t have the Division ... cartilage as a supportive tissue, in some cases, it can function as a bone, if it has a greater firmness. /n/r Stock Market Arbitrage.... /n/r Higher Education.... /n/r The difference between being a cynic and a satirist.... /n/r Sexual Harassment....(I\\'ll be hanged for this) /n/r The Story of Christmas.... /n/r And -- turned in grades for Fall semester!\\r\\n...Only one student (out of 34) failed the course!\\r\\nhttps://www.facebook.com/notes/william-a-boyle/some-thoughts-for-the-end-of-the-semester-/10150182386345686/ /n/r ...What\\'s WRONG with this picture? :O /n/r Some said that the old fiat cu?rency system by the Fed steals the wealth of the working and middle class people for the rich and super rich.  \\r\\n\\r\\nWe know now that Fed manipulates not only the markets but the monetary system in favor of the super rich thus, a new paradigm was invented by Nakamoto in 2009.  Called the digital crypto currency, it was introduced to get out of the manipulation and control of the Fed. \\r\\n\\r\\nBankers says it is a hoax. But crypto currency is rising. One reason perhaps  is--no bank intermediaries thus no interests. \\r\\n\\r\\nIn fact, not only the people are investing into cypto currencies but countries too, like Venezuela is into petro coin now. It is backed by oil. Russia, Syria and Iran too, are thinking of going into crypto currency to fight the sanctions by the US. Possibly, North Korea will jump into it, too.\\r\\n\\r\\nJust recently, however, the bankers  set out a futures trading on crypto currency. Some experts on crypto currency said, however, it is not affected by the chronic cycle of bubble and bursting as the old fiat currency system of the Fed. \\r\\n\\r\\nWhat do you think about this on going \"war on cu?rencies\"? Will the crypto currency of the people finally end the control and manipulation of central banks? /n/r How many people  here think that autonomous vehicles are actually a good idea? It seems like a hack of the system by terrorists would be horrific even if it doesn\\'t happen very often. And it seems like it could interfere with evacuations from hurricanes, fires, etc. if the servers don\\'t have power. /n/r Post deleted due to non credible source and information. Member advised. /n/r WOW! Meteor seen Wednesday, Dec 13, 2017 night from Ibaraki, Japan. Photo credit: KAGAYA. #Meteor #Space\\r\\n\\r\\nhttps://twitter.com/mark_tarello/status/942078996795285504 /n/r Federal Communications Commission /n/r Height of Idiocy.... /n/r A good attitude\\r\\nhttps://m.facebook.com/story.php?story_fbid=10155702827472745&substory_index=0&id=45052217744 /n/r There is a limit.... /n/r Hope for people with this annoyance.. /n/r Anti-censorship game.\\r\\nSuggest Alternative words or phrases for each below.\\r\\nSuggest means for fighting, I.e. undermining, censorship in government.\\r\\n\\r\\nEvidence-based\\r\\nScience-based\\r\\nFetuses\\r\\nTransgender\\r\\nVulnerable\\r\\nEntitlement\\r\\nDiversity /n/r Do you still have some of these around? :O /n/r Terms I Dislike.... /n/r 3 book recommendations for those who want to understand better the future during their x-mas holidays:\\r\\n\\r\\n- ZERO TO NONE, by Peter Thiel\\r\\n\\r\\n- BOLD, by Peter Diamandis\\r\\n\\r\\n- THE FUTURE IS WAITING, by... Martin Gallardo\\r\\n\\r\\nI was very excited to get the first copies last night and I could not wait to share it with you. Very grateful to all the people who read it, so far very positive reviews.\\r\\n\\r\\nYou can get the .pdf version here\\r\\n\\r\\nhttps://martin-gallardo.com/products/the-future-is-waiting?variant=44909386258#qcref=true \\r\\n\\r\\nAnd the Amazon .kindle version here\\r\\n\\r\\nhttps://www.amazon.com/Future-Waiting-compilation-mind-blowing-predictions-ebook/dp/B076YWJVGN/ref=sr_1_2?s=books&ie=UTF8&qid=1513346168&sr=1-2&keywords=the+future+is+waiting /n/r American politicians.... /n/r A little Tuesday Morning Funny. /n/r KYLO KILLS SNOKE /n/r Anytime someone puts a lock on something you own, against your wishes, and doesn\\'t give you the key, they\\'re not doing it for your benefit. /n/r No one has rights over me. /n/r Post deleted as to being a conspiracy theory. Member informed. /n/r After a year -- are there any doubts? /n/r Bored of the Rings..... /n/r An amazing historical account of how these two geniuses interacted with each other... /n/r WOW! Northern Lights seen last night, Dec 13, 2017 from Troms?, Norway. Photo credit: Marianne Bergli. #Aurora #NorthernLights #Norway\\r\\n\\r\\nhttps://twitter.com/mark_tarello/status/941114467034726405 /n/r Be prepared to open your wallets and pocketbooks... /n/r Passion is inversely proportional to the amount of real information available.... /n/r Which is more important, community, or individual freedom? /n/r After decades of war and spending 4.4trilion dollars the US is losing ground in the middle east. Although I am surprised at first withTrump\\'s latest move in Israel, it isn\\'t difficult to understand his decision.   That is if one considers the current situation of the US economy--it hasn\\'t recovered since the 2008 financial mojo. \\r\\n\\r\\nOf course, we all know that proclaiming Jerusalem as the capital  of Israel was used for a long time as a leverage to Israel. I\\'m sure Trump fully knows well the direct consequence of his move--it will unify the Arabs against Israel.  We are seeing this already. \\r\\n\\r\\nIn my view, his audacious move is another way of saying \"let us leave the middle east and let Russia and China do the peace process.\" Of course, Trump is fully aware of the consequences in getting out of the middle eastern war. \\r\\n\\r\\nWhat do you think? /n/r What do you think? \\r\\nIt\\'s okay, Pluto can\\'t hear you! \\r\\n\\r\\nhttps://m.facebook.com/groups/799675960211308?view=permalink&id=831928046986099 /n/r An amazing SF author! (Y) /n/r WOW! Northern Lights seen last night, Dec 11, 2017 from Fairbanks, Alaska, USA. Photo credit: Sacha Layos. #Aurora #NorthernLights #Alaska\\r\\n\\r\\nhttps://twitter.com/mark_tarello/status/940567425099206656 /n/r Here\\'s what the Ole Farmer\\'s Almanac calls for this winter. /n/r THIS IS SCIENCE\\r\\n\\r\\nDiffering opinions are fine and are to be expected when the topics are somewhat nebulous, uncertain or not quantifiable. \\r\\n\\r\\nBut in regards to evolution and to certain findings in cosmology, the uncertainty has largely been removed by convincing evidence. \\r\\n\\r\\nThat\\'s not to say there will never be modifications, all theories in science are amenable to revisions as discoveries are made, but the basic principle of evolution, in particular, is as if carved in Stone. \\r\\n\\r\\nThe same holds true for other scientific principles. Examples...Newton\\'s  precepts in classical physics were modified by Einstein\\'s theory of relativity and HIS ideas were modified by quantum mechanics. \\r\\n\\r\\nSuch flexibility is the hallmark... the nature of science. Driven by new discoveries, it\\'s forever self-correcting, always improving upon itself...always searching for the truth...and in doing so it hopefully improves the world and the human condition. /n/r Comment deleted due to lack of credible source. Please use discretion when responding. /n/r \"There is a great deal of research to establish a strong relationship between career development and student background, particularly socioeconomic status (Hill, Pettus, and Hedin 1990; Mestre and Robinson 1983; Rolle 1977). Scientists tend to come from well-educated white families (Grandy 1994; Pearson 1986). Lack of knowledge and familiarity on the part of underrepresented minorities in terms of what constitutes careers in STEM may contribute to their limited presence in these fields (Hill, Pettus, and Hedin 1990). Knowledge about STEM careers and exposure to scientists\\r\\nand engineers have been found to increase minority students\\' commitment to a STEM major, degree aspirations, and commitment to a STEM career (Good, Halpin, and Halpin 2001; Rolle 1977; Wyer 2001).\" /n/r Monday morning funny! /n/r World\\'s first mindfile powered artificial intelligence robot to complete a university course: Philosophy of Love. /n/r For me it is like a painting of elegance and harmony. /n/r Don\\'t forget, folks! -- \\r\\nThis is the kind of LIBERTARIAN right-wing DEEP BRAINWASHING that we\\'re up against! :(\\r\\nhttps://www.facebook.com/photo.php?fbid=10214616808792073&set=a.4197367970637.180064.1177861531&type=3&theater /n/r Light and sound show! :O\\r\\n(link thx to Alex Maldonado!)\\r\\nhttps://www.facebook.com/donatodandre/videos/10203579772852199/ /n/r \"Information is a \\'lock-and-key\\' phenomenon --\\r\\nAs you do not know which locks you will come up against in your life, it is best to obtain as many keys as you can as early as possible -- \\r\\nThis is known as getting an education.\" /n/r Nuremberg Trials... /n/r (y) Like Lifestyle & please Share! /n/r How long has this been going on? /n/r BRAIN WASHING\\r\\n\\r\\nIndoctrination, be it religious, political or otherwise is a disease of the mind, often a permanent part of one\\'s make up.\\r\\n\\r\\nOnce this mental programming is inculcated into young minds, especially, the thought patterns are almost impossible to erase.\\r\\n\\r\\nThus, the brain washed individuals are impervious and resistant to reasoning. They insist on maintaining their belief system no matter how improbable and unreasonable they may be, often becoming defensive when confronted.\\r\\n\\r\\nThree examples that come immediately to mind are radical Islamists, racists and Christian fundamentalists ( evangelicals). /n/r \"And these blast points, too accurate for Sandpeople. Only Imperial Stormtroopers are so precise.\"\\r\\n\\x97 Ben Obi-Wan Kenobi /n/r (La prova di una particella  contenente quattro tipi di quark ? stata evidenziata per la prima  volta dai dati del collisore Tevatron presso il Fermi National Accelerator Laboratory (Fermilab) in Illinois. La nuova particella ? composta da un quark bottom, uno strange quark, un quark up e un quark down. La scoperta potrebbe aiutare a chiarire le complesse regole che governano i quark: le minuscole particelle fondamentali che formano i protoni e i neutroni all\\'interno di tutti gli atomi dell\\'universo.)\\r\\nThe evidence for a particle containing four types of quarks was first shown by data from the Tevatron collider at the Fermi National Accelerator Laboratory (Fermilab) in Illinois. The new particle consists of a bottom quark, a strange quark, a quark up and a quark down. The discovery could help to clarify the complex rules that govern quarks: the tiny fundamental particles that form the protons and neutrons within all the atoms of the universe /n/r So - Where are all these new \"NAZIS\" coming from?\\r\\nDoes anyone wonder how this \"self-radicalization\" happens?\\r\\nIs it fundamentally different from the online \"self-radicalization\" of the ISIS bozos of a few years back?\\r\\n...Is it \"natural\" or is it being COVERTLY promoted? /n/r Comment deleted. Members must not insult while discussing topics no matter who is right or wrong. /n/r How bad does the smell of gas have to get before they evacuate us. Maybe after we all start puking and passing out?\\r\\n\\r\\n\\'Near walkout at Detroit auto plant over unsafe conditions\\'\\r\\nWorld Socialist Web Site\\r\\n\\r\\nhttp://www.wsws.org/en/articles/2017/12/07/jnap-d07.html\\r\\n\\r\\n[Ambulances and Fire Department emergency vehicles at plant Saturday morning.] /n/r Just WHAT are the Repubs trying to PROVE? /n/r QUESTION: Shortly after the \"Big Bang\", it must have been dark; right?  Until the Higgs Field slowed subatomic particles by giving them mass, which allowed electrons to bind to protons; there was no matter.  You need hydrogen condensed by gravity to start fusion, so how long till the light came on?  Thanks. /n/r Tech question...on Facebook.  I notice they have no live human help available.  My personal page news feed is not loading at all anymore.  Every other FB page loads normally.  Nothing has changed on my edge and two hours of searching for an answer has yielded nothing thus far.  I can still read PMs and my denny page loads fine.  Just missing the news feed.  PM me. /n/r Why was the discussion about ET life, that became a discussion about possible microbes in outer planet ocean(s), get terminated? I thought it was relevant, current and educational. /n/r Thanks a lot to the team and members for accepting me as a member of it. I am really glad. /n/r Would anyone like to discuss Extraterrestrial Life in association with Science, please? /n/r Without looking it up, do you know the next large number name after trillion? /n/r GOD IN THE MACHINE https://www.theguardian.com/technology/2017/apr/18/god-in-the-machine-my-strange-journey-into-transhumanism /n/r Marine plastic pollution is right out of hand. /n/r So... In this reality if you kill one person you will be hunted until the day you die or until you are brought to justice, however if you SALUTER AND MAIM THOUSANDS of  people, a few who just happen to be your fellow countrymen... People will smile in your face a serve you blindly until they day you die AND they will erect statues and build airports dedicated to you.... Got it... /n/r Is it \"Deja vu, all over again\"? /n/r Jews and the Palestinians.....(My Son) /n/r Magical..Super moon from Aberdeenshire, Scotland, this morning, Dec 4, 2017. Thanks to Gordon Robertson @gordo_rob #Supermoon #StormHour\\r\\n\\r\\nhttps://twitter.com/StormHour/status/937766688094871553 /n/r Now 45 knew Flynn was under investigation for lying to the FBI. More Obstruction evidence. /n/r Amazing northen lights over Torsfjorden, Norway #Space #StormHour\\r\\n\\r\\nhttps://twitter.com/TopAstroPics/status/937682051746148357 /n/r Or the whole thing could be faked from the git go. /n/r Spam posts for another group and for an event have been deleted.\\r\\n\\r\\nMembers Matthieu Ehostidc and Sandra D. Sabatini are hereby warned. /n/r ... a physics curiosity for the perversely curious... /n/r #MotivationalMondaysRead\\r\\n*Society Discussion /n/r So the answer some seem to give is that you need the root to get DNA.  I didn\\'t know this.  A very good question though /n/r Which could have a larger quantity?\\r\\nA. Grains of sand\\r\\nB. Stars\\r\\nC. Universes\\r\\nD. Realities /n/r Northern lights #Karakok has now passed #Kautokeino\\'s lowest measurement today at -30.8 degrees and is now coldest in the country by -31.8 degrees. Photo: Kjell H. S?ter taken at Karasjok, Norway\\r\\n\\r\\nhttps://twitter.com/Meteorologene/status/936341971882467328 /n/r \"Making AmeriKa GREAT Again\" (for the Rapacious Kleptocrats) cont. /n/r Ren? Descartes\\' philosophy was one of the most important motivating forces for the development of the modern scientific way of thinking - and this way of thinking has dramatically improved the daily lives of so many people all over the world.\\r\\nRen? Descartes\\' philosophy can be summarized in his quotation:\\r\\n\"Dubito ergo cogito; cogito ergo sum.\"\\r\\n(\"I doubt, therefore I think; I think, therefore I am.\")\\r\\n--Ren? Descartes (1596-1650). /n/r Light pillar in the evening Omsk (Russia, November 30, 2017)\\r\\n\\r\\nhttps://twitter.com/MeteoRUSSIA/status/937005574897758208 /n/r Innovative technology of manufacturing without glue of ecologically pure plate material from a vegetative waste (straw). The basis of technological production of such plate material is ecologically pure hydrolysis of a fibrous substance with the use of carbonic acid. Through this hydrolysis, lignin is liberated from the fiber structure, which is a plasticizer when forming a plate material. \\r\\nThe specific mass of the material is 1.35 g / cm3, the compressive strength is 50 kg / cm2 at a thickness of 20 mm. The preliminary price of the material is no more than $ 5 m2.\\r\\nIt is patented in the Republic of Kazakhstan.\\r\\n100% environmentally friendly material! /n/r ....... \"#REALScienceMarchesOn in-spite of #TheScienceDeniers! /n/r This is what I call \"An Accurate Analysis\". /n/r Humanity has begun to have a scientific understanding of the magnitude of the problem -- WHY do we refuse to believe it?\\r\\n(pssst! ...Have you ever seen a bacterial colony growing in a petri dish?\\r\\n...Did it continue to grow INDEFINITELY?) /n/r Manic Depressive.... /n/r Not to ask a question.... /n/r Here are some interesting facts. Now imagine what life would be like if they had the kind of freedoms we do, and didn\\'t have that embargo /n/r Astrophiz 48: Dr Jacinta Delhaize \\'Star-forming frenzy\\'\\r\\nIn this fabulous extended 60min episode we  feature Dr Jacinta Delhaize who is a Postdoctoral Researcher at University of Zagreb, Faculty of Science in Croatia. https://soundcloud.com/astrophiz/astrophiz-48-dr-jacinta-delhaize-star-forming-frenzy\\r\\n\\r\\nJacinta has devolved a \\'stacking technique\\' to combine data to overcome the problem of detecting weak hydrogen signals from distant galaxies. She has been using data from  the Parkes Dish and the Hershel instrument to helps us understand the role of hydrogen in the evolution of galaxies.\\r\\nAfter recently moving from ICRAR in Western Australia to Croatia,  her research is now looking at how black holes at the centres of galaxies can effect star formation, and is now using data from the Jansky VLA to continue this collaborative research.\\r\\n \\r\\nFor observers and astrophotographers, Dr Ian \\'Astroblog\\' Musgrave tell us what to look for in our morning and evening skies over the next days and weeks, and how to best observe the imminent Geminid Meteor Shower. In \\'ian\\'s tangent\\' he tells us about Australia\\'s early eminence in Space with the 50th Anniversary of the launch of our first satellite, WRESAT\\r\\n \\r\\nJacinta has an excellent youtube vid and you can see her describe her research on the infrared-radio correlation of galaxies at tinyurl.com/jdelhaize\\r\\n \\r\\nHer website is at www.jacintadelhaize.com and she also has a public twitter and instagram account. Both are @jdelhaize\\r\\n \\r\\nIn the news:  Teams of Radio astronomers and optical astronomers both research the Magellanic Clouds in the Southern hemisphere, and both come up with exciting discoveries. /n/r The less I post about Dump, the better I feel.... /n/r #ThrowbackThursdaysRead\\r\\n\\r\\n*Society Discussion /n/r I love a great Political cartoon: Lord help me, this \\'Hits the SPOT\\'...\\r\\n\\x97 Welcome, AmeriKa, to the World of NOW...\\r\\n\\r\\n(Then again, anyone who\\'d posit this graphic transcends national boundaries and TIME/HISTORY re. <Cough COUGH!> \"Civilization\" itself;\\r\\n\\x97 No argument from me, friends.)\\r\\n\\r\\nI *Still* subscribe to the Theory of Evolution.\\r\\n\\x97 I guess I have to acknowledge it doesn\\'t apply \"Across the Board\", eh?\\r\\n\\r\\n*\\r\\n\\r\\nIt\\'s difficult to defend the \\'Worthiness\\' of Humanity sometimes. /n/r ...Does anybody really know what time it is? 3:) /n/r FREE -- and with solutions! :) /n/r Will the Agung volcano be able to cool the planet\\'s climate again?\\r\\n\\r\\n11/29/2017\\r\\n\\r\\nThe awakening of the Agung volcano has alerted the island of Bali, which has forced to evacuate at least 100,000 inhabitants of the area and close the airport in the city, causing in turn that 120,000 tourists are trapped.\\r\\n\\r\\nBut beyond these important disorders for the local population and its visitors, there is some expectation in the scientific community for the possibility that a large explosion may happen in the coming days and that the resulting cloud of ash and gases will later cause some effect in the climate of the Planet. And it is that this same volcano already caused in 1963 a global cooling of the Earth that was estimated of two years of duration.\\r\\n\\r\\nAlert status in Bali\\r\\nLast Saturday the authorities of this beautiful Indonesian island raised the alert level as the ash cloud became even more important, ordering the immediate evacuation of around 40,000 people in the area with the perspective that this week it would be necessary to evict a total of of 100,000 inhabitants.\\r\\n\\r\\nBut many of them are not willing to leave their belongings and their animals, perhaps because it is not the first time that the Balinese see Agung spitting ash into the atmosphere and are accustomed to its roar. The previous major eruption dates from 1963 and left 1,600 dead.\\r\\n\\r\\nThe temperature of the Planet dropped to two degrees\\r\\nAfter that violent explosion of ash and gases to the Indonesian sky, the scientists detected in the two following years a global cooling of the temperature of the planet between 1 and 2 ?C.\\r\\n\\r\\nOther volcanoes that caused similar eruptions later, such as Mount St. Helens in 1980 (the most disastrous eruption in the history of the United States) did not have the same impact on Earth\\'s climate.\\r\\n\\r\\nThe explanation is fundamentally due to two factors: the presence of sulfur in large proportions and the height at which the fumes and gases are high in the eruption.\\r\\n\\r\\nThe relevant role of sulfur in modifying the climate\\r\\nFor an eruption of this type to be able to modify the climate of the planet it must happen that the cloud of ash and gases is able to reach the stratosphere, an area of ??the atmosphere where these compounds can block the arrival of solar energy.\\r\\n\\r\\nAnd is that the high presence of sulfur dioxide makes the filtering of sunlight more effective, contributing more importantly to reduce the temperature of the planet.\\r\\n\\r\\nAgung is located in an area of ??the Planet that, due to its latitude, has the stratosphere at an altitude higher than other regions closer to the Poles, and this is another handicap for ash and sulfur to block the sun\\'s energy, unless the eruption is as powerful as that of 1963 and get rise above the troposphere.\\r\\n\\r\\nThe closest precedent that managed to modify Earth\\'s climate was Mount Pinatubo (Philippines) when it erupted in 1991, sending 20 billion tons of sulfur dioxide to the sky.\\r\\n\\r\\nhttp://www.cazatormentas.com/sera-capaz-el-volcan-agung-de-enfriar-el-clima-del-planeta /n/r More FAKE NEWS encountered via a Graphic Meme I stumbled across...\\r\\n\\r\\n*\\r\\n\\r\\nDo not believe that meme claiming a group of baboons is called a congress. Not only is it wrong, it disrespects baboons.\\r\\n\\x97 A group of baboons is called a \"Troop\". /n/r Aims to be the World\\'s largest Digital Bank\\r\\n*Society Discussion /n/r Politicians... Military Leaders... The Intelligence Community Spooks...\\r\\n\\x97  Oh yeah: we can\\'t forget most of the mainstream media, especially on TeeVee...\\r\\n\\r\\nI say we should Trust Them. What about you? /n/r The first century of the American political experiment... /n/r *Fantasies*\\r\\n\\x97 Not just for Children anymore... /n/r Have we been sold a \"bill of goods\"? /n/r WOW! Northern Lights & Light Pillars earlier this month, Nov 2017 (light reflection from ice crystals) in Norman Wells, Canada. Photo credit: Nicky Lynn. #Canada\\r\\n\\r\\nhttps://twitter.com/mark_tarello/status/934977900000038917 /n/r I must confess that I had never heard of this guy before this article appeared in my inbox. It seems his is a name I should have known. /n/r Post deleted for spam and advertising. Member strongly warned. /n/r Friends, many of you helped us by supporting the restoration of HMS President 1918, one of only three World War One ships left, I am pleased to say that we now have nearly all the funds needed but have one last hurdle to overcome. Richard Desmond, owner of Express Newspapers, after having ran a campaign to raise money for us in the Sunday Express agreed to allow the Ship\\'s walkway to attach to his land at London Bridge. He has now, for no reason, changed his mind and we need that access. \\r\\nCould you email Richard or write to him at\\r\\nThe Northern & Shell Building\\r\\nNumber 10 Lower Thames Street\\r\\nLondon EC3R 6EN\\r\\nto ask him to allow the access and support us as he said he would. His email is via his PA allison.racher@express.co.uk \\r\\nThank you so much! /n/r \"Truth, Justice and The American Way\" (cont.) /n/r WE EXPERIENCE TIME TRAVEL ALL THE \"TIME\".\\r\\n\\r\\nOver 100 years ago, Albert Einstein discovered that the rate at which time flows is affected by two parameters...gravity and the speed of the traveler. These two principles were foundational to both his Special and his General Theory of Relativity. Although it\\'s counter intuitive...it\\'s nonetheless true...the passage of time is not constant. It\\'s variable.\\r\\n\\r\\nHow so, you ask...The faster one travels relative to a stationary observer, the more SLOWLY time passes for the traveler compared to that observer.\\r\\n\\r\\nBUT, the weaker the gravity is for one individual compared to another, the more RAPIDLY time flows.\\r\\n\\r\\nIn other words, weaker gravity SPEEDS UP time passage and a faster velocity SLOWS DOWN time passage..\\r\\n\\r\\nSo here we have to competing affects. The higher relative speed compared to someone stationary on the ground, means time for the passenger passes more SLOWLY.\\r\\n\\r\\nBUT the higher one is above those on the ground, the more the flow of time speeds UP.\\r\\n\\r\\nJust how then do these two competing phenomena affect an airline passenger flying 500 MPH at an altitude of 35,000 feet.\\r\\n\\r\\nBeing 6 miles above the earth, where gravity is a tiny bit weaker than for those on the ground, means time for our passenger passes a bit faster.\\r\\n\\r\\nBut, since passengers are traveling over 500 MPH, time flows a little more slowly than for people on the ground.\\r\\n\\r\\nSo, which affect wins out?\\r\\n\\r\\nAs it turns out, the weaker gravity at the higher altitude, speeds UP the flow of time, more than the higher velocity of the aircraft slows DOWN the flow of time.\\r\\n\\r\\nThese affects have actually been calculated for a passenger who racked up 10,000,000 frequent flier miles at 500 MPH.\\r\\n\\r\\nOur passenger aged 59 millionths of a second more than his homebody wife. He has traveled that far into the future compared to folks who kept their feet on the ground. /n/r Well, if you didn\\'t think things could get worse, they are apparently about to get a whole lot worse.\\r\\nYour rights of equal access to information and communication on the Internet hangs by a thread, and if this \\'gutting\\' of Net Neutrality goes through, what you see and access online will be able to be influenced and controlled by dark money. :/\\r\\nPlease share /n/r NEW: Earth-like exoplanet found orbiting the star Ross 128:\\r\\n\\r\\nhttps://youtu.be/QkRgGRHA4ao?list=PL3RiFKfZj3pv1ZqpFxuZinoGtUGEOankw\\r\\n\\r\\nPlease share this post and comment below. /n/r From Kevin G. Rhoads... /n/r Ren? Descartes.... /n/r Sagan prescience, predicting tRumpism.  https://www.facebook.com/seth.jarvis.14/posts/10209313266742563 /n/r Using Gold Nanoparticles to Kill Cancer (American Physical Society ) http://www.physicscentral.com/explore/action/pnb-nanotherapy.cfm /n/r Aristarchus of Samos.... /n/r Looking with pretty ruth upon my pain. Pain pays the income of each precious thing. /n/r #MedicareForAll, or should we just continue cranking out Hundreds of BILLION$ more for bombs, drones, bullets and all that other \\'Indispensable for National Security\\' crap \\'our\\' government just loves using to simultaneously wage multiple wars we started all around the globe at the behest of the kleptocrats who are literally preying on us? /n/r From M Patricia McLaughlin... (Y) /n/r Thermal-Electric power, or how to turn what comes down from the sun, and when it hits a dark surface, it becomes heat. How to use this process to make electricity? /n/r Remembering Forrest J Ackerman\\r\\n(November 24, 1916 \\x96 December 4, 2008)\\r\\n\\r\\nDuring his career as a literary agent, Ackerman represented such science fiction authors as Ray Bradbury, Isaac Asimov, A.E. Van Vogt, Curtis Siodmak, and L. Ron Hubbard. For over seven decades, he was one of science fiction\\'s staunchest spokesmen and promoters.\\r\\n\\r\\nAckerman was the editor and principal writer of the magazine \"Famous Monsters of Filmland\".\\r\\n\\r\\nAckerman was central to the formation, organization, and spread of science fiction fandom, and a key figure in the wider cultural perception of science fiction as a literary, art, and film genre. Famous for his word play and neologisms, he coined the genre nickname \"sci-fi\". /n/r Nerds keep it real on Black Friday /n/r right or wrong /n/r It turns you on -- and it\\'s FREE! :O /n/r Hi everyone, I think most of us have experienced it, the question is this, when we open our eyes ( after standing under the sun with our eyes closed) for ex:standing in prayer, we see something greenish what causes it, and how it emerges and varnishes can someone explain. /n/r Bronze Aged Savages.... /n/r To Hard.... /n/r Trump kept sitting and talking.... /n/r BREAKING: Few minutes ago a huge #fireball was spotted in Southern Italy and in some zone of #Maghreb. #Meteor #astronomy\\r\\n\\r\\nhttps://twitter.com/meteorologo777/status/933398760570441734 /n/r Red sprites, lightning, zodiacal lights, milky way and orange airglow on La Palma, Canary Islands, Spain last night! Nov 21, 2017 What a combo and experience! #astrophotography #astronomy @StormHour @GTCtelescope @fotoastronomica @weathernetwork @StormchaserUKEU @WeatherNation @WetterOnline @earthskyscience\\r\\n\\r\\nhttps://twitter.com/ADphotography24/status/933264072094527489 /n/r Uber orders up to 24,000 Volvo XC90s for driverless fleet\\r\\n\\r\\nhttps://goo.gl/y7gTcM\\r\\n\\r\\nUber has entered into an agreement with carmaker Volvo to purchase 24,000 of its XC90 SUVs between 2019 and 2021 to form a fleet of autonomous vehicles, according to Bloomberg News. The XC90 is the base of Uber\\'s latest-generation self-driving test car, which features sensors and autonomous driving computing capability installed by Uber after purchase on the XC90 vehicle....\\r\\n\\r\\n#PintFeed #UberSelfDrivingTestCar #VolvoXC90SUVs #uber #Technology #News /n/r What about the notion that God is arbitrary? /n/r Tragically, there are Too Many AHoles out there who agree w/this creep... /n/r Post deleted due to Over-Zealous remarks by a member. Member warned against grandstanding. /n/r If I believed in the Bible.... /n/r Yeah, that\\'s what we Do.\\r\\n\\r\\n(At least you can see here that *I* know how to spell \"Yeah\".) /n/r WOW! Northern Lights seen last night, Nov 20, 2017 from Talkeetna, Alaska, USA. Photo credit: Dora Miller. #Aurora #NorthernLights #Alaska\\r\\n\\r\\nhttps://twitter.com/mark_tarello/status/932959683010334721 /n/r WE JUST SAW THE MOST INCREDIBLE NORTHERN LIGHTS IN ICELAND SOME MINUTES AGO!! Nov 21, 2017 @NorthLightAlert @AuroraAddicts @StormHour @ThePhotoHour @TamithaSkov @earthescope @KPAuroraAlert @severeweatherEU @dartanner #northernlight #Auroraborealis\\r\\n\\r\\nhttps://twitter.com/Muhammediceland/status/932773886093086720 /n/r Being Right is Often not Enough.... /n/r From Kimball Corson... :( /n/r Post and its poster on how to engage in software piracy deleted.\\r\\n\\r\\nTo avoid imperilling the group\\'s existence by breaking FB\\'s rules, instaban\\x99?. /n/r Primary Erythromelalgia (EM or PM) /n/r I\\'d read about how \"Safety Factors\" have no impact whatsoever in this \"governmental decision making\", *So HERE We GO!\":\\r\\n\\r\\n\"Keystone Pipeline Leak Won\\'t Affect Last Regulatory Hurdle\\r\\n\\x97 A state official says discovery of a 210,000-gallons oil spill from the Keystone pipeline will not affect the decision of Nebraska regulators next week on a massive expansion of the system.\"\\r\\n\\r\\nhttps://www.usnews.com/news/business/articles/2017-11-17/keystone-pipeline-leak-days-before-nebraska-expansion-ruling /n/r November 1973 President Nixon\\r\\ntold the nation on TV,\\r\\n\"I am not a crook.\"\\r\\nWhen will Trump do the same? /n/r It is no accident that among the younger generation there is rising interest in and support for socialist policies. A recent survey found that more young people in America would choose to live under socialism or communism than under capitalism.\\r\\n\\r\\n\\'The government attack on US college students\\'\\r\\nWorld Socialist Web Site\\r\\n\\r\\nhttps://www.wsws.org/en/articles/2017/11/20/pers-n20.html /n/r Kimball Corson writes: Corporate Tax Cuts?\\r\\nPresent corporate effective rates, before any cuts, are about the same as many effective rates in foreign nations. We are high in our published rate of 35% only. To get a sense of this, in the extreme, consider the following: /n/r Sounds like an interesting event!\\r\\n...This November 30th!\\r\\nhttps://www.facebook.com/events/1391128724342379/?notif_t=event_calendar_create&notif_id=1511211801782050 /n/r A person becomes what the environment he is surrounded by. /n/r Middle Section of a SAR Arc. Went right across the sky from West to East. Watched and photographed for 1 hour. Taken August 21/ 2017 10: 54 p.m Eastern Manitoba, Canada @subauroralarcs @AuroraMAX @BIAUS @TheWeatherNetUS @aurorawatch @StormHour @CBC @Photo_Space @_SpaceWeather_\\r\\n\\r\\nhttps://twitter.com/BrentMckean501/status/932419580743499777 /n/r From #RandyBresnik, NASA Astronaut /n/r Really I couldn\\'t get the idea about gravitational waves /n/r WOW, This is too funny !!!  I first saw this post tonight, but maybe you saw it after Estel Cooper posted it Thursday. Too bad this post isn\\'t an actual fact !!! /n/r Juvenile Detention Sentence.... /n/r Donald J. TrumpVerified account @realDonaldTrump\\r\\nNov 17\\r\\n\\r\\nPut big game trophy decision on hold until such time as I review all conservation facts. Under study for years. Will update soon with Secretary Zinke. Thank you!\\r\\n40,857 replies 21,750 retweets 113,968 likes /n/r How I got my kids to swim.... /n/r Never attribute to malice that which can be adequately explained by stupidity. /n/r On November 18th 1978 the Cult leader Jim Jones instructed 400 members of his church, \"People\\'s Temple\", to commit suicide in Guyana. So when is the next mass killing in the States ? /n/r SAR ARC: August 21st , 2017 10:51 P.M facing east. Taken in Eastern Manitoba, Canada. The Aurora was also strong low in the Horizon to the north. Air glow was very strong also at this time. @subauroralarcs @AuroraMAX @BIAUS @TheWeatherNetUS @aurorawatch @StormHour @CBC\\r\\n\\r\\nhttps://twitter.com/BrentMckean501/status/932284984806838272 /n/r November 19, 1863 famous\\r\\nGettysburg Address was delivered by\\r\\nPresident Lincoln.... /n/r No one understands the working of a false democracy more than America does at the moment.... /n/r November 19th, 1969 Apollo 12 lands on the Moon.... /n/r We have all been born with an addiction,\\r\\nit\\'s called breathing. /n/r Tonic Immobility (TI).... /n/r He is the Biggest underachiever of all Time.... /n/r Hamlet Act 1, scene 3, 78\\x9682....Thanks Pat.... /n/r Do not invoke conspiracy as explanation when ignorance and incompetence will suffice, as conspiracy implies intelligence. /n/r Alexander the Great, became a keystone of Western Philosophy. Alexander became legendary as a classical hero in the mold of Achilles. /n/r Somehow I don\\'t think Mr. Spock or Mr. Data would approve of Facebook\\'s AI robot algorithms...\\r\\n\\r\\n(Now watch as I get banned for a month for making this comment. /n/r The more Trump lies, the more Frequent Flyer Points he gets, so he can use them for that Big Jet Aeroplane to fly around in.\\r\\nIt\\'s called, \"Compulsive Spending\" /n/r No one mentions what it is like to be groped by a woman.... /n/r Teach them to earn things, not demand things.... /n/r A huge leap for trucking /n/r In Canada, we have a 50% female cabinet.... /n/r Dunning\\x96Kruger Effect.... /n/r Self aggrandizement is defined as exaggerating one\\'s own importance or people who like their own post. /n/r From Steve Cooperman... :)\\r\\nhttps://www.facebook.com/grammarly/photos/a.158139670871698.33824.139729956046003/1783753021643680/?type=3&theater /n/r how many stupid deniers are left here? /n/r Turkey still does not admit to this.... /n/r Aurora borealis.. Oulu, Finland on Nov 7, 2017. Thanks to Thomas Kast\\r\\n@ThomasKast1 #Northernlights #StormHour\\r\\n\\r\\nhttps://twitter.com/StormHour/status/931592444877131778 /n/r When will US politicians learn anything about economics? Yesterday, the House passed a \"Tax Reform\" bill that can be expected to reduce the US GDP and cost the US hundreds of billions of dollars because of cuts to preventive health care. /n/r Art and technology are never strangers.\\r\\n\\r\\nWhile I was at Symbolics, one of our customers was Thinking Machines, with their then pioneering massively parallel architecture, the Connection Machine.\\r\\n\\r\\nHow cool that it\\'s now in MoMA.\\r\\n\\r\\nMore below... /n/r When will US politicians learn anything about economics? Yesterday, the House passed a \"Tax Reform\" bill that can be expected to reduce the US GDP and cost the US hundreds of billions of dollars because of cuts to preventive health care. /n/r In the original Roman calendar.... /n/r To be lonely, one must first know what it is not to be. /n/r I am thinking about all of the recent exposures of attacks on women by their economically \"superior\" men and how that disparity protected this evil and silenced those women, and has done so historically for ALL working class people. \\r\\nFranken being in the rich and powerful capitalist class automatically suppresses any \"middle or poor class\" (capitalist termed) working class people from enacting their human rights, as they, including this woman sexually abused by Franken, are also suppressed by their own mindset that this is simply the way it is. They are convinced there can only be \"rich, middle class, and poor\" and there can be nothing more but to suck up to the rich and hope this helps them to become one themselves to escape its evil.\\r\\nShe said: \"I wanted to shout my story to the world with a megaphone to anyone who would listen, but even as angry as I was, I was worried about the potential backlash and damage going public might have on my career as a broadcaster.\"\\r\\nTHAT is capitalism. \\r\\nThe working class must come to class consciousness to understand they are the majority being repressed and victimized, and that the capitalist class can only perpetuate this evil by assimilating them to its cause. \\r\\nThere is only one ethical solution and that is to join in a working class revolution to bring workers to absolute democratic power over the capitalist class if they ever expect to take any and all of its evil down. /n/r As my friends know I don\\'t engage in much *Trump Bashing*; it\\'s too damned Easy and besides, millions of my FB compadres do it vigorously on a daily basis.\\r\\n\\x97 Ripping the GOP\\'s guts out? Yeah. I DO do that, especially when they do Absolutely INSANE BS like this... /n/r \"No, it\\'s not Pluto, but lurking in the dark outer reaches of our solar system, twenty times farther from the Sun than Neptune, is what NASA claims is a large ninth planet....NASA estimates that Planet Nine is about 10 times as massive as Earth, making it a rocky \"super-Earth\" (more on super-Earths later). It has a wide elliptical orbit that takes it as far as 100 billion miles from the Sun...A super-Earth is a rocky planet with slightly more or substantially more mass than Earth, though scientists believe that planets with much more than 1.6 times Earth\\'s radius can no longer be rocky....Super-Earths are common; most solar systems have them, and therefore it\\'s likely that our solar system is no exception. If Planet Nine really is out there, its secrets are about to be revealed.\"\\r\\n\\r\\nhttps://secondnexus.com/science/nasa-9th-planet-super-earth/2/ /n/r Is there \"Hope\" left?... /n/r What is Hyperspace?\\r\\n\\r\\nCan it be used for Travel? /n/r small wins in math... c/o my daughter /n/r WOW! Northern Lights seen Tuesday, Nov 14, 2017 night from Fairbanks, Alaska, USA. Photo credit: Sacha Layos. #Aurora #NorthernLights #Alaska\\r\\n\\r\\nhttps://twitter.com/mark_tarello/status/931179593616314368 /n/r I am looking to learn R language.Please guide me how to start as a Beginner i.e IDE and reference documents,videos,tutorials would be of great help.Currently I am working on Java with Eclipse IDE.\\r\\nThanks in Advance /n/r Hi there\\r\\nDoes someone here have experience working with stochastic partial differential equations (SPDE) for spatial modeling using integrated nested Laplace approximations (INLA) in R? I would love to hear from your experience.\\r\\nThank you in advance ;) /n/r Help!! i need some subject, publication, book about using business intelligence/big data in banking(for exemple fraud detection..) tnks y /n/r Most #banking, #financial services, and #insurance (BFSI) organizations are striving to adopt a fully #data-driven approach to grow their #businesses and enhance customer services. Experts believe, like most other industries, #bigdata #analytics will be a game changer for companies in the financial sector. For more visit http://bit.ly/2Db1Rdz /n/r Urgent Hiring for the following job SEO / SOCIAL MEDIA SPECIALIST, let\\'s start your 2018 with a new Challenge ...\\r\\nA company considered as a major player in its industry is looking for an SEO / Social Media Specialist\\r\\n\\r\\n\\x95 Knows how to make content move and go viral across Facebook, Twitter, and other social networks\\r\\n\\x95 Understands the essentials for optimizing content for SEO\\r\\n\\x95 Creative mindset, analytical and writing skills\\r\\n\\x95 Updated on the latest digital marketing trends \\r\\n\\r\\nA competitive compensation package plus performance incentives and bonus and career growth await the successful candidate. Please email your resume at \\r\\nhead. recruitment2017@gmail.com /n/r Hi, I\\'m searching a subject about combined internal audit with business intelligence /n/r I\\'m searching for case studies about Data Mining in B2B Business Company. Any idea? /n/r HAHAHA\\r\\nBitcoin Vs Money by Google Trend!\\r\\nBitcoin Burst Very Soon! :) /n/r Walk-in for Data Engineering Professionals\\r\\n\\r\\nhttps://www.facebook.com/events/100109240784704/ /n/r Walk-in for Business Analytics Professionals\\r\\n\\r\\nhttps://www.facebook.com/events/526501987728210/ /n/r Wikipedia is a rich source of well-organized textual data, and a vast collection of knowledge. What we will do here is build a corpus from the set of English Wikipedia articles, which is freely and conveniently available online.\\r\\nhttps://www.kdnuggets.com/2017/11/building-wikipedia-text-corpus-nlp.html /n/r MOL2NET: FROM #MOLECULES TO #NETWORKS (>10 000 followers)!!! Official group link: https://www.facebook.com/groups/chembioinfo.networks/ Topics: #Science (All Areas), #Chemistry, #Physics, #Statistics, #Medicine, #Computational science, #Nanotechnology, #Bioinformatics, #Education, etc. CALL FOR PAPERS: MOL2NET, International Conference Series (Free of cost), #MDPISciforum, #Switzerland, http://sciforum.net/conference/mol2net-03, Submission until 2017-Nov-30. Workshops: #USA, #Spain, #Italy, #Brazil, etc. Selected Papers for Journal #Nanomaterials, IF = 3.55, #Experimental and #Computational #Nanosciences, http://bit.do/mol2net-nanomat-issue. Chairperson: Prof. Humbert Gonzalez-Diaz, ORCID: https://orcid.org/0000-0002-9392-2797 /n/r Any free course for Hadoop or Spark for data engineering? /n/r Chaque Jour est une occasion de merci au Cr?ateur car notre vie est entre ses mains. Sur cette terre sois tu choisis d\\'?tre mauvais soit tu choisis d\\'?tre une bonne personne, c\\'est le libre arbitre.\\r\\n\\r\\nla m?chancet? sur cette terre s\\'est accrue, de plus en plus de guerres, l\\'esclavage de retour, l\\'adoration du dieu argent, sorcellerie, nous montre que beaucoup ont choisis le mal et ils sont nombreux ? le faire.\\r\\n\\r\\nle mal est devenue tellement recurent que c\\'est devenue la norme. or le salaire du p?ch? c\\'est la mort mais le don gratuit de Dieu c\\'est la vie ?ternel.\\r\\n\\r\\nchoisissons de faire le bien, choisissons de d?noncer le mal, de d?noncer l\\'esclavagisme non pas seulement en libye mais en arabie saoudite, au liban et dans tous ces pays arabes o? plusieurs sont hypocrites ils rev?ts une apparence de pi?t? pourtant ils sont plein de m?chancet? et de p?ch?.\\r\\n\\r\\nil crie Allah 6 fois par jour aux yeux de tous mais viol, vole et tue en cachette. \"or le diable ne vient que pour voler, d?rober et ?gorger\" A bon entendeur\\r\\n\\r\\nChoisissons le bien, aimons notre prochain, prenons soin de ceux qui nous sont proches...nos ?pouses et ?poux, nos familles notre semblable car tu aimeras ton prochain comme toi m?me et tu ha?ras le mal /n/r #Hadoop Video #Tutorial Complete Playlist.\\r\\n\\r\\nKnow More..\\r\\nhttp://cutt.us/AYpvM /n/r Hello all\\r\\n\\r\\nPlease can you help me \\r\\n\\r\\nHow can I get Dataset about job searching in Egypt?\\r\\n\\r\\nThanks /n/r Explaining simple Ensemble learning for time series forecasting in R ->\\r\\nhttps://petolau.github.io/Ensemble-of-trees-for-forecasting-time-series/ /n/r Networking and networking models at\\r\\n\\r\\nhttps://m.facebook.com/groups/540482969435657 /n/r Data science of network modelling at\\r\\n\\r\\nhttps://m.facebook.com/groups/540482969435657 /n/r How does the amount of tax collected differ around the world? What are the global trends in tax collection? How reliant are developing countries on natural resource revenues?\\r\\n\\r\\nThe recently updated Government Revenue Dataset has the data needed to answer all these questions and many more.\\r\\nhttp://bit.ly/2yYkIXN /n/r Curso de Introducci?n a Big Data con MongoDB y Hadoop\\r\\n\\r\\nYa son cientos los alumnos interesados que han aprendido a utilizar las herramientas de Big Data para la gesti?n de informaci?n para sus productos o servicios. Os invitamos a conocer de primera mano una de las tecnolog?as mas importantes de nuestro presente. Muchas gracias.\\r\\n\\r\\nhttps://culture-lab.es/curso/curso-online-introduccion-a-big-data-con-mongodb-y-hadoop/ /n/r Data Science and Analytics related discussion, trends, activities, opportunities and information can be found on following page... https://www.facebook.com/groups/1501661686712546/ /n/r Network models at\\r\\n\\r\\nhttps://www.facebook.com/EurostatStatistics/photos/a.1846529728965286.1073741828.1846196185665307/1972533073031617/?type=3 /n/r Hello everyone. Does anyone know a library or database which contains semantic clusters of words? The use-case is movie and video analytics. Primary for EN, in the future also for PL. The output should be like that w1,w2,w3 are associated with anger, w4,w5,w6 are associated with action and so on. The best output be some memberships function mf(w,c) which returns value in the [0,1] interval based on how much is the word w close to cluster c. /n/r FinTechs are looking for big data analysis applications:\\r\\n\\r\\nhttps://m.facebook.com/groups/540482969435657 /n/r Artificial Intelligence, Blockchain applications, Machine learning: join at\\r\\n\\r\\nhttps://www.facebook.com/groups/bigmefi/ /n/r Anybody has some good coding music to recommend? (By \"coding music\" I mean the kind with no lyrics that mess with your thinking.)\\r\\n\\r\\nMost of the time I just listen to Chopin, Hans Zimmer, also \"Social network\" soundtrack. But after a while I just get bored when repeated too many times.\\r\\n\\r\\n?? /n/r INTERESTED IN BLOCKCHAIN APPLICATIONS?\\r\\nJOIN:\\r\\nhttps://www.facebook.com/groups/bigmefi/ /n/r Looking for a Quant or Datascientist cofounder, future CSO/CTO of a fintech startup based in Paris and offering a P2P credit insurance service for BtoBtoC or BtoC markets.\\r\\nSkills needed: team leader/builder, applied math (probabilities - monte carlo method -, quantitive finance, stats, graph analysis), algorithms, machine learning, api and web/mobile clients dev.\\r\\nMore details by MP. /n/r Machine learning and Artificial Intelligence - how it is leveraging the growing body of digital data to help solve humanity\\'s greatest challenges. https://ibm.co/YPC-MN #IBMML #IBMDSX /n/r Talking with a QA expert and designer of robust testing frameworks as we discover the importance of pair-wise testing.  https://ibm.co/YPC-PG1 /n/r Spent some quality time with the CIO of Wumart in China and presented my book to him /n/r About GST enabled Accounting Software to contact us info@mavericksindia.com or call us on 020-27272626 /+91-8983344954 /n/r Here\\'s a free code to the Big Data Analytics World Championships  -- TEXATA100. Round 1 starts 30th September 2017 online. Register at: http://www.texata.com /n/r Blockchain and P2P economics discussion on at:\\r\\nhttps://www.facebook.com/groups/540482969435657/admin_activities/ /n/r \"The short sighted data scientist might see this as a threat, giving lesser-trained colleagues an opportunity to compete with or outshine them \\x96 but I don\\'t believe that\\'s the case. Instead, I see believe this trend will serve only to empower data scientists within their organization. The ability to get buy-in from leadership and staff has always been one of the biggest challenges for the profession, and giving more colleagues a chance to understand the unbridled power of data will only make that obstacle easier to overcome.\" /n/r Big data and data science in economics and finance\\r\\nhttps://www.facebook.com/groups/bigmefi/ /n/r I have a question which would be happy to know your idea about.\\r\\nIn your opinion, can high perplexity of natural languages all be sought in brain structure using current imaging tools or knowledge gathered by neuroscientists, or much of that cannot be seen as that is caused by consciousness or mind or etc?\\r\\n\\r\\nis high perplexity of natural languages due to complexity of brain structure ; or instead, because of complexity of consciousness, mind or etc ? /n/r would you regard transfer learning as special category of analogy? /n/r I am new to data science, what is the most used analysis tool?   And what does it do better, than the others?\\r\\n\\r\\nPython, R, Rubi, Or the Old School Excel?   Feel free to mention any that are missing and you prefer over the mentioned options. /n/r If you looking for a #WebDevelopment Company..!!\\r\\nThen Contact us We have Skilled web programmers to deliver professional #WebApplication development solutions and much more #WebServices.\\r\\nFor more details please visit our website:- http://www.mavericksindia.com/ /n/r Anyone from/in Dallas? /n/r Pra quem estiver em SP /n/r Hello. I\\'m new here.\\r\\n\\r\\nInterested to learn about Big Data from the very Basic. Can anyone suggest me the best books to start with?\\r\\n\\r\\nThanks for your cooperation. /n/r Hello Experts,\\r\\n\\r\\nPlease advise [Gaming Industry]:\\r\\nI have some data in multiple tables with hundreds of columns.\\r\\nI have created an SQL statement that takes what I think I will need to predict some columns (number of deposits, amount of deposits, life time value etc).\\r\\nUsing Python I have imported the data, cleaned and scaled it.\\r\\nNow after I have run PCA on my data frame I see (using bar plot) that I have around 7 principal components.\\r\\n\\r\\nHow can I derive from these 7 principal components which of the columns of my data frame are actually most informative?\\r\\n\\r\\nThe goal is to take these columns as input to multilinear regression to predict other target columns.\\r\\n\\r\\nPlease help me with this/suggest anything else that can help me.\\r\\nThanks in advance!\\r\\nSteve /n/r Data Science and Analytics related discussion, trends, activities, opportunities and information can be found on following page... https://www.facebook.com/groups/1501661686712546/ /n/r Hire & get hired as a remote data scientist! Enlarge your talent pool & job opportunities to the whole planet! New FB group for AI job offers!! https://www.facebook.com/groups/1054526381348851/ /n/r ????????, ? ??????????????, ??????????, ??????????-????????????, ??????? ????? ????? ?:\\r\\n- ??????? ? ?? ????????\\r\\n- ?????? ? ???????????\\r\\n- ?????????????? ????? ???????????\\r\\n- ??????????? ??????? ???????\\r\\n- ?????? ?????????? ??????????\\r\\n- ?????? ??????????? ? ??????????\\r\\n\\r\\n?????? ????????, ???? ??????? ??? ????? ? Python ? ???????? ????????.\\r\\n\\r\\n?????? ????? ??????, ?????? ????????? ??????? ? ?????? ???????????? ?????????????? ? ???????? ?????????? ??? ???? ????. /n/r Herding Unicorns: how to manage a data team British Museum profusion #datascience #data #business #management /n/r The deadline for the CIKM workshop \"Data & Algorithm Bias\" is getting closer! Submit your work before July 2nd! More information in http://dab.udd.cl/2017 /n/r Launch of the Data Science section of the Royal Statistical Society #DSSLaunch RSS Data Science profusion #datascience #statistics #data #science /n/r Dear All,\\r\\nI am looking for any studying material for the canonical correlation analysis. If anyone know a good material please contact me. /n/r Making Money Out of Data ranked #5 on the Kindle eBook: #5 BestSeller in the Econometrics Category.\\r\\n\\r\\nLearn from my experience at large Corporates on how they Make Millions of Dollars out of Data through #analytics.\\r\\n- Read 5 Real Business Success Stories from the industry and a step-by-step guide to an innovative approach to creating incremental value through advanced analytics techniques. /n/r Want to know about data science in economics and finance? Join in at:\\r\\n\\r\\nhttps://www.facebook.com/groups/bigmefi/ /n/r Making Money Out of Data ranked #5 on the Kindle eBook: #5 BestSeller in the Econometrics Category.\\r\\n\\r\\nLearn from my experience at large Corporates on how they Make Millions of Dollars out of Data through #analytics.\\r\\n- Read 5 Real Business Success Stories from the industry and a step-by-step guide to an innovative approach to creating incremental value through advanced analytics techniques. /n/r Hi All - I\\'m looking for someone with expertise in machine learning and NLP.  The initial conversation would be around a contractual role and can turn into a equity holding/ full time role. /n/r Hello Experts,\\r\\nI am working on some challenge:\\r\\nMy data is taken from Games companies (online games/casinos/lotto etc.)\\r\\nI have a lot of data - transactions data, user data, tons of columns.\\r\\n\\r\\nPlease help me to understand which dashboards/charts/analysis should I produce?\\r\\nAlso I need machine learning algorithms to be applied to predict user behavior and actions.\\r\\n\\r\\nPlease advise,\\r\\nSteve /n/r What is your vision and thought on data visualisation? /n/r Making Money Out of Data: The book contains five business usecases from 5 industries and explains standardised analytics process to successfully deliver analytics / data science project to create value for an organisation. #Money #Analytics #BigData #AI #DataScience #ArtificalIntelligence /n/r Making Money Out of Data: The book contains five business usecases from 5 industries and explains standardised analytics process to successfully deliver analytics / data science project to create value for an organisation. #Money #Analytics #BigData #AI #DataScience #ArtificalIntelligence /n/r Keep your world clean and green.\\r\\nSave trees, Save the environment!!\\r\\nClean city, Green city!!\\r\\n#Happy_world_environment_day /n/r Hi All. I am a programmer and my plan is to create an educational system that will predict student outcome based on the student response time during recitation. My problem is, I don\\'t have knowledge in different classifiers. But here\\'s my different attributes to consider:\\r\\nGender M/F\\r\\nUnitsEnrolledPercentage 50%, 13%, etc\\r\\nTravelHours 1.5, 2, etc\\r\\nResponseTime 43%, 80%, etc\\r\\nResponseResult Correct/Wrong\\r\\nSleepHours 8, 7, 5, etc\\r\\n\\r\\nPredicted outcome: passed, failed\\r\\n\\r\\nIs ID3 capable of handling these data to make predictions?\\r\\n\\r\\nThank you in advance. /n/r can we generalize no free lunch theorem for machine learning algorithms? in a sense that there is no best ML method context independently? /n/r I am co-organizing the CIKM 2017 workshop on \"Data & Algorithm Bias\" in Singapore, together with Ciro Cattuto, Leo Ferres, Ricardo Baeza-Yates, Daniela Paolotti and Jeanna Matthews. The deadline is July 2nd. More information at: dab.udd.cl. /n/r Is ID3 optimal or suboptimal in sense of feature-by-feature maximization of mutual-info wrt to labels over all kinds of decision trees?\\r\\n(to make myself clear, maybe we can use dynamic programing search methods like beam-search to choose order of feature selections in ID3 to improve final mutual info score. do you confirm?) /n/r http://www.kdnuggets.com/2017/05/intro-mxnet-python-api.html\\r\\nThis post outlines an entire 6-part tutorial series on the MXNet deep learning library and its Python API. In-depth and descriptive, this is a great guide for anyone looking to start leveraging this powerful neural network library. /n/r Data Science: an FB group focused on machine learning in economics and finance\\r\\n\\r\\nhttps://www.facebook.com/groups/bigmefi/ /n/r I am pleased to announce Data Science Phd Positions and Scholarships in Applied Economics and Management (AEM), at the University of Bergamo and Pavia.\\r\\n***The deadline to apply is the 15th of june, 2017***\\r\\nApplications on-line at:\\r\\nhttps://www.unibg.it/\\x85/bando-di-concorso-lammissione-ai-cor\\x85\\r\\n(Read the ENG files only) /n/r http://datastructures.conferenceseries.com/\\r\\n\\r\\nData Structures-2017 conference is here to share the knowlege about Ransomware intrested speakers can participate in this prestegious conference and share their views about Ransomeware.\\r\\n\\r\\nsubmit your abstracts at:http://datastructures.conferenceseries.com/abstract-submission.php /n/r Identifying type of sports from many sport events. What will be the approach? /n/r Data Science and Analytics related discussion, trends, activities, opportunities and information can be found on following page... https://www.facebook.com/groups/1501661686712546/ /n/r Guys any oNe any idea..!! about the Error in ANN\\'s /n/r Hey Everyone !!\\r\\n\\r\\nIs there a machine learning approach to extract key data from pdfs?\\r\\nwith key data I mean - let\\'s say price (if it contains)(price of anything - airway/railway/bus/hotel tickets...etc, phone bill/electricity bill/house bill)?\\r\\nI could only think of regex..nothing else...Please suggest any better approach?\\r\\n\\r\\nThanks /n/r Data Science and  Analytics related discussion, trends, activities, opportunities and information can be found on following page... https://www.facebook.com/groups/1501661686712546/ /n/r Is there any PDF or E-book which is helpful for beginners to learn Statistical concepts such as Regression , Clustering etc. /n/r MOL2NET: FROM MOLECULES TO NETWORKS!!!\\r\\nPublic group (>10 000 members): https://www.facebook.com/groups/chembioinfo.networks/ Tags: #Science (#Multidisciplinary), #Chemistry, #Computational Science, #Data #Analysis, #Bioinformatics, #Networks, #Nanotechnology, #Biotechnology, etc. The group is also the host of  #MOL2NET International Conference Series on Multidisciplinary Sciences, #MDPI, #Sciforum, #Switzerland, UPV/EHU #Basque Country, Spain. Page: http://sciforum.net/conference/mol2net-03, RGate: http://bit.do/rg_mol2net, Assoc Workshops: #Miami, #USA, #Bilbao, #Spain, #Soochow, #China, etc. Online Submission: You have to login or sign in at http://sciforum.net/login to submit your short communication (1-2 pages recommended), proceeding abstract, slide presentation, or video conference. Submissions to general sections and workshops will remain open from now on until 30 November 2017. /n/r Comenzamos despu?s de Semana Santa un nuevo #curso de #BigData con MongoDB y Hadoop presencial en #Madrid y #Online para particulares y empresas el pr?ximo lunes 17 de abril ;) \\r\\n\\r\\nhttps://culture-lab.es/curso/curso-de-introduccion-a-big-data-con-mongodb-y-hadoop/ /n/r Hi anyone is having code for regression or classification for students placement analysis? /n/r Hello, everyone, I need this book\"CompTIA Linux+ / LPIC-1 Cert Guide (Exams LX0-103 & LX0-104/101-400 & 102-400),Thank you /n/r Analytics related latest trends and information can be found on following page.. https://www.facebook.com/groups/1501661686712546/ /n/r Do you think the rise of sentient machines is held back more by the hardware requirements or the software/knowledge engineering necessary to simulate something like consciousness? /n/r Hone your skills with #Hadoop #onlinetraining at Glory It Technologies Pvt. Ltd. Hadoop is an open-source framework for distributed storage and processing of the #bigdata in a distributed computing environment across clusters of computers by using simple programming models.\\r\\n\\r\\nHadoop services are used for data storage, data access, data processing, data governance, security, and operations. And it is widely used across industries like finance, government, media & entertainment, information services, health-care, and much more.\\r\\n\\r\\nTo know more, visit http://www.gloryittechnologies.com/Bigdata-Online-Training.html /n/r Tech Trend 2017 is a unique event showcasing the emerging technologies, providing participants with an opportunity to gain access to the most innovative, and leading companies, professionals, entrepreneurs, and academics in the world. Tech Trend 2017 features some of the leading companies, and CEOs in the world that are consistently pushing the boundaries of innovation. Attendees will also gain first-hand insights of the future and understand the technologies disrupting businesses and driving the new economy.\\r\\n\\r\\nTech Trend 2017 will cover a wider-range of topics like Internet of Things, Machine Learning, Cyber Security, Artificial Intelligence, Big Data etc. We will be expecting at least 300 attendees for the event made up of leading professionals, executives, decision makers, academics, and entrepreneurs from around the region.\\r\\n\\r\\nVisit https://lnkd.in/fc-yG99 to complete your registration\\r\\n\\r\\n#TechTrend #CASUGOL #BigData #HRAnalytics #IoT #SmartCity #MachineLearning #Conference /n/r Tech Trend 2017 is a unique event showcasing the emerging technologies, providing participants with an opportunity to gain access to the most innovative, and leading companies, professionals, entrepreneurs, and academics in the world. Tech Trend 2017 features some of the leading companies, and CEOs in the world that are consistently pushing the boundaries of innovation. Attendees will also gain first-hand insights of the future and understand the technologies disrupting businesses and driving the new economy.\\r\\n\\r\\nTech Trend 2017 will cover a wider-range of topics like Internet of Things, Machine Learning, Cyber Security, Artificial Intelligence, Big Data etc. We will be expecting at least 300 attendees for the event made up of leading professionals, executives, decision makers, academics, and entrepreneurs from around the region.\\r\\n\\r\\nVisit https://lnkd.in/fc-yG99 to complete your registration\\r\\n\\r\\n#TechTrend #CASUGOL #BigData #HRAnalytics #IoT #SmartCity #MachineLearning #Conference /n/r Hello Experts,\\r\\nI have a question for you. Consider you have a business of arbitrage - If I order some product from company A, and can cancel it (free until date T), the price fluctuates and I can reorder on a lower price.\\r\\nHow to find the optimal algorithm for the optimal price of the item/order where I rebook and save money for my customer (and get a fee for the service). /n/r any idea for how to apply Business Intelligence for logistics?? /n/r HI Everyone,\\r\\nHow can we implement pl/sql cursor functionality in Hive/Pig/Spark. If anybody worked on same please provide your inputs. Thanks In Advance. /n/r How the Logistic Regression Model Works in Machine Learning \\r\\n\\r\\nhttp://bit.ly/2oslRVi  \\r\\n\\r\\n#MachineLearning #DataScience #Python /n/r Dear all, \\r\\nI have a question regarding the Association analysis in R. Does anyone have examples of the association analysis for the store or ecommerce, where the input data are stored in lines for each product, so one transaction might have more then one  line with product ID or product supplier. The number of products in more than 10000.\\r\\nThanks in advance. /n/r is here any one who work with scala and spark ? /n/r ML algorithms CheatSheet :) /n/r Understanding, generalisation, and transfer learning in deep neural networks http://bit.ly/2nCFmas\\r\\n\\r\\n#DeepLearning #NeuralNetworks #DataScience #MachineLearning /n/r Is it possible to combine on-line analytical processing (OLAP) with data mining /n/r special type of programming language used to provides instructions to the monitor is \\r\\na) FPL. b) SML. c) DML. d) jcl\\r\\nwhat is the correct answer? /n/r Develop key #presentation skills and learn how to present your ideas with #TLSU.\\r\\nJoin us for High Impact Presentation Skills Program in Banglore.\\r\\nFor more information, mail us at training@teamleaseuniversity.ac.in /n/r Develop effective #presentation skills with High Imapct Presentation Skills Program in Banglore at #TLSU.\\r\\nFor more information, mail us at training@teamleaseuniversity.ac.in /n/r Our experienced trainers facilitate excellent Soft Skills #Training for Graduates & #Professionals. \\r\\nEnrol Now!\\r\\nFor more information, visit us at form.teamleasetraining.com /n/r Opportunities & risks of open & syndicated data #GartnerDA #opendata #syndicateddata #data #datascience #bigdata #infopreneurship #business /n/r Ducati & Accenture optimize racing performance using data science #GartnerDA #IoT #analytics #datascience #machinelearning #bigdata #tech /n/r Hadoop & Spark: Opportunities and Risks #GartnerDA #datascience #spark #hadoop #datamanagement #bigdata #machinelearning #tech #business /n/r ATSS-IICMR, NIGDI, PUNE !!!\\r\\n(Approved by AICTE & Affiliated to Savitribai Phule Pune University,\\r\\nAccredited by NAAC, Recognized by Gov. of Maharashtra).\\r\\n\\r\\n******Upcoming Events******\\r\\n\\r\\n******Industry Visit To *******\\r\\n\\r\\n******Mazak India*******\\r\\n\\r\\non 23rd March, 2017 /n/r A digital society is emerging #GartnerDA #datascience #digitalsociety #society #tech #culture #sociology #analytics #bigdata #digital #data /n/r Hello Everyone,  \\r\\n\\r\\nI really need your opinion about HR Analytics. You all have speciality of your own and i request you to share your opinions on the noted below points.\\r\\n\\r\\n1. How we can use NPS Score in the field of HR Analytics?\\r\\n\\r\\n2. Most of the Companies used bell curves and its significance for appraisal process in the past but now companies are rejecting it. Why?\\r\\n\\r\\nIt is my request, please share your views about the above mentioned points.\\r\\n\\r\\nRegards,\\r\\nAtul Kumar /n/r Driving profitability with further analytics #GartnerDA #datascience #analytics #bigdata #IoT #businessintelligence #business #tech /n/r Using data lakes from pointless to profitable #GartnerDA #datascience #datalake #bigdata #analytics #businessintelligence #business #tech /n/r Hi,\\r\\n\\r\\nWho can help me with some game analysis task? /n/r Did you know!\\r\\n#android #fact /n/r Top 36 #Tableau #Interview Questions and Answers For 2017 https://goo.gl/ahbyRR /n/r Visual Analytics with SSRS & SSAS on iOS Android Windows 10 \\r\\n? https://goo.gl/jWA29w\\r\\n==========\\r\\nVisual Analytics with SSAS and SSRS on iOS Android and Windows 10 is a  course in which a student having no experience in analytics, reporting and visualizations would be trained step by step from basics. The intention of this course is to empower students with the knowledge of developing mobile dashboards for senior leadership of an organization like CEO, COO etc.\\r\\nThese skills can potentially yield salaries of $85 \\x96 $150k based only on your experience of data visualizations and mobile reporting techniques using Microsoft BI Tools using the latest version of SQL Server Analysis Services and SQL Server Reporting Services 2016.\\r\\nCourse includes practical hands-on exercises as well as theoretical coverage of key concepts. Anyone pursuing this course would be able to clearly understand about Microsoft Business Intelligence Architecture, Microsoft Mobile Reporting Architecture and understand how SSAS fits in this architecture along with other tools like Power BI. /n/r Any one knows about a big data analytics (data lakes, hadoop, spark ...) architecture case study in banking? Or any reference ?\\r\\nThanks /n/r is there anyone working on Big data testing.. /n/r Colors for Data Science A-Z: Data Visualization Color Theory \\r\\n================\\r\\nA fun and entertaining journey thorough colour theory and basic colour knowledge to help you create effective Data Science visualisations.\\r\\nSo why is this an important course for a Data Scientist?\\r\\nThink about this\\x85\\r\\nYou\\'ve just completed an incredible Analytics project.\\r\\nYou did the data prep, the modeling, and now you have the insights.\\r\\nBut we all know that this is not the end\\x85\\r\\nYou still need to present your findings to your manager, client or even a large audience....\\r\\n? https://goo.gl/xaEmN3 /n/r I have a simpler job this time.  I want to create a simple HTML page where people can login.  Once they login there will be two tasks: 1) I want to have an iframe through which people can do certain tasks. 2) I want the iframe to be able to get the user\\'s login ID so we can track what tasks the person did.   Ping me for more information. /n/r The Ultimate Hands-On Hadoop - Tame your Big Data! \\r\\n---------------\\r\\nThis course is comprehensive, covering over 25 different technologies in over 14 hours of video lectures. It\\'s filled with hands-on activities and exercises, so you get some real experience in using Hadoop \\x96 it\\'s not just theory.\\r\\nYou\\'ll find a range of activities in this course for people at every level. If you\\'re a project manager who just wants to learn the buzzwords, there are web UI\\'s for many of the activities in the course that require no programming knowledge. If you\\'re comfortable with command lines, we\\'ll show you how to work with them too. And if you\\'re a programmer, I\\'ll challenge you with writing real scripts on a Hadoop system using Scala, Pig Latin, and Python....\\r\\n? https://goo.gl/460Tsz /n/r Hello Scraping Experts!  I need help pulling data by doing a google search on a name + city and pulling information that shows up on page search.  Customer willing to pay handsomely.  Please add me as friend or message me to discuss more. /n/r when researcher have 1 IV and 2 DVs which test should be applied to interpret data? /n/r Apache Spark 2.0 with Scala - Hands On with Big Data! \\r\\n-------------\\r\\nNew! Updated for Spark 2.0.0.\\r\\n\"Big data\" analysis is a hot and highly valuable skill \\x96 and this course will teach you the hottest technology in big data: Apache Spark. Employers including Amazon, EBay, NASA JPL, and Yahoo all use Spark to quickly extract meaning from massive data sets across a fault-tolerant Hadoop cluster. You\\'ll learn those same techniques, using your own Windows system right at home. It\\'s easier than you might think, and you\\'ll be learning from an ex-engineer and senior manager from Amazon and IMDb.\\r\\nSpark works best when using the Scala programming language, and this course includes a crash-course in Scala to get you up to speed quickly. For those more familiar with Python however, a Python version of this class is also available: \"Taming Big Data with Apache Spark and Python \\x96 Hands On\".\\r\\n? https://goo.gl/4ccWk8 /n/r This online course was specifically designed to help you understand Complex Architectures of Hadoop and its components, guide you in the right direction to start with, and quickly start working with Hadoop and its components.\\r\\n\\r\\nTraditional Big Data courses can cost $300 or more, but since Udemy courses are online and on-demand, you\\'ll get the same world-class instruction for only $10! Plus, you\\'ll get lifetime access to your content, can learn at your own pace on any device, and you\\'re protected by a 100% money-back guarantee.\\r\\n\\r\\nJoin today to avail the offer..!!\\r\\n\\r\\nhttps://www.udemy.com/big-data-and-hadoop-for-beginners/?couponCode=BIGDATA10 /n/r Data Science, Apache Spark & Python: Analysiere echte Daten! \\r\\n-----------------\\r\\nAuswertungen von \"Big Data\" werden immer wichtiger, Experten werden h?nderingend gesucht. Du lernst in diesem Kurs die hei?este Technologie, Apache Spark kennen. Dieses wird bereits von unz?hligen Unternehmen verwendet, darunter Amazon, eBay, Groupon, TripAdvisor! Lerne jetzt Apache Spark \\x96 ganz bequem auf deinem eigenen Computer...........\\r\\n? https://goo.gl/R8EJZW /n/r Machine Learning for Data Science \\r\\n-------------\\r\\nThank you all for the huge response to this emerging course!  We are delighted to have over 2300 students in over 102 different countries and for the overwhelmingly positive and thoughtful reviews.  It\\'s such a privilege to share this important topic with everyday people in a clear and understandable way.Unlock the secrets of understanding Machine Learning for Data Science! In this introductory course, the \"Backyard Data Scientist\" will guide you through wilderness of Machine Learning for Data Science.  Accessible to everyone, this introductory course not only explains Machine Learning, but where it fits in the \"techno sphere around us\", why it\\'s important now, and how it will dramatically change our world today and for days to come.\\r\\n? https://goo.gl/69j6wf /n/r Hello everyone - I have a company looking for help with pulling data from company websites.  They have a list of 8000 websites and want to pull the name of the company, the address, phone number, products and prices.  Totally understood you cannot pull all this information at once --> please let me know if you can help.  Add me or message me to chat. /n/r What is the best regressor for predicting logarithmic relationship between dependent and independent? Python packages please.... /n/r I have got a problem. I have terms like BLUE, RED, GREEN, BENZ, AUDI, BMW, ROLEX, TIMEX, TITAN . Now how to cluster these terms in such a way that colors should become a cluster, Car companies should become a cluster and Watch brands should become one. Can anyone help me doing this? I have to implement it in my project. I am using Java. /n/r Find the MongoDB Certification Preparation Guide makes the process simpler and easy to prepare for a certification exam. https://goo.gl/ohjUJD /n/r ?? Learn Big Data: The Hadoop Ecosystem Masterclass\\r\\n>> bit.ly/2m3Yqgx\\r\\nThis Big Data Hadoop - The Complete Course covers the topics from the basic level of beginner till the advanced professional levels required for Big Data Hadoop Certification. \\r\\nIt\\'s a must have course for prospective Big Data experts. \\r\\nThe course covers #Hadoop, #HDFS, #Map_Reduce, #YARN, #Apache_Hive, #PIG, #Impala, #Scoop and #ZooKeeper /n/r Does anyone know how to access edx BerkeleyX\\'s Stat2 course as its showing Enrollment is closed. /n/r Hey guys,\\r\\n\\r\\nNo post de hoje abordamos o uso da declara??o SELECT no MySQL e suas v?rias formas de implementa??o.\\r\\nhttp://programetododia.com.br/#!/mysql-select\\r\\nEnt?o corre l?, d? uma olhada, e aprenda um pouco mais sobre esse banco de dados sensacional ! \\r\\nAproveite e curta a nossa p?gina no face ! ( se ainda n?o tiver curtido )\\r\\n\\r\\nSo, see you guys soon e Programe Todo Dia ! /n/r Apache Spark 2.0 with Scala - Hands On with #Big #Data!\\r\\n\\r\\n \"Big data\" analysis is a hot and highly valuable skill \\x96 and this course will teach you the hottest technology in big data: Apache Spark. Employers including Amazon, EBay, NASA JPL, and Yahoo all use Spark to quickly extract meaning from massive data sets across a fault-tolerant Hadoop cluster. You\\'ll learn those same techniques, using your own Windows system right at home....\\r\\n? https://goo.gl/4ccWk8 /n/r Hey guys anyone worked on python and R with Visual studio need some help /n/r Hey guys,\\r\\n\\r\\nHoje no Programe Todo Dia, te daremos alguns bons motivos para voc? aprender / usar MySQL na sua aplica??o.\\r\\nEnt?o ? s? clicar e dar uma conferida !\\r\\nhttp://programetododia.com.br/#!/why-mysql\\r\\nAproveite e curta a nossa p?gina no face ! ( se ainda n?o tiver curtido )\\r\\n\\r\\nSo, see you guys soon e Programe Todo Dia ! /n/r IIHT Viman nagar is organizing walk-in drive for its client Cognizant Technologies. Please find the details below:\\r\\nWalk-in Date: 3rd Week of March (Date to be announced)\\r\\nEligibility: 2016 pass-out Engg - Electronics / Computers / IT\\r\\nVenue: Pune (Location to be announced)\\r\\nRegister now at https://goo.gl/3B6pzQ /n/r IIHT Viman nagar is organizing walk-in drive for its client Cognizant Technologies. Please find the details below:\\r\\nWalk-in Date: 3rd Week of March (Date to be announced)\\r\\nEligibility: 2016 pass-out Engg - Electronics / Computers / IT\\r\\nVenue: Pune (Location to be announced)\\r\\nRegister now at https://goo.gl/3B6pzQ /n/r Big Data Online Course: Learn Hadoop, HDFS, MapReduce, Hive & Pig. Enroll today at just $0.99.\\r\\n#PremiumMembership\\r\\nCourse Link: http://skl.sh/2lnjHS1 /n/r Hi All - I have about 200 company links for LinkedIn and trying to get the description off of those pages...any idea if there\\'s a fast way to do this i.e. scrape? I have the links already. /n/r In todays (24th Feb.,2017) Sakal news paper, article on MCA course by Dr Deepali Sawai, Director IICMR-MCA , Nigdi, Pune\\r\\n\\r\\nFor IICMR, MCA DTE Code (MC-6154) 100% Placement Assistance, International Certification Provided...\\r\\n\\r\\nFor more details call : 9921000870 / 9822951262\\r\\ndo visit - www.iicmr.org /n/r Hello Everyone,\\r\\nI\\'m looking for an online course for NLP, I remember one of the folks here share a link for a very good course for online program from India , but I couldn\\'t remember what was the link, any suggestions/ideas? /n/r Hello Everyone - I\\'m looking for some customer service calls to use for some text analysis - looking for calls with USA customers...anyone have access to any? Happy to pay for the right data. /n/r Can anyone help me in SEM analysis /n/r With #Mavericks, you get more a desktop application. You get a solution that\\'s unrivaled in the industry. Top quality, Integrated, Customized.\\r\\nA perfect fit for you and your business. /n/r Hello experts,\\r\\nCan you please explain to me what is 2D histogram?\\r\\nI use python for this and want to understand what are the values the bins represent. /n/r Greetings from ATSS-IICMR, NIGDI, PUNE !!!\\r\\n\\r\\nWe Make You Future Ready.... Join MCA, a guaranteed launching pad for innumerable career options.\\r\\n\\r\\nFor MCA DTE Code (MC-6154) 100% Placement Assistance, International Certification Provided...\\r\\n\\r\\nFor more details call : 9921000870 / 9822951262\\r\\nwww.iicmr.org /n/r There are multiple openings for engineering roles at all levels at NeoGrowth Credit Pvt. Ltd. \\r\\nInterested candidates can share their resume at aswanimanish92@gmail.com for referral.\\r\\nWork Location - Bangalore\\r\\nSalary :- Best in the market\\r\\nColleges : IIT/NIT/IIIT and other Tier 1 colleges\\r\\nOpen position:- \\r\\nSoftware Development Engineer I/II/III \\x96minimum 1.5 year \\r\\nPlease share your resume only if you are interested in the above profile and have the required work ex.\\r\\nNote: While replying change the subject based on the role and work ex, and preferably provide CV in pdf format. Feel free to forward to any other relative/friend of yours whose profile would match\\r\\nThe turnaround would be quick for best fit candidates. /n/r Greetings from ATSS-IICMR, NIGDI, PUNE !!!\\r\\n\\r\\nIBM\\'s Academic Initiative Event(Watson Internet of Things) for B.Sc., BCS, BCA (All Stream) 2017 passout is expected in the 4th week of Feb. 2017 @ ATSS-IICMR, Nigdi, Pune.\\r\\n\\r\\nInterested & Eligible students kindly register your name @ below link as earlier...\\r\\n\\r\\nREGISTRATION LINK:\\r\\nhttps://goo.gl/forms/oVQRj7sEOtgu1W002\\r\\n\\r\\nWill send confirmation mail to the registered students only.\\r\\n\\r\\nWarm wishes on behalf of ATSS-IICMR, NIGDI, PUNE!!!\\r\\n\\r\\n-- \\r\\n\\r\\nBest Regards,\\r\\n\\r\\nMahesh  Deshmukh\\r\\nTraining & Placement Officer,\\r\\nATSS-IICMR,  Nigdi Pradhikaran, Pune 411 011\\r\\nLandline : +91-020-2765 7648 | Cell No. : +91-9923234570\\r\\nMail ID: deshmukh.mahesh11@gmail.com  I  mahesh.deshmukh@iicmr.org\\r\\nLinkedIn : https://www.linkedin.com/pub/mahesh-deshmukh/54/72b/113\\r\\nwww.iicmr.org /n/r Big Data is making Big buzz today due to Big opportunities with Big Salary\\r\\nWant to understand how Big Data is changing the industry and how you can make a career in it? Grab the opportunity to learn the Data analytics it from industry expert. Register at https://goo.gl/NwncY0\\r\\n\\r\\nLike and Share with your family, friends, colleagues & Acquiantances who may be benefited out of it. /n/r Big Data is making Big buzz today due to Big opportunities with Big Salary\\r\\nWant to understand how Big Data is changing the industry and how you can make a career in it? Grab the opportunity to learn the Data analytics it from industry expert. Register at https://goo.gl/NwncY0\\r\\nLike and Share with your family, friends, colleagues & Acquiantances who may be benefited out of it. /n/r Greetings from ATSS-IICMR, NIGDI, PUNE !!!\\r\\n\\r\\nWe Make You Future Ready.... Join MCA, a guaranteed launching pad for innumerable career options.\\r\\n\\r\\nFor MCA DTE Code (MC-6154) 100% Placement Assistance, International Certification Provided...\\r\\n\\r\\nFor more details call : 9921000870 / 9822951262\\r\\ndo visit - www.iicmr.org /n/r Hans Rosling died.  :-(\\r\\nSuch a wonderful inspiration!\\r\\nMay he rest in peace.\\r\\n\\r\\n#datascience #statistics #math #sciencenews #science #educator /n/r Dear All, \\r\\nI need small assistance on following issue related to KPI\\'s scorecard:\\r\\n1) How to calculate or define KPI weightage ?\\r\\n2) After getting weightage, how to calculate overall score with respect to KPI Actual value, KPI Target value and weightage /n/r We Design & Develop App\\'s On\\r\\nLatest Technology  In Pocket-Friendly Cost For All Your Business Needs /n/r The Learning Zone, Pune invites you to take that first step on the road to a great career by learning how being a student is different than being an Executive and what needs to be practiced today to become a Successful Professional Tomorrow?\\r\\nReserve Your Seat for February 12, 2017 \\r\\nINTRODUCING 1-DAY MAGICAL TRAINING MODULE CAMPUS TO CORPORATE JOURNEY by Renowned Senior Mindset & Behaviour Trainer, Mr. Ramesh Sood , Master Practitioner NLP\\r\\nReserve your seats at https://goo.gl/6sIFK6 /n/r The Learning Zone, Pune invites you to take that first step on the road to a great career by learning how being a student is different than being an Executive and what needs to be practiced today to become a Successful Professional Tomorrow?\\r\\n\\r\\nReserve Your Seat for February 12, 2017 \\r\\n\\r\\nIntroducing 1-DAY MAGICAL Training module - CAMPUS TO CORPORATE JOURNEY by Renowned Senior Mindset & Behaviour Trainer, Mr. Ramesh Sood , Master Practitioner NLP\\r\\n\\r\\nReserve your seats at https://goo.gl/6sIFK6 /n/r Here is a head() of my DataFrame df:\\r\\n\\r\\n                     Temperature  DewPoint  Pressure\\r\\nDate                                                \\r\\n2010-01-01 00:00:00         46.2      37.5       1.0\\r\\n2010-01-01 01:00:00         44.6      37.1       1.0\\r\\n2010-01-01 02:00:00         44.1      36.9       1.0\\r\\n2010-01-01 03:00:00         43.8      36.9       1.0\\r\\n2010-01-01 04:00:00         43.5      36.8       1.0\\r\\nI want to select from August 1 to August 15 2010 and display only the Temperature column.\\r\\n\\r\\nWhat I am trying to do is: df.loc[[\\'2010-08-01\\',\\'2010-08-15\\'],\\'Temperature\\']\\r\\n\\r\\nBut this is throwing me an error.\\r\\n\\r\\nGenerally speaking what I want to learn is how, using loc method I can easily take a range of row i to row k and column j to p and show it in dataframe using loc method:\\r\\n\\r\\ndf.loc[[i:k],[j:p]] /n/r Dear all,\\r\\nI have started working on my Final Year Project that is Text Analysis to detect crime related articles on web . For this I have to classify web documents. But I am not sure from where to start ? I am beginner in text mining and classification.  I want to learn text mining in JAVA from beginning.Please suggest me any good book or tutorials at the beginner level.\\r\\nI have already studied Machine Learning and Artificial Intelligence courses. I have also gone through books like Practical Machine Learning Tools and Techniques, 2nd Edition (The Morgan Kaufmann Series in Data Management Systems) but honestly they did not make much sense to me. i would like a book or a resource or tutorials that has\\r\\ninstructions from ground zero and implements Java.\\r\\nAlso which open source framework would be good to implement here.\\r\\nI am using netbeans 8.2 IDE therefore if you are suggesting me any open source then please tell me how do i integrate it with netbeans too. I would like to design front end and backend programming in Java. \\r\\nThanks /n/r Big Data is making Big buzz today due to Big opportunities with Big Salary\\r\\nWant to understand how Big Data is changing the industry and how you can make a career in it? Grab the opportunity to learn the Data analytics it from industry expert. Register at https://goo.gl/NwncY0\\r\\n\\r\\nLike and Share with your family, friends, colleagues & Acquiantances who may be benefited out of it.\\r\\n\\r\\n#IIHTVimannagar, #IIHT, #BigData, #Hadoop, #DataScience /n/r Is there any site which enables learning R for free? /n/r Hello experts,\\r\\n\\r\\nI have graduated with B.Sc. Applied Math and M.Sc. Financial math.\\r\\nWorking as business analyst mostly configuring and testing a big financial program.\\r\\n\\r\\nMy goal is to be data scientist.\\r\\nPlease comment/suggest what do you think about my way of turning to be data scientist:\\r\\n1) Learning all the courses in data camp system.\\r\\n2) Completing 2-3 data science, machine learning and models specializations in Coursera.\\r\\n3) Completing the MIT specialization in data science.\\r\\n\\r\\nHere and there I will listen and do what people do in R/Python on YouTube.\\r\\n\\r\\nMy pleasure,\\r\\nSteve /n/r Heard about Cloud Computing but do you really understand it? Cloud Computing is future of present day network and system administration technologies and the change is already in making a big impact...Salaries are sky -rocketing. Attend our seminar to gain more understanding.\\r\\n\\r\\nKey Take Away:\\r\\n1. Understanding of Cloud Computing\\r\\n2. Public / Private / Hybrid Cloud\\r\\n3. PaaS / IaaS\\r\\n4. AWS / Azure / VMWare etc.\\r\\n\\r\\nLimited seats available. Ensure to reserve your seats now at https://goo.gl/3oIXF0\\r\\n\\r\\nLike and share with your family, friends, colleagues and acquaintances who may be benefited out of it.\\r\\n\\r\\n#IIHTVimannagar, #IIHT, #CloudComputing, #AWS, #VMware, #Azure /n/r Happy Lunar New Year!\\r\\n\\r\\nEnroll for the upcoming CASUGOL Internet of Things (IoT) MasterClass with the promo code below and get an EXCLUSIVE discount!\\r\\n\\r\\nPromotion Code: CMCNY2017\\r\\n\\r\\nSee you in Class! /n/r So People call it over-fitting.. Here you go Computer Vision. #EndOfWorld /n/r It\\'s time. Time for a new adventure, time to broaden your horizon. Today we\\'re launching BaseCamp\\'s new edition: BaseCamp Data & Travel! This summer we host the first batch in Medellin, Colombia. New location, new format, more adventure! Explore Colombia with us, step off the beaten path and find your own. Applications are open now: www.basecamp.ai/data-and-travel #futureisdata /n/r Data Science and Machine Learning with Python - Hands On!\\r\\n4.5 (2,910 ratings) 21,605 students enrolled\\r\\nhttps://goo.gl/BAwQIx\\r\\nHeavy Discount $200 >> $15\\r\\n\\r\\n2 Days Left\\r\\nIncludes:\\r\\n9 hours on-demand video\\r\\n2 Articles\\r\\nFull lifetime access\\r\\nAccess on mobile and TV\\r\\nCertificate of Completion\\r\\n#data #iot #analytics #ai #bigdata #datscience #machinelearniing #dalalearning #dataviz #rstats /n/r Data Science and Machine Learning with Python - Hands On!\\r\\n4.5 (2,910 ratings) 21,605 students enrolled\\r\\nhttps://goo.gl/BAwQIx\\r\\nHeavy Discount $200 >> $15\\r\\n\\r\\n2 Days Left\\r\\nIncludes:\\r\\n9 hours on-demand video\\r\\n2 Articles\\r\\nFull lifetime access\\r\\nAccess on mobile and TV\\r\\nCertificate of Completion\\r\\n#data #iot #analytics #ai #bigdata #datscience #machinelearniing #dalalearning #dataviz #rstats /n/r Profusion Curious Club: Marketing trends & data science trends for 2017 with Michael Brennan #marketing #datascience #2017 profusion /n/r Dear colleagues, your are welcome to joint our group. https://www.facebook.com/groups/chembioinfo.networks/ and participate on MOL2NET Conference Series on Multidisciplinary Sciences, http://sciforum.net/conference/mol2net-02 /n/r Iiht Viman Nagar wishes you a happy Makar Sankranti, Lohri, Pongal and Bihu!\\r\\n\\r\\nAre you looking for a career in System Administration or Networking? Get an insight from an industry expert and experienced trainer. Attend our free demo batch. Register at https://goo.gl/3oIXF0 \\r\\nCall us @ 7720040531 to know more.\\r\\n\\r\\nLike and share with your family, friends, colleagues and acquitances who may be benefitted out of it.\\r\\n\\r\\n#IIHT, #IIHTVimannagar, #MCSA, #Networking, #MicrosoftCertification /n/r Hi All - Our team is publishing an article related to data science  which will be ~ two pages and looking for a guest writer to help contribute about a page.  The goal is to showcase interesting data science topics coming directly from a person in the industry.  Please message me directly if interested.\\r\\n\\r\\nWe\\'re happy to cite you and your LinkedIn profile / website in our article.  Our last article had ~ 15K views. /n/r Contact for freelancing projects and online/offline training in following technologies. Follow the link below for more information.\\r\\n-MATLAB\\r\\n-SAS\\r\\n-MS Excel\\r\\nEmail: rahulkashyap@geekycody.com\\r\\nPhone: +91-9717480811\\r\\nwww.geekycody.com /n/r Science matters AI & Machine Learning #RSScienceMatters #datascience #AI #statistics #machinelearning #artificialintelligence #science #tech /n/r [Reminder - #CallForStartup] Hi, I\\'m here to remind you all that Calls for #DataDriven2017 (-10 days) and #Codemotion Rome & Amsterdam (-30 days) are still open. Hurry up!\\r\\nLink for Data Driven Innovation: http://bit.ly/C4SDDI17\\r\\nLink for Codemotion Rome & Amsterdam: http://bit.ly/C4SCM17 /n/r Hello - all looking to see what methods members here use to train their data? Do you hire external firms (MTurk), some other process? /n/r Symbolic Machine Learning: http://buff.ly/2jaAtCn\\r\\n#datascience #machinelearning #logic #statistics #artificialintelligence #AI #tech /n/r hii guys.. i wanna learn hadoop. i do have basics in java. how and where can i start learning hadoop. thanks for ur suggestions /n/r Despite undeniable progress, the public sector is lagging behind compared to the private sector with regard to the control and operation of large volumes of data. However, new legislations and the policies of opening public data in many countries encourage modernization of IT infrastructure projects. These massive data sets are a largely untapped resource but if governments around the world really want to take advantage of Big Data, you need to ask some basic principles.\\r\\n\\r\\nhttp://ecmapping.com/2016/12/29/government-services-in-the-era-of-big-data/ /n/r R-Language or MatLab? which one is better? Any good resource for MatLab? /n/r If all variables are dependent on each other then can it predicted if yes how.? and not why? /n/r The basics of data mining with practical case studies is in this book:\\r\\n\\r\\nGiudici,P. \"Applied data mining\", Wiley, 2003 /n/r Learn about the basic concepts of data warehousing & data mining. https://play.google.com/store/apps/details?id=com.lifekart.eduquiz.datawarehouse /n/r Merry Christmas! /n/r Just created a FB page for sharing links to cool/random/open datasets on the web. I think it would be a very valuable to resource for us involved with Data Science, ML or AI. I\\'m surprised there isn\\'t a page for this already. Feel free to join if you\\'re interested! https://www.facebook.com/groups/1251805194890487/ /n/r Is there any free online practicing platform for big data technologies e.g. hadoop, hbase, hive /n/r #CallForStartup - Codemotion opens 3 calls for tech #startups: Data Driven Innovation comes back in 2017 for a new edition with even more big-data content; Codemotion conferences in Rome and Amsterdam are waiting for startups with #developer attitude!\\r\\nSubmit till 19th January for #DataDriven2017 and 7th February for #Codemotion Rome & Amsterdam!\\r\\nLink for Data Driven Innovation: http://bit.ly/C4SDDI17\\r\\nLink for Codemotion Rome & Amsterdam: http://bit.ly/C4SCM17 /n/r Kindly visit and like our #academic #writing page thank you... /n/r As an aspiring Data Scientist who is working on a Masters Degree, which 2 classes should I take out of this list: 1) Machine Learning, 2) Social Network Analysis, 3) Big Data and NoSQL, 4) Big Data and Iot? Any suggestions/advice is appreciated. Thanks /n/r i have a set of temperature humidity pressure values that are classified like rainy,cloudy,sunny etc\\r\\nconsider all these values are measured in 24 hr duration\\r\\nhow can i predict the weather using it? i mean how will i get a single prediction like chances of rain, heavy rain,clearing like that?? \\r\\n\\r\\nthanks in advance /n/r Are you serious? about your career? ?Looking for 6 months #training in Jalandhar? ?\\r\\nIf yes, then visit #AEGIS Institute - Industrial Training Center & meet our professional staff; you will feel the difference. ??\\r\\n#Online #Registration \\r\\nhttp://tinyurl.com/aegis-6-months-training-2017\\r\\nEnquire now http://www.aegiseducation.in/ or ? 9041349575 /n/r hii everyone.. as everyone knows that data science is the sexiest job in this century, i wanted to become one. But the point is i dont know from where i have to start. I am about to complete my B.Tech in CSE. please give me some suggestions( from scratch to advanced level)  so that i can start learning and begin my career as data scientist or analyst. /n/r Working in the private cloud. A series of interviews with key talent working on various cloud based technologies.   This week it\\'s Machine Learning in the private cloud http://bit.ly/YPC-KW1 /n/r Any person here in this group who already worked with rough sets for Outlier detection? /n/r I want to know how to transform text data to valid data set then how can I make data mining on it \\r\\nplease help with step by step /n/r Interested in big data in economics and finance? Join the FB group \\r\\nSTATECONOMICS /n/r Hello,\\r\\n\\r\\nI am new to Data Science and want to learn about Data Analytics.\\r\\nSo would like to know that if there is any INSTITUTE ( Online or Classroom) or any INTERNSHIP Program in Hyderabad, India which will be the best to join in it. \\r\\n\\r\\nPreferably i am looking who can teach based on real time examples rather than just focusing on theory part.\\r\\n\\r\\nYour help would be appreciable.\\r\\nThanks /n/r can someone help me... i don\\'t know python,r or any kind of data mining.\\r\\n\\r\\nnow i need to analyze a set of sensor values saved in my database.. say temp,humidity,pressure etc \\r\\nand i need to analyze this and get results like it will rain,cloudy,rainy etc\\r\\n\\r\\nhow to do this.. which is the simplest algorithm to do this in python. can anyone provide me the code for it..???\\r\\n\\r\\nsay i will be analyzing only 100 readings of each sensor at a time..ie,100 temp values,100 humidity values,100 pressure values.. and based on this i need the results.. \\r\\n\\r\\nso please suggest me the simplest algorithm and provide me a python code for the same... i don\\'t know python alsoooo...  please help me\\r\\n\\r\\nThanks in advance /n/r Kindly visit and like our #academic #writing page thank you... /n/r Enhance your #BigData Analytics, Administration skills and Drive Businesses Forward with the Hadoop Administration Training at Koenig Solutions.\\r\\nGet course details here:  https://goo.gl/K0NWUW   #Koenigsolutions /n/r Build your career in the World\\'s Most Popular and highly compensated technical role with the  #Hadoop  #Developer Certification at  #Koenig Solutions. Get course details here: https://goo.gl/tzgvRd #koenigsolutions /n/r Online/Offline training for SAS (Base and Advanced), MATLAB, Excel (Basic and Advanced). MATLAB projects also available. \\r\\nContact rahulkashyap@geekycody.com\\r\\nhttp://www.geekycody.com/ /n/r Online/Offline training for SAS (Base and Advanced), MATLAB, Excel (Basic and Advanced). MATLAB projects also available. \\r\\nContact rahulkashyap@geekycody.com\\r\\nhttp://www.geekycody.com/ /n/r Kindly visit and like our #academic #writing page thank you... /n/r Welcome to the Big Data, Data Science, Data Mining & Statistics group!\\r\\n\\r\\nWe have a very diverse group of people with many different backgrounds and skills. Take advantage of this diversity by asking questions and learning from one another. We also have two threads for learning resources, one for online courses and one for books. Have fun and learn heaps!  -  Henrik Nordmark - Data Scientist\\r\\n\\r\\nGround rules:\\r\\n\\r\\nAny posts that are advertisements will be deleted and will be grounds for banishment from the group. You can however post genuine job postings provided you are not a recruiter. \\r\\n\\r\\nIf you wish to share information about a free educational resource such as new course on Coursera, edX or Udacity that you plan to sign up for or have taken in the past that is perfectly ok. You can also share about books, blog posts and podcasts that you have found useful so long as it is not self-promotion. Use common sense when sharing with the community.\\r\\n\\r\\nWishing you all the best in your data science journey! /n/r Is the 10 fold cross validation technique is the Best methodology to evaluate a classification model with data set of 321 samples.? /n/r What is the significance of weighted average over traditional average in a classification accuracy with three class? \\r\\nWhy I should choose weighted average accuracy over average accuracy.? /n/r I absolutely love this... a tab-bar system for hybrid apps / web apps:\\r\\n\\r\\nSource here: https://github.com/ErlendEllingsen/app-tab-bar\\r\\n\\r\\nDemo here: https://erlendellingsen.github.io/app-tab-bar/live_demo/tabs.html /n/r Henrik, thanks for letting me join. -Jeff /n/r Apache #Spark: A Unified Engine for #BigData Processing\\r\\n\\r\\nhttp://cacm.acm.org/magazines/2016/11/209116-apache-spark/fulltext /n/r Wide & Deep Learning for Personalized Recommendations #datascience #deeplearning #machinelearning #AI #statistics #bigdata /n/r Future of #AI panel discusses whether jobs will be obliterated? #ArtificialIntelligence Future Advocacy /n/r Transparency of AI? #ArtificialIntelligence Future Advocacy /n/r Technological unemployment in the near future? #ArtificialIntelligence Future Advocacy #Economics /n/r An intelligent future? #ArtificialIntelligence Future Advocacy /n/r Hi all , \\r\\n\\r\\nIn many classification learning algorithms the result always be a probability of relation of one of the classes .\\r\\n\\r\\nHow can i interpret this probability in term of business needs ?\\r\\nHow can i say if this observation has these factor measure then it belongs to class X ? \\r\\n\\r\\nNeed help please ... Hopefully on SVM . /n/r Visually Linking #AI, #MachineLearning, #DeepLearning, #BigData and #DataScience\\r\\n\\r\\nhttp://bit.ly/2dobCHC ? /n/r #Creative Solution for your #Business..!\\r\\ninfo@mavericksindia.com\\r\\nhttp://www.mavericksindia.com/ /n/r Three Stages of AI\\r\\n\\r\\nhttp://buff.ly/2dFOJFm\\r\\n\\r\\n#BigData #MachineLearning #DataScience #AI /n/r Hard core SQL Server & Data Science -- free webinar today \"SQL Server R Services - Configuration and Management\" direct from the product team, 12 pm Eastern  https://attendee.gotowebinar.com/register/372645759112367875 /n/r Select from a list of 85 MATLAB projects\\r\\nhttp://geekycody.com/matlab-projects-list-on-geekycody/ /n/r I want to know a person with Economics &Staistical back ground have any prospect of learning bBig Data Hadoop Ramesh /n/r Mavericks Web Services (I) Pvt. Ltd.\\r\\n \"Made with passion driven with values\" /n/r #MOL2NET #Researchgate #Collaborative #Project, click the link and follow our project: an online scientific conference devoted to promote multidisciplinary collaborations among a network of face-to-face workshops and social networks.  https://www.researchgate.net/project/MOL2NET-Conference /n/r Hackers, this is your chance to hack for a fully paid trip to Hong Kong (and much more)!!\\r\\n\\r\\nJoin the HackTrain this 4-6 of November for an immersive hackathon where 120 developers, designers & entrepreneurs will be building websites and apps whilst on moving trains across Europe! \\r\\n\\r\\nApplications are open at http://uk.hacktrain.com\\r\\n\\r\\nIf you have any questions feel free to comment below or drop us a line on hello@hackpartners.com /n/r look at that stuff\\r\\n\\r\\nHey, \\r\\n\\r\\nLook  at this  great and exetremely interesting stuff,  it\\'s admired by so many people, just take a look <http://kivyquoho.thearcadiaschool.com/e4crb>\\r\\n\\r\\nKind regards, saxena.ankit87 /n/r Don\\'t  believe the hype: The data scientist shortage is being overblown. You just need to know where to look. http://deloitte.wsj.com/cio/2016/08/11/the-myth-of-the-data-scientist-shortage/ /n/r Pok?mon Going Going Gone /n/r CALL FOR PAPERS: MOL2NET International Conference Series on Multidisciplinar Sciences, MDPI, SciForum, Basel, Swiztherland, HQ UPV/EHU, Campus Bizkaia, Basque Country, http://sciforum.net/conference/mol2net-02\\r\\n ASSOC WORKSHOPS: The conference is online without registration, publication, or travelling costs. However, associated workshops run presentially on their organizing centers like: #IWMEDIC-04, Univ. of Coru?a, #Spain, #SUIWCS-01, Univ. of Soochow, #China, #WRSAMC2016, Univ. of Paraiba, #Brasil, etc. http://bit.do/mol2net-workshops /n/r Last call: sign up for #SpaceAppCamp at http://www.app-camp.eu/. For inspiration check ESA - European Space Agency http://bit.ly/29VJUXg. This is where #coding meets #Earthobservation. Sign up and be part of this unique event in Frascati, Italy. /n/r Last call: sign up for #SpaceAppCamp at http://www.app-camp.eu/. For inspiration check ESA - European Space Agency http://bit.ly/29VJUXg. This is where #coding meets #Earthobservation. Sign up and be part of this unique event in Frascati, Italy. /n/r Submit your idea related to #transport for the BMVI #EarthObservation Challenge for Digital Transport Applications. Exploit the potential of Copernicus in possible combination with other geo-data and develop new transport applications. Submission is open until 25 July at http://bit.ly/29Otv15 /n/r Last call for Astrosat\\'s End-to-End EO Challenge: Watch the video & submit your idea at http://bit.ly/29OdDeQ #ISS #sensors Stevenson Astrosat /n/r FREE HADOOP REGULAR BATCH starts on 20th @ 7AM by Mr.SREERAM(Data Scientist). /n/r FREE HADOOP REGULAR BATCH starts on 20th @ 7AM by Mr.SREERAM(Data Scientist). /n/r Like to be a Data Scientist?\\r\\n Attend webinar on Data Science by  Mr. Ravi Subject Matter Expert\\r\\n When: 14th July 2016, 7 A.M IST / 13th July 2016 9-30 PM EST\\r\\n High Demand & Highest Pay for Data Scientist\\r\\n Click to register  :  (https://attendee.gotowebinar.com/register/7404924216406746113 ) /n/r CALL FOR PAPERS: #MOL2NET 2016, 2nd International Conference on Multidisciplinary Sciences is accepting submissions of communications. http://sciforum.net/conference/mol2net-02\\r\\n\\r\\nONLINE & FREE OF COST: Participation, and Publication of Posters and Communications (1-2 pages) is free of cost and online with doi number asap upon acceptance, saving traveling and registration costs.\\r\\n\\r\\nASSOCIATE WORKSHOPS: The conference have multiple #online and #presential #workshops associated to it worlwide, like #IWMEDIC2016, Coru?a, Spain, #SUIWML2016, Soochow, China, #MODEC2016, Puyo, Ecuador, and #CIESA2016, Toluca, M?xico. See details: http://sciforum.net/conference/mol2net-02\\r\\n\\r\\nSUBMISSION: Submissions link shall be open from now on and until 2016-Nov-25 when the conference is planned to be officially started, and the online discussion begins, please use direct submission link: http://sciforum.net/user/submission_for_conference/129\\r\\n\\r\\nKEYWORDS: #Science (Multidisciplinar), #Chemistry (All areas), #Medicine, #Physics (Applied), #Computer sciences, #Biotechnology, #Nanotechnology, #Mathematics, #Statistics, #Data analysis, #Bionformatics, #Complex #Networks, #Systems #Biology, #Materials science, etc.\\r\\n\\r\\nMOL2NET International Conference on Multidisciplinary Sciences, \\r\\n 2015, Dec, 05\\x9615, MDPI Siforum, HQ UPV/EHU Bizkaia, \\r\\nhttp://sciforum.net/conference/mol2net-1, the first edition now officially closed; attracted Papers/abstracts: 100+, Participants: 150+, Institutions: 30+, Countries: 20+ (including committees & authors)\\r\\n\\r\\n* Participants (committees/authors) are affiliated to 30+ institutions such as: #Standford University, #Harvard Medical School, University of #Pennsylvania, University of #Minnesota, Commonwealth University of #Virginia, University of #Miami, University of #Nebraska, #EMBL-EBI #Cambridge, University of #Paris Sud, #CNAM Paris, University of #Reading, University of #Strathclyde, University of Rostock #Germany, University of Porto, University of Santiago de #Compostela, University of #Coru?a, Chinese Academy of Sciences, and Universidade Federal de Goi?s, etc. along with the HQ host institutions University of #Basque Country #UPV/EHU and #Ikerbasque, Basque Foundation for Sciences.\\r\\n\\r\\n*The countries represented include, but are not limited to, #USA, #Canada, #M?xico, and #Brasil (in #America); #UK, #France, #Germany, #Portugal, and #Spain (in #Europe) as well as #China, #India, #Japan, and #Vietnam (in #Asia).\\r\\n\\r\\nSincerely yours\\r\\nConference Chairperson\\r\\n Prof. Humberto Gonzalez-Diaz, PhD., Pharm.Lic.\\r\\n IKERBASQUE Professor of Department of Organic Chemistry II,\\r\\n University of Basque Country UPV/EHU, Campus Bizkaia /n/r Hi,Can someone please guide me about possible machine learning approaches (other than sequence labeling) for parsing free form text into p.o.box,city,state,country /n/r VACANTES DATA SCIENCE - MINERIA DATOS\\r\\n\\r\\nActualmente tenemos unas vacantes de Miner?a de Datos en un importante Banco l?der en el Sistema Financiero Colombiano.\\r\\n\\r\\nPERFIL: (solo enviar Hoja de Vida si cumple con el perfil)\\r\\n\\r\\n\\x95 Nacionalidad: Colombia\\r\\n\\x95 Profesionales de carreras relacionadas con las ciencias exactas (matem?ticas, f?sica, ingenier?a, estad?stica, sistemas, etc\\x85) con Postgrado (de preferencia en temas relacionados con Data Science y Miner?a de Datos).\\r\\n\\x95 Experiencia y conocimiento en  modelaci?n matem?tica, estad?stica y miner?a de datos (como regresi?n, redes neuronales, aprendizaje autom?tico, etc\\x85)\\r\\n\\x95 Buenas habilidades en programaci?n (Phyton, R, SAS, SPSS, Spark, Hadoop, SQL\\x85)\\r\\n\\r\\nCONDICIONES LABORALES:\\r\\n\\r\\nLugar: MEDELLIN, Colombia. \\r\\n\\r\\nEl salario fijo Anual Total es de  61 millones de pesos (incluyendo cesant?as, primas)\\r\\n \\r\\nBono anual  entre 0 y 6 salarios mensuales de acuerdo a los resultados del banco.\\r\\n\\r\\nBeneficios extralegales como cr?ditos a tasas bajas, seguros de vida y de salud, exenci?n en cuota de manejo, etc..\\r\\n\\r\\nPROCESO DE SELECCI?N:\\r\\n\\r\\n\\x95 Enviar  hoja de vida al correo:  aibarraquiceno@hotmail.com   \\r\\nPlazo: Domingo 31 de Julio del 2016 11:59 pm. \\r\\n\\r\\n\\x95 Prueba T?cnica: Si su hoja de vida clasifica recibir? un correo con las instrucciones para realizar una prueba t?cnica que consiste en el desarrollo de un modelo predictivo. \\r\\n\\r\\n\\x95 Entrevista. Los que tengan los mejores modelos se citar?n a entrevista con el gerente del ?rea y si clasifican entrar?n al proceso de selecci?n del Banco.   (puede ser por Skype si no se encuentran en Medell?n). \\r\\n\\r\\n\\x95 Decisi?n final. La idea es tener finalizado el proceso a fines del mes de Agosto 2016 para empezar labores en Septiembre. /n/r We Are Your Campus Engagement Network. /n/r The UK votes to leave the European Union: a very sad day in European and world history. #eureferendum #brexit /n/r 3 Enterprise Business Intelligence Trends That Can Benefit Your Business \\r\\n  When it comes to changes in the business-intelligence (BI) technology market, old giants of the...  Keep on reading:  3 Enterprise Business Intelligence Trends That Can Benefit Your Business /n/r A Complete Tutorial to Learn Data Science with Python from Scratch /n/r What version of MS Office are you using in your job? /n/r I want to classify the Websites into the Business Categories, the way SIC codes does (http://siccode.com/) or even better than this using machine learning. For Initial steps I tried fetching \"About_us\" text from the websites and applied LDA on it, which gives me a list of words.\\r\\nI want to know if my initial steps are going to help further or is there any better model/technique, I could apply for the same.\\r\\nAny Resource or direction towards this will be very helpful\\r\\nThanks /n/r T-Systems Open Telekom Cloud Challenge \\x96 Handling Big Data with Cloud Computing\\r\\nDo you have a groundbreaking idea or service how to use #Sentinel data in combination with other data sources on @Telekom\\'s public cloud platform? The T-Systems Open Telekom Cloud Challenge is particularly interested in solutions that offer benefits to European citizens and their public administrations, which face challenges in meeting increased demand for mobility, urban logistics, tourism, and more while simultaneously reducing negative environmental impacts. Make #bigdata part of your idea through #cloud services, submit your promising solution at www.t-systems.copernicus-masters.com/, and get your project off the ground! http://bit.ly/1WrUaXH /n/r At Strata+Hadoop World in London over the next three days! Yay! Any of my data minded friends also here? #StrataHadoop #datascience #bigdata /n/r Does hadoop supports only HDFS file system? /n/r Lucknow tops Fast Track Smart Cities Challenge.....Chandigarh, Newtown Kolkata and Panaji included in the new list of 13.... Check out the interactive Visualization here: http://goo.gl/bjOUat #Update #Visualization #SmartCities #India /n/r #Python + #Hadoop for #BigData applications #pydatalondon #datascience #statistics #machinelearning #Spark #PySpark /n/r Indirect #Data Is the Travel Industry\\'s Secret Weapon \\r\\n  This travel season is shaping up to be the busiest since 2008, according to results from several...  Keep on reading:  Indirect #Data Is the Travel Industry\\'s Secret Weapon /n/r Treating Information as an Asset \\r\\n  The emergence of chief #data officers (CDOs) in many organizations and across industries indicates a growing recognition of information as a strategic business asset \\x96 one distinguished from the technology through which it flows. In fact, by 2020, Gartner predicts that 10 percent of organizations will have a highly profitable business unit specifically for productizing and commercializing their information assets. \\r\\n   \\r\\n  According to Douglas Laney, vice president and distinguished analyst at Gartne...  Keep on reading:  Treating Information as an Asset /n/r If \\'data is the new oil\\', why aren\\'t more companies drilling into analytics? http://deloitte.wsj.com/cio/2016/04/27/industrialized-analytics-datas-new-potential/ /n/r Predictions 2018. Like our page for more https://www.facebook.com/CTEPL /n/r Data Scientist is needed for immediate hiring for a multinational company located in Gurgaon, 10-12 years\\' overall experience with 7-8 years in Data Science with qualification as: - Graduated from Computer science or engineering  - Experience with Machine Learning, predictive analysis, regression modelling, R, Phython, etc. Data mining experience is must- For all interested candidates, please send your resume to sameer.johar@aslhr.com mentioning \"Data Scientist\" in the subject. /n/r Game Of Math...The MaxEnt Algorithm and Game of Thrones: \\r\\nhttp://www.mathisintheair.com/eng/2016/03/24/game-of-math-the-maxent-algorithm-and-game-of-thrones/ /n/r Great coverage of Israeli Big Data Spy Tech on the Financial Times /n/r Analytics can transform audit, but it won\\'t put auditors out of work. http://deloitte.wsj.com/cio/2016/03/22/innovation-in-audit-takes-analytics-ai-route/ /n/r Does anyone have useful resources for data science use cases in education?\\r\\n\\r\\nMany thanks fellows. /n/r I am preparing a possible lab session for the PASS Business Analytics Conference.  The conference attendees are analysts and directors:  the two-hour lab is intended for hands-on experience.  \\r\\n\\r\\nMy working title is \"SQL Server Data Science with R and Python\" -- based on what you know of what BI analysts may do with SQL Server 2016, what do you believe I should promise in an abstract? /n/r My first attempt to create a Data Visualization App using R.  This App lets you view the details of the Smart Cities of India(in different stages) on a Map and a Table. I would love to hear your feedback on this and please share the link to the app if you find it useful. \\r\\n\\r\\nApp Link: https://avinashr.shinyapps.io/Smart_Cities_India /n/r It has just started this morning. Don\\'t miss the boat ! Join for FREE at https://goo.gl/G5VOPl /n/r What Industries Will Be Next to Adopting Data Science? /n/r What is needed to build a data science team from the ground up?\\r\\nThe recruitment and hiring managers should focus on the individual skills that are needed on the data science team and aim to hire people with strengths in these skills.\\r\\n\\r\\nRead full article here: http://goo.gl/yCAoCe\\r\\n\\r\\nFeel free to share it on your blog, but please remember to keep a link to our site www.3blades.io or better yet, to the original post url http://goo.gl/yCAoCe\\r\\n\\r\\nThanks! /n/r J-3 BREAKING NEWS > le patron d\\'AIRBNB rejoint les 900 dirigeants du \\r\\n#WEB2B2016 \\r\\n18/02 - PARIS\\r\\nTheEvent\\r\\n17h50 salle amphit?atre - KEYNOTE 7 - Economie du PARTAGE, UBERISATION\\x85 Qu\\'allons nous devenir ?\\r\\n> Denis JACQUET Entrepreneur du Net depuis 2000, fondateur de Parrainer la Croissance, edufactory\\r\\n> Gr?goire LECLERCQ Pr?sident de la F?d?ration des Auto-entrepreneurs\\r\\n> Vincent RICORDEAU Co-Fondateur et PDG Kisskissbankbank\\r\\n> Pascal PICQ pal?oanthropologue, ma?tre de conf?rences du Coll?ge de France\\r\\n> Olivier MATHIOT Cofounder, CEO, Priceminister (sous reserve)\\r\\nNEW > Nicolas FERRARY, CEO, Airbnb /n/r Getting Started with Azure Machine Learning & DataZen (Mobile BI and Data Analytics for Any Device)\\r\\n\\r\\nhttps://msevents.microsoft.com/CUI/EventDetail.aspx?EventID=1032733296&Culture=en-IN&community=0 /n/r Data science, undoubtedly, requires numerous and varied skills. Analytics Week, the world\\'s largest analytics network, identified 25 data skills that make up the field of data science, trough a survey conducted to 490 data professionals from different companies. Their project ended up being an excellent way of providing a look into the field of data science. /n/r Data science, undoubtedly, requires numerous and varied skills. Analytics Week, the world\\'s largest analytics network, identified 25 data skills that make up the field of data science, trough a survey conducted to 490 data professionals from different companies. Their project ended up being an excellent way of providing a look into the field of data science. /n/r Do Big Data help To Evolve or Adapt ? /n/r You don\\'t need \\'perfect\\' data for analytics. http://deloitte.wsj.com/cio/2016/02/04/you-dont-need-perfect-data-for-analytics-analytics/ /n/r What do you think about the explosion of solution for Big Data ? /n/r Start Up Facts.\\r\\nFor more, Like Us https://www.facebook.com/CTEPL/ /n/r App and Workspace Discovery Demo: http://goo.gl/ZSNBrm /n/r I want to switch my career.please advise me what needs to done to get job in data science profile. /n/r Thank you for accepting me in the group. Greetings to all. I am here for whatever you need me. /n/r Our client is a technology startup in the online media space. Leveraging automation and crowdsourcing, they create high quality media data including captions, timed transcripts, indexes and video intelligence for clients in the online education, enterprise and entertainment industries. \\r\\n\\r\\nThey are currently searching for a Data Scientist who is passionate about machine learning, NLP and big data.\\r\\nResponsibilities include deriving insights from data from various sources - libraries of online media, user data, social media, etc using natural language processing, big data and machine learning techniques.\\r\\n\\r\\nThe ideal candidate will be highly skilled in NLP, Machine Learning Techniques. Hands on development skills in Python and R. \\r\\nThe ability to communicate technical concepts to business users, debate technical tradeoffs and create technical specifications is a must.\\r\\n\\r\\nYou will work in premier locations in NYC, casual environment, on diverse and fast-paced projects, and get experience by working with scary-smart people in a dynamic startup environment. We are hiring now, if you are interested, send your resume at dejan@ivyexec.com /n/r Analytics is no longer a nice to have, leading organizations will link analytic initiatives firmly to financial objectives, increase investments in advanced analytics, evolve comprehensive analytics centers of excellence, and incorporate a wider range of exogenous data. /n/r [ BigData Startups aboard the world\\'s first #RailTech accelerator! ]\\r\\n\\r\\nA 3-month programme where startups can obtain up to 25,000 funding, office space in central London, and unique opportunities to trial + sell products to the largest train operating companies in the world!\\r\\n\\r\\nWe are looking for startups and fast-growing scale-ups solving problems in industries outside the railway sector! Examples can be resource management, indoor navigation, data analytics, prediction, between many other disruptive enterprise services!\\r\\n\\r\\nApply at accelerator.hacktrain.com! /n/r The purpose of this group is to bring together \"young\" (not-so-young are welcome as well) Bayesian statisticians (not-so Bayesian are welcome as well) in order to foster interactions, tell people about interesting workshops, conferences, jobs, scholarships, or anything related to the Bayesian world. /n/r Big Data Market -Forecast And Analysis 2018 -Transform the Unstructured Data for Government \\r\\n\\r\\n#Big #Data is a term use to describe the process of collecting, organizing, and analyzing large sets of big data to discover hidden patterns, unknown correlations, and other useful information.\\r\\n\\r\\nGet Brief Information@ http://goo.gl/NVkrXa\\r\\n\\r\\nGermany is one of the emerging countries in big data technology market, and it is expected to grow in the upcoming years due to the adoption by various sectors such as internet, e-commerce, advertising, and others.  \\r\\n\\r\\nBig data help you to understand the information contained within the data, and help to identify the data which is most important for the business future business decisions.\\r\\n\\r\\n Big data also find disruptions to the production process before and as they take place. These findings can save noteworthy amount of money on equipment or machinery, and reduce labor expenditure on accidental maintenance and repairs. /n/r How and where can i learn big data analytics /n/r Analytics can reduce the complexity from your business processes. http://deloitte.wsj.com/cio/2015/12/21/taming-complexity-with-analytics/ /n/r Big data is a term use to describe the process of collecting, organizing, and analyzing large sets of big data to discover hidden patterns, unknown correlations, and other useful information. Germany is one of the emerging countries in big data technology market, and it is expected to grow in the upcoming years due to the adoption by various sectors such as internet, e-commerce, advertising, and others.  Big data help you to understand the information contained within the data, and help to identify the data which is most important for the business future business decisions. Big data also find disruptions to the production process before and as they take place. These findings can save noteworthy amount of money on equipment or machinery, and reduce labor expenditure on accidental maintenance and repairs.\\r\\n\\r\\nDownload Complete PDF Brochure : http://goo.gl/NVkrXa /n/r STATISTICAL INFERENCE, a must read !!! /n/r Hey every one , i have a problem understading Data Analysis & Statistics (ACP method , ....) , if you have something  can help me like tutorial videos Or good links to check out , thank you /n/r FINAL CALL! For all those interested in data and statistics and making them more accessible and understandable for all. #BeyondGDP #Data #StatsForAll #Stats  #SocEnt  #SocEntData  #Wellbeing  #SocialInnovation  #Innovation /n/r Spring XD 1.3 GA introduces Flo for Spring XD 1.0 + a job composition DSL http://spring.io/blog/2015/11/19/spring-xd-1-3-ga-and-flo-for-spring-xd-1-0-ga-released /n/r How many bits mem0ry are occupied by logical address and physical address /n/r Hey guys,\\r\\nI have a dilemma I wish to do a certification in data science(R, Predictive modelling), but most companies who hire for data science also requires Big data knowledge( Hadoop, Hbase etc). So wats the difference between the them. How these two differ from Machine learning. On the whole which course should I do? /n/r Big Data Halloween /n/r Are there any courses (short term/post-graduate...) in Data Science in Indian Universities? /n/r How to decide number of nodes in artificial neural network? please suggest some material(s) on this, if possible. /n/r The Big Data HackTrain hackathon is coming with double the prizes, hence double the fun :D\\r\\n\\r\\nThe winners of the HackTrain will not only get a fully paid trip for four to Singapore, but they will also obtain a fully paid trip for two to Amsterdam!!\\r\\n\\r\\nThe most epic hackathon in a train is coming at 200km/h, and you don\\'t want to miss it!!\\r\\n\\r\\nApplications are still open at http://hacktrain.com /n/r ?Please Like and Share the following FB Page: Big-Data Analytics Company?\\r\\n\\r\\nDear all, \\r\\n\\r\\nI\\'m sure you are doing great as always!\\r\\nMy name is Ken Tanaka (4th Year Medical student at Chiba University Medical School in Japan). \\r\\n\\r\\nI founded Big-Data Analytics company and entered business plan competitions. Since the number of \"Likes\" of the company\\'s facebook page is one of the selection criteria of the competition, I would appreciate so much if you can kindly \"Like\" and \"Share\" the following FB page. \\r\\n\\r\\n https://facebook.com/profile.php?id=1084593791580673&_rdr\\r\\n\\r\\nThanks again and I wish you all the best for your future success!\\r\\n\\r\\nBest, \\r\\nKen /n/r [All aboard the Train Hackathon]\\r\\n\\r\\nHack. Build. Design. Launch Big Data prototypes whilst on a moving train! Obtain a chance to win a fully paid trip to Singapore and Amsterdam!\\r\\n\\r\\nApply now at http://hacktrain.com/\\r\\n\\r\\nIf you have any questions please feel free to send us a message!\\r\\n\\r\\nSee you aboard Trainhackers! #HackTheRails #LetsDoThis /n/r Big Data Economics, Towards Data Market Places: Nature of Data, Exchange Mechanisms, Prices, Choices, Agents & Ecosystems\\x85.\\r\\n\\r\\nThe utilization of enormous information to accomplish operational proficiency, ascend in value-based and unstructured information, development in use of huge information for showcasing exercises, development out in the open segment, and issues in regards to information security are a key\\'s portion calculates that are in charge of the Japan\\'s development huge information market. On the other hand, it is extremely vital to investigate all arrangements of information to recognize all the concealed examples in the information. Huge information serves to comprehend distinctive data identified with information, and empowers parameters which are essential to take business-related choices later on.\\r\\n\\r\\nTo Know More Access PDF Brochure@ http://goo.gl/ncfv2J , /n/r NEWS I Congratulations to the Top 12 World Finalists at 2015 Big Data Analytics World Championships for Enterprise (TEXATA).  Well done to: IBM, FICO, KPMG, Barclays, HP Labs, TNG Quant, Intrum Justitia, 6Sense, Universidad Aut?noma de Madrid, University of Potsdam and Castlight Health >> http://www.texata.com/finalists/ /n/r The factors which turned the decision for Munich Re in favor of #SAS were the speed at which the #analyses were carried out, the upward graph in the tech graph, the performance of the team for SAS overall and the ability of the system to deliver and deploy results swiftly. Read more https://goo.gl/IlAoP9 /n/r #MOL2NET (ONLINE & FREE OF COST), 2015,15-30 Nov, MDPI Sciforum\\r\\nInternational Conference to Foster Interdisciplinary Collaboration in Science. \\r\\nEvent call: https://www.facebook.com/events/545795472245161/\\r\\nOfficial Conference web: http://sciforum.net/conference/mol2net-1 \\r\\nThe scope includes, but is not limited to, Experimental #Chemistry (all branches), #Materials, #Nanosciences, #Medicine, #Neurosciences, #Biomedical #Engineering, #Biosciences, #Biotechnology, #Statistics, #Bionformatics, #BigData #Analytics, #Computer and #Network #Sciences. \\r\\n\\r\\nCONFERENCE CHAIRMAN: Prof. Humbert Gonzalez-Diaz, IKERBASQUE Professor, Department of Organic Chemistry II, University of Basque Country UPV/EHU, Bizkaia.\\r\\n\\r\\nADVISORY COMMITTEE (See full list in the following link)\\r\\nhttp://sciforum.net/conference/MOL2NET-1/page/organizers\\r\\n\\r\\nNOTES: \\r\\n * The conference is Totally Online, no physical presence is needed saving travelin costs. We accept experimental works, theoretical works, or experimental-theoretic works in different areas including, but not limited to, all areas mentioned above; as well as #Legal and #Regulatory issues. \\r\\n\\r\\n * Proceedings wil be Published Online, Open Access, Totally and Free of Charges (not cahrges wil be lieved to authors). Online submission system is ready and will be open until 2015, Nov, 10, please, follow the instructions: \\r\\n\\r\\n(1) read call for papers: http://sciforum.net/conference/MOL2NET-1/page/call\\r\\n\\r\\n(2) Read instructions to authors: http://sciforum.net/conference/MOL2NET-1/page/instructions\\r\\n\\r\\n(3) Download template: http://sciforum.net/bundles/sciforumversion2/images/conference/MOL2NET-1/MOL2NET-template.docx\\r\\n\\r\\n(4) Sing up and submit the title, authors, and abstract of your short communication: http://sciforum.net/user/submission_for_conference/83\\r\\n\\r\\n(5) Wait for abstract approval email to login and submit the full version (.doc and .pdf) of your short communication (2-3 papes) or research paper, slide presentation, or video (optional): http://sciforum.net/user/submission_for_conference/83 /n/r Please help me.What are the Decision tree multi label classification algorithms like Naive bayes in Data Mining.. /n/r Hi,\\r\\nI would like to know, that is it mandatory to scale a data set for implementing a two class(binary), logistic regression algorithm for  classification ? Does it improve precision ?\\r\\nThanks in advance. Any suggestion will be much appreciated !! /n/r Round 1 of TEXATA Big Data Analytics Competition starts in 3 days. Who\\'s in your company or university league? Share with interested friends. http://www.texata.com/ /n/r Hi all. Can anyone recommend a good resource (a book, a course, a youtube channel, etc) on Bayesian Probability and Statistics?  Intermediate level and ideally something that is free to view/download, please :).  Thanks in advance for your help! /n/r Aoa.......Can anyone have the derivation of expected mean squares for two factor factorial experiment.......E(MSA),E(MSB),E(MSAB) and E(MSE)???? /n/r Aoa.......Can anyone tell me the applications of factorial design??? /n/r UNICORE (Uniform Interface to Computing Resources) offers a ready-to-run Grid system including client and server software. You are invited to join and post pertinent content: https://www.facebook.com/groups/320795044682/ /n/r Thought of Day- Co-relation does not imply Caucasian /n/r Hi Everyone\\r\\nI am planning to take up Data Science/Big Data as my specialization for my Masters study.I came across different courses such as Data analytics, business analytics. Are they both same.\\r\\nI also wanted to know which mid-ranged universities (preferably Public) in the US offer the best of Data Science courses.\\r\\nAnd what are the Job Prospects in the US for a person with Data science as his specialization. /n/r If you are interested in Tech then stay in touch...jobs, news, jokes, pics, discussions and much much more.\\r\\nhttps://www.linkedin.com/grp/home?gid=7470692\\r\\nhttps://www.facebook.com/revolutiontechnologyltd\\r\\nwww.revolutiontechnology.co.uk \\r\\n@revtechnology1 /n/r If you are interested in Tech then stay in touch...jobs, news, jokes, pics, discussions and much much more.\\r\\n\\r\\nhttps://www.linkedin.com/grp/home?gid=7470692\\r\\nhttps://www.facebook.com/revolutiontechnologyltd\\r\\nwww.revolutiontechnology.co.uk \\r\\n@revtechnology1 /n/r i was able to download only 7 days (one weak) tweets using twitter API but i am in need of all tweets related to particular hash tag and geo code . any one please share or email the python script at mail4rajesh87@gmail.com thanks in advance . Plse help to share. /n/r Hi All,\\r\\n\\r\\nI am very happy to join such a knowledgeable group.\\r\\n\\r\\nIs here anyone can give me a formula to calculate KPI with four parameters,\\r\\n\\r\\nQuantities\\r\\nObtained Quantities\\r\\nTime Utilization \\r\\nError\\r\\n\\r\\nI hope you guys will respond me asap.\\r\\n\\r\\nThank you.\\r\\n\\r\\nKind Regards,\\r\\nTariq Hussain /n/r hi friends... please suggest Books related to basics of Big Data. i want to make those books as reference books to my bachelor of degree students. \\r\\nthanks in advance..... /n/r Group focused in topics related to software agents and agent systems. You are invited to join and post pertinent content: https://www.facebook.com/groups/299640474516/ /n/r Going to the European Conference on Data Analysis! Yay! #ecda2015 #datascience #statistics #machinelearning #dataanalysis #essex #bigdata /n/r Hello guys I\\'m planning to create a music library in my college accessible to all students inside the campus.It will be available only on intranet for that I want to implement concepts of Hadoop in it,can anyone suggest what frameworks should I use and in what ways I can implement them?? /n/r Hey guys! \\r\\n\\r\\nI\\'m currently developing my undergraduate thesis for Computer Science and I\\'m looking for a problem related to Data Science that I can solve in 6-8 months. It would be great if this problem is also related to sales, marketing or organizational development since I like those topics too. \\r\\n\\r\\nHave you worked in this fields? What kind of problems have you encountered? What are the trending applications of Data Science right now?\\r\\n\\r\\nThanks for your response! /n/r Fundamental Basics of Information Technologies; You are invited to join and post pertinent content: https://www.facebook.com/groups/184721844885433/ /n/r Know some great Data Scientists and Big Data friends in your Business or University? Enter the TEXATA 2015 Big Data Analytics World Championships for Business and Enterprise. Two Online Rounds and Live World Finals in Austin Texas. Round 1 Starts in 5 Weeks >> http://www.texata.com /n/r The Annals of Computer Science and Information Systems (ACSIS) is an online journal-style series reporting theoretical and applied research results in computer science and information systems. The series publishes contributions in a broad area that crosses the boundaries between science (including social science), engineering and management. It accepts publications resulting from the commercial market, as long as the discussed industrial problems stimulate the related sciences and can impact the engineering profession. Publisher: Polish Information Processing ociety. Volume proposals are welcome. More info available at: annals-csis.org. You are invited to join and post pertinent content: https://www.facebook.com/groups/938470596191055/ /n/r Love CSS? Join this group for CSS4 updates. Share for a reason. \\r\\n\\r\\nhttps://www.facebook.com/groups/css4.group /n/r #DexLab #Analytics presents an Introductory Session on #Ms #Excel #VBA Macros & Dashboards. To be conducted by industry professional working in to consulting and analytics, you will be given an insight in to the following areas:\\r\\n\\r\\n \\x95 Overview of Excel and its application in various domains\\r\\n \\x95 Functions in Excel\\r\\n \\x95 Graphs\\r\\n \\x95 V Look Up and Pivot Tables\\r\\n \\x95 Macro recording in Excel and automation\\r\\n \\x95 Over view if Dashboards.\\r\\n\\r\\nDate: Saturday, 22nd August 2015\\r\\nTime: 2:00 to 3:30 PM\\r\\nVenue: K 3/5, DLF Phase 2, Gurgaon, Haryana - 122 002.\\r\\n\\r\\nThe session will be followed by Q&A sessions. The session is free of cost and would require prior registration. To register your self call at +91124450222. Visit at www.dexlabanalytics.com /n/r #DexLab #Analytics presents an Introductory Session on #Ms #Excel #VBA Macros & Dashboards. To be conducted by industry professional working in to consulting and analytics, you will be given an insight in to the following areas:\\r\\n\\r\\n \\x95 Overview of Excel and its application in various domains\\r\\n \\x95 Functions in Excel\\r\\n \\x95 Graphs\\r\\n \\x95 V Look Up and Pivot Tables\\r\\n \\x95 Macro recording in Excel and automation\\r\\n \\x95 Over view if Dashboards.\\r\\n\\r\\nDate: Saturday, 22nd August 2015\\r\\nTime: 2:00 to 3:30 PM\\r\\nVenue: K 3/5, DLF Phase 2, Gurgaon, Haryana - 122 002.\\r\\n\\r\\nThe session will be followed by Q&A sessions. The session is free of cost and would require prior registration. To register your self call at +91124450222. Visit at www.dexlabanalytics.com /n/r DexLab Analytics presents an Introductory Session on Ms Excel VBA Macros & Dashboards. To be conducted by industry professional working in to consulting and analytics, you will be given an insight in to the following areas:\\r\\n\\r\\n \\x95 Overview of Excel and its application in various domains\\r\\n \\x95 Functions in Excel\\r\\n \\x95 Graphs\\r\\n \\x95 V Look Up and Pivot Tables\\r\\n \\x95 Macro recording in Excel and automation\\r\\n \\x95 Over view if Dashboards.\\r\\n\\r\\nDate: Saturday, 22nd August 2015\\r\\nTime: 2:00 to 3:30 PM\\r\\nVenue: K 3/5, DLF Phase 2, Gurgaon, Haryana - 122 002.\\r\\n\\r\\nThe session will be followed by Q&A sessions. The session is free of cost and would require prior registration. To register your self call at +91124450222. Visit at www.dexlabanalytics.com /n/r The RuleML Initiative is an international non-profit organization covering aspects of Web rules and their interoperation. You are invited to join and post pertinent content: https://www.facebook.com/groups/ruleml/ /n/r 1- Incremental clustering algorithms\\r\\n2- Online clustering algorithms\\r\\n3- Data stream clustering algorithms\\r\\n\\r\\nWhat does it means incremental clustering , Are the following expressions related? Does some of them include others? What is the difference between them? What are the constraints that each one should face unlike others? /n/r The area of Scalable Computing has matured and reached a point where new issues and trends require a professional forum. As a response to this need, the SCPE Facebook group focuses on all topics pertinent to, broadly understood, scalable computing. In this way it goes hand-in-hand with the \"Scalable Computing: Practice and Experience\" international peer-reviewed journal. The SCPE publishes original refereed papers that address the present as well as the future of parallel and distributed computing. The journal focus on algorithm development, implementation and execution on parallel and distributed architectures, as well on application of parallel and distributed computing to the solution of real-life problems. You are invited to join and post pertinent content: https://www.facebook.com/groups/251002403356/ /n/r Why Python is mostly used in big data analytics? /n/r Hi guys,i hav chosen my final year project in web analytics,i have only some basic knowledge about that,can anybody give some modules or pls refer me some websites to develop my project /n/r Extension du deadline de notre conf?rence KDDA\\'2015 jusqu\\'au 15/08/2015. Pri?re diffuser au maximum aupr?s de vos contacts nationaux et internationaux. Merci.\\r\\nhttp://www.esi.dz/kdda/ /n/r New group devoted to issues related to establishing voluntary interoperability among heterogeneous Internet of Things (IoT) platforms. \\r\\nIt is to be somewhat more narrow in scope / focused than \"more general\" groups dealing with IoT. You are invited to join and post pertinent content: https://www.facebook.com/groups/487893988043170/ /n/r Hi guys, have any of you worked on meta-search engine for travel, and especially for flights? Data Science Society invited Data Scientists from Skyscanner to tell us more about it tonight.\\r\\nWe will do a live event and webinar so book your time slot tonight and check out the FB event for the streaming link. You will have the opportunity to raise all your questions at slido.com #DSS. /n/r Group for people interested in Advances in Business ICT (ABICT) approached from a multidisciplinary perspective. You are invited to join and post pertinent content: https://www.facebook.com/groups/282894838704/ /n/r Sign up to One R Tip A Day /n/r Group devoted to selected aspects of computer science and information systems (covered by the annual FedCSIS conference; organized in cooperation with units of IEEE and ACM; indexed in Web of Science; acceptance rate for regular papers ~25%). You are invited to join and post pertinent content: https://www.facebook.com/groups/367888070292/ /n/r I\\'d like to draw your attention to Crunch, a Practical Big Data conference with Alistair Croll (author of Lean Analytics), Doug Cutting (founder of Hadoop) and several international experts from all around the world, including Spotify, Pinterest and SurveyMonkey: http://www.crunchconf.com/#speakers . Crunch is Prezi.com\\'s and USTREAM\\'s continued non-profit effort to organize world-class conferences in the region (Oct 29-30, Budapest). Early Bird tickets are still on sale until 15 July, check it out! http://www.crunchconf.com/#tickets . /n/r Group devoted to sharing information concerning distributed systems, grid computing, cloud computing, scalable computing, large scale distributed computing, and related topics. You are invited to join and post pertinent content: https://www.facebook.com/groups/278357194562/ /n/r Group devoted to distributed (primarily agent-based) computer systems in which autonomous entities negotiate with one another, typically on behalf of humans, in order to come to mutually acceptable agreements. You are invited to join and post pertinent content: https://www.facebook.com/groups/122962947743348/ /n/r Today\\'s One R Tip A Day /n/r #RiseUp   to  the challenges  to be a  winner...\\r\\nwww.adgeco.com /n/r Today\\'s One R Tip A Day /n/r Persons interested in Agent-based computing are invited to join our Facebook group: https://www.facebook.com/groups/299640474516/ /n/r Submission Deadline, July 2015 : Conferences of Computer Science & Electronics\\r\\nFB Page : www.facebook.com/guide2research\\r\\nPDF File : www.guide2research.com/conf/july-2015.pdf\\r\\n\\r\\nBy Publisher:\\r\\n->IEEE www.guide2research.com/conferences/ieee\\r\\n->ACM   www.guide2research.com/conferences/acm\\r\\n->Springer www.guide2research.com/conferences/springer\\r\\n->Elsevier www.guide2research.com/conferences/elsevier /n/r Persons interested in Cyber-Physical Systems are invited to join our Facebook group: https://www.facebook.com/groups/584689371646354/ /n/r I am using intelli j idea (like eclipse)with scala integration , i create a scala sbt project , i use spark 1.4.0 and scala 2.11.6 , I am getting error on :\\r\\nimport org.apache.spark.{SparkContext, SparkConf}\\r\\n\\r\\nthe file buid.sbt contains this code :\\r\\n\\r\\nname := \"simple\"\\r\\n\\r\\nversion := \"1.0\"\\r\\n\\r\\nscalaVersion := \"2.11.6\"\\r\\n\\r\\nlibraryDependencies += \"org.apache.spark\" % \"spark-core_2.10\" % \"1.4.0\"\\r\\n\\r\\nwhen i use maven to build java application i have not problem just the problem is when i try to build scala application with sbt using intellij idea /n/r Need Jobs? Need to Post Your Offers? Questions About HR? \\r\\nJoin a great hub for everyone involved in HR \\x96 Business Owners, Entrepreneurs, HR Executives, Managers, Supervisors, Officers and anyone who deals with people. We hope to learn from one another through a compendium of articles, learning ideas and nuggets of wisdom. In this regard, everyone is enjoined to participate, and we especially encourage those who have a wealth of experience to share their knowledge and be an instrument for the widening of the horizon of all the HR Professionals and Practitioners in the Philippines. /n/r Looking for a Business and Big Data Competition? Registrations Open for TEXATA 2015 World Championships >> http://www.texata.com /n/r Dear Friends! By filling in this questionnaire about the global IT education you would make an impact! Please share it with your friends!\\r\\nhttps://ru.surveymonkey.com/s/XROOKIE2015 /n/r a movie called regression /n/r Hello,\\r\\n\\r\\nWhat are the key aspects of data quality on data mining ? \\r\\n\\r\\nAny advice or suggestion will be much appreciated .\\r\\nThank you . /n/r SanDisk InfiniFlash Rethinking Delivering Flash at Data Center Scale [Video]\\r\\nhttp://bit.ly/1KUBSYJ /n/r SanDisk InfiniFlash Use Cases [Video]\\r\\nhttp://bit.ly/1HSSrxQ /n/r The Persuasiveness of a Chart Depends on the Reader, Not Just the Chart\\r\\nhttp://bit.ly/1KMNDx8 /n/r Map Design Gone Wrong Do People Even Care Anymore?\\r\\nhttp://tinyurl.com/njufbry /n/r SanDisk InfiniFlash Use Cases [Video]\\r\\nhttp://tinyurl.com/oqbnvte /n/r 5 Trends in Big Data revealed\\r\\nhttp://bit.ly/1G24P0l /n/r At the Allen & Overy Fintech event on Democratising Finance thru P2P lending #datascience #fintech #p2plending #FundingCircle #TransferWise /n/r Dadeh Kavan Company Implement your ADF project with low cost\\r\\nPlease send Email for Oracle Projects\\r\\nHamedoracle@gmail.com /n/r Call for Position Papers: 2015 Federated Conference on Computer Science and Information Systems (FedCSIS); deadline on Monday: June 1, 2015; https://www.facebook.com/events/1610586142548063/ /n/r Call for Position Papers -- FedCSIS 2015: more info at: https://www.facebook.com/events/1610586142548063/ /n/r Call for Papers; High Performance Computing Solutions for Complex Problems; Paper submission: September 19, 2015; more info: \\r\\nhttps://www.facebook.com/events/386079591589615/ /n/r Teri mehfil se nikle kisi ko khabar tak na hui,Tera mud mud ke dekhna hamein badnam kargaya\\x85.. /n/r Ola, o Summit ? gratuito. H? uma trilha dedicada a Big Data onde farei uma das palestras. /n/r Music video-sample from FedCSIS 2014 conference is available at:  https://www.fedcsis.org/resources/fedcsis-records/FedCSIS_2014_band_videoclip.mp4 (evenings are very social ;-) /n/r FedCSIS 2015; in cooperation ACM + IEEE; indexed in Web of Science and Scopus; 4+ days till deadline; https://www.facebook.com/events/684283518363800/ /n/r Advances in Artificial Intelligence and Applications 2015; technical cooperation ACM + IEEE; indexed in Web of Science and Scopus; submission in 9 days; https://www.facebook.com/events/962405283784396/ /n/r Do you want to learn Hadoop but confused from where to start?\\r\\nCheck this beginner\\'s guide for Hadop.\\r\\n\\r\\nhttp://saphanatutorial.com/prerequisites-for-learning-hadoop/ /n/r Less than 2 weeks till submission deadline -- precise down-clock on the conference www site: https://www.facebook.com/events/684283518363800/ /n/r AnalytiX DS invites all to \"Texas Modeling User Group Spring 2015 Meeting\" \\r\\nhosted by ?#CAERwin? on 28th April in ?#Texas?. Register yourself before 24th April \\r\\nat http://bit.ly/1PrARI9. http://analytixds.com/events/ ?#DataManagement? ?#DataGovernance? ?#DataModeling? /n/r WANT to LEARN MORE or EARN MORE?\\r\\n\\r\\n-\\r\\n\\r\\nWebsite link ===>>> https://NewEdgeMath.com /n/r ? ? WANT to LEARN MORE or EARN MORE? ? ?\\r\\n\\r\\n-\\r\\n\\r\\nWebsite link ===>>> https://NewEdgeMath.com /n/r AnalytiX DS invites all to \"Texas Modeling User Group Spring 2015 Meeting\" hosted by #CAERwin on 28th April in #Texas. Register yourself before 24th April at http://bit.ly/1PrARI9. #DataManagement #DataGovernance #DataModeling /n/r Some good speakers lined up for the last day of Hadoop Summit Europe 2015! :-D\\r\\n#datascience #bigdata #hadoopsummit /n/r Christian, Leo and I in front of the venue for #HadoopSummit2015 (Proof that we were there) #datascience #hadoop /n/r Christian, Leo and I in front of the venue of the #HadoopSummit2015 (Proof that we were there) #datascience #hadoop /n/r ? ? Want to Learn More or Earn More? ? ?\\r\\n\\r\\nhttps://www.facebook.com/video.php?v=839855122763388\\r\\n\\r\\nWebsite link ===>>> https://NewEdgeMath.com /n/r ? ? Want to Learn More or Earn More? ? ?\\r\\n\\r\\nhttps://www.facebook.com/video.php?v=839855122763388\\r\\n\\r\\nWebsite link ===>>> https://NewEdgeMath.com /n/r trick to buy rs 634 sandisk 16 gb dual pendrive for rs 46\\r\\nsteps to get the offer-\\r\\ndownload the snapdeal app after that u will get a pop up message put invite code. code is given below\\r\\nimportant step-\\r\\nsign up with this invite code- 7TVJ105095\\r\\nto get rs 50\\r\\nthan sign up with your email id or facebook id.\\r\\nafter this open the snapdeal app share section and their u will came to know how to buy this pendrive only for rs 46\\r\\nfor proof- /n/r Calling all students, teachers, and professionals who want to be part of \\r\\n\\r\\nan exceptional social community $$$\\r\\n\\r\\nhttps://www.facebook.com/video.php?v=839855122763388 /n/r FedCSIS 2014 Proceedings are now available in Thomson Reuters Web of Science Core Collection: http://apps.webofknowledge.com/Search.do?product=WOS&SID=P2GVKkoj9Lbyzqhbefd&search_mode=GeneralSearch&prID=1651d1e1-186b-4e49-8a14-e69a2b28a10c /n/r EECSI 2015 Call for papers: Deadline 15 Apr 2015 http://iaesonline.com/eecsi /n/r KDDA\\'2015 is an annual leading International Conference on Knowledge Discovery and Data Analysis. The purpose of the conference is to bring together researchers and actors from academia, industry, and government to advance the science, engineering, and technology in Data science in general. \\r\\n\\r\\nhttp://www.esi.dz/kdda /n/r Hi guys,\\r\\nPredicting outcomes with data has reached a new level in today\\'s time. Here is a #free #webinar on #Predictive modeling with #R titled \\'The Whys and Hows of Predictive modelling\\' which will cover interesting practices in the 21st Century that has changed the way we look at data and captivated the attention of #statistical programmers, #analysts and #data scientists worldwide. Want to know more?click here. http://bit.ly/1aMzwvw /n/r Hi guys,\\r\\n\\r\\nPredicting outcomes with data has reached a new level in today\\'s time. Here is a #free #webinar on #Predictive modeling with #R titled \\'The Whys and Hows of Predictive modelling\\' which will cover interesting practices in the 21st Century that has changed the way we look at data and captivated the attention of #statistical programmers, #analysts and #data scientists worldwide.  Want to know more?click here. http://bit.ly/1aMzwvw /n/r Can someone answer these questions  ?\\r\\n\\r\\n1)How can the Mapper process each word in order to produce a more refined output?  Describe at least two processes.\\r\\n2)What could be some unintended consequences of your processing methods on input that is not a word? For example, your input files includes \"words\" such as:\\r\\n1.21.11\\r\\n-65\\r\\n1,000,000 /n/r The Clouds Economy on New Delhi World Book Fair,2015\\r\\n\\r\\nNew Delhi World Book Fair,2015 at Pragati Maidan,New Delhi.Our Stall No.was 210 at Hall No:11.The event was scheduled from 14th Feb to 22nd Feb.\\r\\n\\r\\nOnly now, The Clouds Economy [Kindle Edition] for only  $6.29\\r\\nhttp://amzn.to/1EtZTSf /n/r Hi BI Gurus,\\r\\nI am database architect and have primarily worked with SQL Server. I\\'m now trying to hone my skills on BI (SSAS,SSIS with a bit of SSRS).  I would really appreciate if you could guide me to some online study material with some HandsOn labs. Cheers!! /n/r This is a heads-up for aspirants who specialize #Hadoop #java, a study on current job trends for #Bigdata jobs in 2015. To view the complete video click here. http://goo.gl/hL0mQQ /n/r Dear Friends,\\r\\nI found a #free #webinar on #mapreduce #design #patterns titled \" #Tailored #Big #Data #Solutions using MapReduce Design Patterns \". If you are interested to learn more about various applications of design patterns in Big Data using MapReduce, you can also attend the webinar for free !\\r\\nClick here to register:http://bit.ly/1w0yp5d /n/r Dear Friends,\\r\\nI found a #free #webinar on #mapreduce #design #patterns titled \" #Tailored #Big #Data #Solutions using MapReduce Design Patterns \". If you are interested to learn more about various applications of design patterns in Big Data using MapReduce, you can also attend the webinar for free !\\r\\nClick here to register:http://bit.ly/1w0yp5d /n/r Hi Every one kindly send me your personal no and Email Id\\'s I have 50 Hours of Hadoop Training Videos with Cloudera Certification Dumps Hurry Up its free.\\r\\nIf You see these types of posts any where please do not give your id rather ask them questions why cnt they share it over here? Why will they waste there time sending to all the emails and what happens many people write these types of posts and get ids of all hadoop bigdata datascience enthusiastic people and later they send them promotion mails and use these ids Data base . they sell it to other companies also. Kindly read it properly I do not have anything to send. /n/r Hi Every one kindly send me your personal no and Email Id\\'s I have 50 Hours of Hadoop Training Videos with Cloudera Certification Dumps Hurry Up its free.\\r\\nIf You see these types of posts any where please do not give your id rather ask them questions why cnt they share it over here? Why will they waste there time sending to all the emails and what happens many people write these types of posts and get ids of all hadoop bigdata datascience enthusiastic people and later they send them promotion mails and use these ids Data base . they sell it to other companies also. Kindly read it properly I do not have anything to send. /n/r Why is Cloud Computing growing so rapidly! #Tyronesystems /n/r Hadoop Ecosystem summary to keep the track of hadoop related projects,focused on FLOSS environment\\r\\nClick on this link ---->http://www.hadooptpoint.com/hadoop-ecosystem-2/ /n/r Your Data Center Nightmare could make you win Amazon Shopping vouchers! /n/r Bridges: Connecting Researchers, Data, and HPC: January 30, 2:30pm EST, by Google Hangout and in person at Pittsburgh Supercomputing Center, 300 S Craig St, Pittsburgh PA.\\r\\n\\r\\nJoin us for a preview of Bridges, #PSC\\'s innovative new computing resource for #BigData and #DataAnalytics which will be available at no charge to the U.S. open research community. Bridges will serve a wide variety of research and applications, providing a high degree of interactivity, gateways and tools for gateway-building, and a very flexible user environment.\\r\\n\\r\\nIf you can\\'t come in person, you can watch this event via the web. Please let us know if you will attend so we can plan for you: http://bit.ly/1zrgpC7 /n/r How to upload files on cloud ? Learn from Rajni Sir :) /n/r Do You Really Know #Big #Data! #Tyronesystems /n/r Dear Friends,\\r\\nI found a #free #webinar on #DevOps titled Redefining your #IT #Strategy. It is absolutely free and anyone who wants to learn more about #DevOps Culture, #Tools, #Automation, #Integration in short about #Business #Management  can join and attend. It is not spam!\\r\\nClick here to register: http://bit.ly/1une4ke /n/r in just 1 day of starting this we got 25 users registered. hope we can now move on to this new platform and build india\\'s own social networking platform by monetization process inside india and a  lot of employment in india. ( http://www.ucctu.com ) /n/r Follow these tips to protect your #data from #Data #Disaster! #Tyronesystems /n/r This article clears our confusion between choosing Online mode of learning and Classroom format especially for Big Data Education Space\\r\\nRead here: http://www.bigdataeducation.in/is-online-course-good-for-me-or-classroom-format/ /n/r Hello to all members of Big Data, Data Science, Data Mining /n/r If you are looking for Job change or to build your career, Learn Cyber Security- Ethical Hacking on weekends it\\'s the right option\\r\\nPlacement Assistance available \\r\\n contact: +91-9840730610 / 9787401008 / 9600077954\\r\\nwww.htcitmr.ac.in\\r\\n YouTube: ITMR India\\r\\n Facebook: HTC Institute of Technology Management and Research \\r\\n Twitter: HTC ITMR\\r\\n LinkedIn : HTC Institute of Technology Management and Research \\r\\n write to us : training@htcitmr.ac.in /n/r Hi Folks..Greetings!!..I m analyzing Different Classification Algorithms to do predictive analysis on a dataset. The training data I have has 5 attributes, 4 of which are independent.2 of the independent attributes are continuous and 2 are discrete.I have been working to fit this data into Logistic Regression model and generate Predictions. Now on this background I have couple of questions for which I require answers  -\\r\\n\\r\\nQ1). Does the ratio of my negative and positive cases in the training data affects learning of my Regression Model ?\\r\\nQ2). Should I consider co-related attributes for training the model or only independent ones. Does it affect Performance or Cost Function ?\\r\\n\\r\\nWill Seek Suggestions or Open Discussion. Thanks in advance !! /n/r Hello Friends plz help me,\\r\\n  I want to perform comparision between two tables.\\r\\n  like I have two table \"product1\" and \"product2\". both have columns \"product_name\" and    \"product_price\" so now I want to compare product_price of both tables on the behalf product_price comparision display the product_name.\\r\\nExample:-\\r\\nfirst product1 table\\r\\nhbase(main):001:0> create \\'product1\\',\\'cf1\\'\\r\\n0 row(s) in 1.5980 seconds\\r\\nhbase(main):008:0> put \\'product1\\',\\'row1\\',\\'cf1:product_name\\', \\'shoe\\'\\r\\n0 row(s) in 0.0240 seconds\\r\\n\\r\\nhbase(main):009:0> put \\'product1\\',\\'row1\\',\\'cf1:product_price\\',\\'2000\\'\\r\\n0 row(s) in 0.0090 seconds\\r\\n\\r\\nhbase(main):010:0> put \\'product1\\',\\'row2\\',\\'cf1:product_name\\', \\'shoe\\'\\r\\n0 row(s) in 0.0040 seconds\\r\\n\\r\\nhbase(main):011:0> put \\'product1\\',\\'row2\\',\\'cf1:product_price\\',\\'4000\\'\\r\\n0 row(s) in 0.0040 seconds\\r\\n\\r\\nhbase(main):013:0> put \\'product2\\',\\'row1\\',\\'cf2:product_name\\',\\'shoe\\'\\r\\n0 row(s) in 0.0190 seconds\\r\\n\\r\\nhbase(main):014:0> put \\'product2\\',\\'row1\\',\\'cf2:product_price\\',\\'2500\\'\\r\\n0 row(s) in 0.0090 seconds\\r\\n\\r\\nhbase(main):015:0> put \\'product2\\',\\'row2\\',\\'cf2:product_name\\',\\'shoe\\'\\r\\n0 row(s) in 0.0110 seconds\\r\\n\\r\\nhbase(main):016:0> put \\'product2\\',\\'row2\\',\\'cf2:product_price\\',\\'3500\\'\\r\\n0 row(s) in 0.0130 seconds\\r\\nhbase(main):012:0> scan \\'product1\\'\\r\\nROW                                   COLUMN+CELL                                                                                               \\r\\n row1                                 column=cf1:product_name, timestamp=1418879753601, value=shoe                                              \\r\\n row1                                 column=cf1:product_price, timestamp=1418879810299, value=2000                                             \\r\\n row2                                 column=cf1:product_name, timestamp=1418879834315, value=shoe                                              \\r\\n row2                                 column=cf1:product_price, timestamp=1418879843868, value=4000                                             \\r\\n2 row(s) in 0.0310 seconds\\r\\n\\r\\nSecond product2 table:\\r\\nhbase(main):001:0> create \\'product2\\',\\'cf2\\'\\r\\n0 row(s) in 1.5980 seconds\\r\\nhbase(main):013:0> put \\'product2\\',\\'row1\\',\\'cf2:product_name\\',\\'shoe\\'\\r\\n0 row(s) in 0.0190 seconds\\r\\n\\r\\nhbase(main):014:0> put \\'product2\\',\\'row1\\',\\'cf2:product_price\\',\\'2500\\'\\r\\n0 row(s) in 0.0090 seconds\\r\\n\\r\\nhbase(main):015:0> put \\'product2\\',\\'row2\\',\\'cf2:product_name\\',\\'shoe\\'\\r\\n0 row(s) in 0.0110 seconds\\r\\n\\r\\nhbase(main):016:0> put \\'product2\\',\\'row2\\',\\'cf2:product_price\\',\\'3500\\'\\r\\n0 row(s) in 0.0130 seconds\\r\\n\\r\\nNow I want to comapre these two colomns fields on the behalf of price display the product_name. /n/r How to compare two columns in Hbase? How to perform comparisons in Hbase? /n/r HADOOP BIG DATA\\r\\nQLIKVIEW\\r\\nTABLEAU\\r\\nMICRO STRATEGY\\r\\nETL TESTING\\r\\nHADOOP BIG DATA\\r\\n\\r\\nCLASS ROOM AND ONLINE TRAINING @ BANGALORE @ 09620684481 /n/r Filters in HBase\\r\\nClick on this link--->http://www.hadooptpoint.com/filters-in-hbase-shell/ /n/r Hello friends, I have Installed Hbase but when I want to create table in Hbase I am getting Exception please help me how can I solve this issue.\\r\\n\\r\\nhbase(main):002:0> create \\'test\\',\\'cf\\'\\r\\n\\r\\nERROR: org.apache.hadoop.hbase.PleaseHoldException: org.apache.hadoop.hbase.PleaseHoldException: Master is initializing\\r\\n\\r\\nHow can I solve this????? /n/r Hello frnds I want to work on Nosql. so please guide me how to install Hbase how to configure Hbase, I tried but its not Working.\\r\\nPlease help me how should i do this. /n/r FIRST of ITS KIND CAREER-CHANGING DIPLOMA IS NOW WITHIN YOUR REACH.\\r\\n Professional Diploma in Business Intelligence & Analytics \\x96 University Recognized Program only on Weekends with Placement Assistance.\\r\\n\\r\\nTalk to our experts\\r\\n HTC Towers, No. 41, GST Road, Guindy Chennai \\x96 600 032\\r\\n +91-9840730610 / +91-9787401008 / +91-9600077954\\r\\n training@htcitmr.ac.in\\r\\nwww.htcitmr.ac.in\\r\\n FB: HTC Institute of Technology Management & Research\\r\\n You Tube: ITMR India \\r\\n Twitter: HTC ITMR /n/r Apache Tez Introduction In Simple Way :-)\\r\\n\\r\\nWhy Apache Tez Much Faster Than Mapredue ? :-)\\r\\n\\r\\nSubscribe&Read @------> http://www.hadooptpoint.com/apache-tez-introduction/ /n/r Mapreduce,Hive,Pig,Sqoop,Hbase And More Real Time Interview Questions&Answers :-)\\r\\n\\r\\nhttp://www.hadooptpoint.com/category/interview-questions/ /n/r REPORTING/ Analytical TOOLS:\\r\\nTABLEAU\\r\\nQLIKVIEW\\r\\nHADOOP BIG DATA\\r\\nSAP HANA\\r\\nCLASS ROOM AND ONLINE TRAINING@ BANGALORE @ 09620684481 /n/r How Facebook uses Hadoop and Hive (Interesting Article ) :-) :-)\\r\\n\\r\\nRead & Subscribe @--------> http://www.hadooptpoint.com/facebook-uses-hadoop-hive/ /n/r Introduction to Hive In Simple Way (Exclusive) :-) (y)\\r\\n\\r\\nRead & Subscribe To Our Page ------> http://www.hadooptpoint.com/introduction-hive/ /n/r Hadoop MapReduce Counters\\r\\nclick on this link---->http://www.hadooptpoint.com/hadoop-mapreduce-counters/ /n/r can anyone suggest me some good research areas in data mining for my Ms thesis.I will be thankful. /n/r REPORTING / ANALYTICAL TOOLS\\r\\nHADOOP BIG DATA\\r\\nTABLEAU\\r\\nQLIKVIEW\\r\\nMICRO STRATEGY\\r\\n\\r\\nHADOOP BIG DATA\\r\\nINFORMATICA\\r\\nCLASS ROOM AND ONLINE TRAINING @ BANGALORE @ 09620684481 /n/r How does Hadoop MapReduce Job works\\r\\nclick on this link---->http://www.hadooptpoint.com/hadoop-mapreduce-job-works/ /n/r Hadoop MapReduce Introduction & with example\\r\\nclick on this link--->http://www.hadooptpoint.com/hadoop-mapreduce/ /n/r Hadoop Hive ORC File Format\\r\\nclick on this link----->http://www.hadooptpoint.com/hadoop-hive-orc-file-format/ /n/r REPORTING/ Analytical Tools\\r\\nHADOOP BIG DATA\\r\\nTABLEAU\\r\\nQLIKVIEW \\r\\n\\r\\nSALESFORCE CRM\\r\\nClass room and online training @ banagalore @ 09620684481 /n/r Hadoop Hive Input Format Selection\\r\\nClick on this link-------->http://www.hadooptpoint.com/hadoop-hive-input-format-selection/ /n/r Distributed Execution Engines for solving bigdata problems\\r\\nclick on this link---->http://www.hadooptpoint.com/parallel-distributed-processing/ /n/r REPORTING/ Analytical TOOLS:\\r\\n\\r\\nHADOOP BIG DATA\\r\\nQLIKVIEW\\r\\nTABLEAU\\r\\nTIBCO SPOTFIRE\\r\\nHADOOP BIG DATA\\r\\nSALESFORCE CRM\\r\\nCLASS ROOM AND ONLINE TRAINING @\\r\\nBANGALORE @ 09620684481 /n/r Hive Buckets Optimization Techniques:\\r\\nclick on this link--->http://www.hadooptpoint.com/hive-buckets-optimization-techniques/ /n/r Reporting / Analytics tools:\\r\\nQLIKVIEW\\r\\nTABLEAU\\r\\nTIBCO SPOTFIRE\\r\\nHADOOP\\r\\nSALESFORCE CRM\\r\\nCLASS ROOM AND ONLINE TRAINING @ BANGALORE @ 09620684481. /n/r Introduction to hive partition \\r\\nhttp://www.hadooptpoint.com/introduction-hive-partition-big-data/ /n/r Machine Learning on Bigdata\\r\\nclick on this link--->http://www.hadooptpoint.com/big-data-machine-learing/ /n/r QLIKVIEW\\r\\nTABLEAU\\r\\nTIBCI SPOTFIRE\\r\\nHADOOP\\r\\nSALESFORCE CRM\\r\\nCLASS ROOM AND ONLINE TRAINING @ BANGALORE @ 09620684481 /n/r Top 10 Hadoop Use Cases\\r\\nclick on this link---->http://www.hadooptpoint.com/hadoop-use-cases/ /n/r Top 25 HBase Interview Questions for freshers and experience\\r\\nclick on this link----->http://www.hadooptpoint.com/hbase-interview-questions/ /n/r Preliminary Call for Papers; 6th International Workshop on Advances in Business ICT (ABICT\\'15); https://www.fedcsis.org/2015/abict /n/r Congratulations to the inaugural TEXATA 2014 Big Data Analytics World Champion - St?phane Sbizzera (KPMG France). VIVE LA FRANCE!! Second Place = Kristin Nguyen (HP Labs Singapore). Third Place = Konstantin Tretyakov (University of Tartu, Estonia). Amazing weekend in #BigDataAnalytics #Business #Austin #Texas  Sign up for 2015 www.texata.com /n/r Purchase Review of Predictive Analytics with Microsoft @Azure Machine Learning bit.ly/1yK4730 #MLatMSFT /n/r If you post many public  FB texts our research  is looking for you.\\r\\nEnroll to this academic research, share your public posts and make up to 50$ per upload\\r\\n(For academic research use only)\\r\\nFor more details:\\r\\nwww.semantic-networks.com /n/r Top 25 Hive interview questions for freshers and experience----part2\\r\\nhttp://www.hadooptpoint.com/hive-interview-questions-part-2/ /n/r Introduction to Apache Spark...click on below link\\r\\nhttp://www.hadooptpoint.com/introduction-apache-spark/ /n/r JdbcConnection for Hive....click on below link\\r\\nhttp://www.hadooptpoint.com/hive-jdbc-connection/ /n/r Top 25  Hive interview questions for freshers and experience\\r\\nhttp://www.hadooptpoint.com/hadoop-hive-interview-questions/ /n/r HADOOP (BIG DATA),\\r\\nQLIKVIEW\\r\\nTABLEAU\\r\\nTIBCO SPOTFIRE\\r\\nCLASS ROOM AND ONLINE TRAINING @ BANGALORE @ 09620684481. /n/r Top 25 Pig Interview Questions For Freshers and Experience :-)\\r\\n\\r\\nRead & Register @------> http://www.hadooptpoint.com/pig-interview-questions/ /n/r we can store data into HBase but retreive is difficult because most of the people know SQL queries.There is a one solution for that.The solution is Apache Phoenix\\r\\n\\r\\nhttp://www.hadooptpoint.com/apache-phoenix-over-hbase/ /n/r I have done webCrawling using Jsoup. Now I want a ready-mate tool for web-crawling. Please help me if u know  any best tool for this friends. /n/r Invite your friends in Texas/USA along to the Sunday Awards Ceremony from 2:00pm-5:00pm at the University of Texas at Austin on Sunday 23rd November 2014.  If you love Big Data & Analytics - it\\'s Free and a great networking and career opportunity:  http://www.texata.com/livefinals/ /n/r Top 25 Frequently Asked Interview Questions On Big Data Hadoop :-(\\r\\nRead&Register @----> http://www.hadooptpoint.com/bigdata-hadoop-interview-questions/\\r\\n#bigdata #hadoop #interview /n/r simple explanation and advantages of HDFS\\r\\nhttp://www.hadooptpoint.com/hadoop-distributed-file-system/ /n/r Hadoop Interview Questions for freshers and experience\\r\\nhttp://www.hadooptpoint.com/bigdata-hadoop-interview-questions/ /n/r *How Facebook uses Hadoop and Hive*\\r\\n\\r\\nhttp://www.hadooptpoint.com/facebook-uses-hadoop-hive/ /n/r Apache Phoenix is a SQL skin over HBase........,,,,\\r\\nhttp://www.hadooptpoint.com/apache-phoenix-over-hbase/ /n/r I want to fetch all the product details. My concern is that I get the details of the loaded products(one page that is shown) only, When I scroll down the page ,  no. of product inceases . But I want to fetch all the product details without scrolling the page. /n/r BIg data hadoop Mapreduce Java Programs on Real Time Flights Data :-)\\r\\n\\r\\nhttp://tutorialshadoop.com/big-data-hadoop-mapreduce-java-programs/ /n/r Hi All,\\r\\n\\r\\nThis isn\\'t a \\'big\\' data question but I\\'m going to post up here as I think there is not a bad chance that someone here will have some good suggestions.\\r\\n\\r\\nI have some data on a process ... this process occurs in the low five figures of per year and originates in a dozen or so different settings. There are about 8 or so variations of the process itself (some of which even merge but I want to ignore that for now). The process also has a handful of different outcomes. We know the duration of the process for all settings and outcome types although one outcome is \\'un-outcomed\\' which has a duration from origin until \\'today\\' (rolling).\\r\\n\\r\\nThere are some values for how long the process \\'should\\' take but sometimes it takes longer.\\r\\n\\r\\n What I want is one-big-in-your-face graphic that shows all of that.\\r\\n\\r\\nA table won\\'t do it. A crude tree diagram won\\'t do it.\\r\\n\\r\\nI need a tree diagram template that allows me to do the following:\\r\\n\\r\\nFor each origin show branches from a tree with widths determined by the relative frequency of that source. For each sub-type of the process twigs coming off those branches also with their width determined by relative frequency of the whole. Twigs should vary in length with the average duration of that sub-type coming from that origin. When a twig is longer than the recommended time it should change from green to red. \\r\\n\\r\\nDoes anyone know of (preferably free) software that can do this?\\r\\n\\r\\nMichael Gibney /n/r Imperial Society of Innovative Engineers bring one more platform where you can develop you design and manufacturing skills and show your Iron \" ISIE-Indian Karting Race .....\\r\\nGrab this opportunity and be part of this grand event No Virtual Round ..only 70 teams allowed.\\r\\nDownload rule book : http://imperialsociety.in/isie_karting.pdf visit at us: www.imperialsociety.in Event Venue: Kari Motor Speed, Coimbatore dates: January, 2015 contact us for any sort of query: +91-8427417781/9041466699 /n/r Have a number of openings for folks looking to break into the IT industry. Looking for recent college graduates any major. \\r\\nContact; Anil\\r\\n732-791-2971- anild@cloudeeva.com\\r\\nhttp://www.cloudeeva.com/ /n/r More Clouds ...\\r\\n\\r\\nCloud ?konomie\\r\\nhttp://amzn.to/1wkCtsn\\r\\n\\r\\nThe Clouds Economy\\r\\nhttp://amzn.to/1uC7Yhx\\r\\n\\r\\nEconom?a de las Nubes\\r\\nhttp://amzn.to/Y2ToVc\\r\\n\\r\\nand FanPage\\r\\nhttp://bit.ly/thecloudseconomy /n/r Hello every body\\r\\n\\r\\nI need this program \" SPSS Clementine\"\\r\\n\\r\\nor any other program that I can use for Data Mining\\r\\n\\r\\nAny one can help me, please ?! /n/r 2-Day Workshop on SEM and its Applications at University of Malaya, Kuala Lumpur (27th - 28th September, 2014). We also accept Local order / Purchase order. /n/r HTC Institute of Technology Management & Research\\r\\n FIRST of ITS KIND CAREER-CHANGING DIPLOMA IS NOW WITHIN YOUR REACH. Professional Diploma in Business Intelligence & Analytics University Recognized Program only on Weekends!\\r\\n Talk to our experts\\r\\n HTC Towers, No. 41, GST Road, Guindy Chennai \\x96 600 032\\r\\n +91-9840730610 / +91-9787401008 / +91-9600077954\\r\\n mohammed.samiuddin@htcitmr.ac.in\\r\\nwww.htcitmr.ac.in\\r\\n FB: HTC Institute of Technology Management & Research /n/r How is EDUREKA online training ? Anyone who has taken this course can provide their valuable feedback please. Thank you :) /n/r STATISTICAL ANALYSIS USING SPSS\\r\\nat University of Malaya, Kuala Lumpur\\r\\n(13th to 14th September 2014)\\r\\n\\r\\nTime:\\r\\n9:00 am \\x96 5:00 pm\\r\\n\\r\\nSpeaker: \\r\\nProf. Dr. Ananda Kumar\\r\\nUniversity of Malaya\\r\\n\\r\\nFee:\\r\\nRM 300 / person\\r\\nLocal Order/Purchase Order Accepted\\r\\n\\r\\nFor further information and registration please call us at Tel: +60322422387; Mobile: +60102610787; or email us at register@panoplyconsultancy.com or visit: www.panoplyconsultancy.com /n/r 1-Day Workshop on Microsoft OneNote at University of Malaya, Kuala Lumpur (23rd August, 2014). We also accept Local order / Purchase order. Thank you. /n/r anybody really working on big data projects or its just a hype???? why i am asking this because i am planning to go for hadoop and related technologies would it be worth /n/r Hi,\\r\\n\\r\\nI want to interest you in participating in the project of \"The Clouds Economy\", and thus in promoting knowledge and new cloud solutions among new customers. \\r\\n\\r\\nThe project consists of three components:\\r\\n\\r\\n1. \"The Clouds Economy\" is a richly illustrated knowledge publication describing the multidimensionality of cloud computing. A must read for customers of clouding.\\r\\n2. \"The International Atlas of Cloud Services and Tools\" is an appendix attached to \"The Clouds Economy\". The purpose of the supplement is to create a global, unique discount offer of cloud computing services only for readers / users of the publication. \\r\\n3. CloudsEconomy.com is planned, the extended online version of \"Atlas\". Web-service to be used as Cloud Laboratory allows viewing and testing the latest cloud computing solutions.\\r\\n\\r\\nOctober 2, 2013: First edition of \"The Clouds Economy\" in five languages (DE, EN, ES, PL, RU) in ebook form.\\r\\n1 April 2014: The second edition of \"The Clouds Economy\". The book was published by Academic Press Chiron - Sweden in English as a paperback and ebook. Publisher plans to release the next version of the language in the form of an ebook.\\r\\n\\r\\nIn October 2014, we plan to publish additional special version of \"The Clouds Economy\" in the form of paperback and ebook. A special version will include an expanded version of the \"Atlas\".\\r\\n\\r\\nAs part of the promotion of \"The Clouds Economy\" along with \"The International Atlas of Cloud Services and Tools\" will be distributed free of charge during the following events:\\r\\n\\r\\nMobility    \\r\\nDate: October 23, 2014\\r\\nLocation: Warsaw, Poland\\r\\n\\r\\nCompTIA EMEA Member & Partner Conference\\r\\nDate: November 5, 2014 - November 6, 2014\\r\\nLocation: Queen Elizabeth II Conference Centre, London, United Kingdom \\r\\n\\r\\nIDC Third Platform ICT: The New Enterprise DNA\\r\\nDate: November 27, 2014\\r\\nLocation: Warsaw, Poland\\r\\n\\r\\nIf you are interested in participating in the project, please let me know. I\\'ll send you a booklet. \\r\\n\\r\\nIf you have an account on FB, I invite you to \\'like\\' project Page: https://www.facebook.com/thecloudseconomy\\r\\n\\r\\nCheers!\\r\\n\\r\\n-- \\r\\nMatt Mayevsky /n/r Hurry! Last Few Slots Left! Free Webinar: Data Visualization - How to unlock value in Data \\x96 30th July, 2014, 1 PM EST\\r\\n[RSVP: https://attendee.gotowebinar.com/register/1157063724852071682]\\r\\nHow to join?\\r\\nRegister here to join the webinar: https://attendee.gotowebinar.com/register/1157063724852071682\\r\\n\\r\\nWhen? 30th July, 2014 at 1:00 PM EST.\\r\\n\\r\\nWhere? From the comfort of your laptops, desktops, smartphones etc.\\r\\nConveying meaning in data quickly is the focal point of analytics. Visual analytics helps you discover new relationships in data, prompts you to ask new questions, and helps you convey what you see to others. Join us for this webinar to learn how to unlock the potential of your data using data visualizations. \\r\\nSave the date! 30th July, 2014 (Wednesday), Time: 1 PM EST\\r\\nFor more details and upcoming webinars, stay tuned to our webinar hub page: http://www.perceptive-analytics.com/data-visualization-designer/#webinar\\r\\n\\r\\nWebinar Objectives:\\r\\n\\x95 Understand how to make sense of vast data quickly\\r\\n\\x95 Elicit questions you did not ask before \\r\\n\\x95 Using visualizations to discover new data relationships \\r\\n\\x95 Learn how data visualization can help identify hidden insights in data\\r\\n\\x95 Explore various visualizations hand-picked by experts\\r\\n\\r\\nAbout Speaker:  Chaitanya Sagar, CEO of Perceptive Analytics.\\r\\n(http://in.linkedin.com/in/chaitanyasagar/)\\r\\nChaitanya Sagar is the founder and CEO of Perceptive Analytics. He is a Chartered Accountant (equivalent to CPA) and also holds a MBA from the Indian School of Business. He has a total experience of 15 years serving 300+ clients from medium to large companies in the USA, India, Australia, Europe and Middle East. He is an expert in creating Data Visualizations and has made presentations at international conferences. \\r\\n\\r\\nAbout Perceptive Analytics:  \\r\\n\\r\\nPerceptive Analytics is a Data Analytics company, offering specialized services in Data Visualization, Dashboard Design, Marketing Analytics, Web Marketing Analytics, Spreadsheet Modeling and Application Solutions. We have the reputation of being a trusted advisor with a penchant to deliver compelling value. We help clients unlock hidden insights using our cutting edge data visualizations. The clientele we serviced include a wide range of companies from listed companies to start ups in Silicon Valley to privately owned multi-billion dollar companies. \\r\\nRSVP here: https://attendee.gotowebinar.com/register/1157063724852071682\\r\\n\\r\\nContact Info:  Chaitanya Sagar, cs@perceptive-analytics.com /n/r Quinnox Off-Campus Drive for Freshers on 4th August 2014 \\r\\nhttp://goo.gl/R4VVXx\\r\\n\\r\\nMeridium Inc Hirings Freshers as Software Engineer Trainee.....\\r\\nhttp://goo.gl/D2GCj7\\r\\n\\r\\nWalkin Drive for Freshers on 30th-31st Jul 2014\\r\\nhttp://goo.gl/FpZeo1\\r\\n\\r\\nIBM Off-Campus Drive for Freshers on 31 Jul 2014\\r\\nhttp://goo.gl/HQK1CM\\r\\n\\r\\nSyntel Walkin Drive for Freshers on 31st Jul 2014\\r\\nhttp://goo.gl/sDHlFi\\r\\n\\r\\nChennai Walkin Drive for Testing on 2 Aug 2014\\r\\nhttp://goo.gl/gSoq3D\\r\\n\\r\\nGrapeCity Walkin for Freshers on 2nd Aug 2014\\r\\nhttp://goo.gl/RO4drb\\r\\n\\r\\nWalkin Drive for Freshers on 30th Jul-1st Aug 2014\\r\\nhttp://goo.gl/myjBZ1\\r\\n\\r\\nInnoBeez Walkin Drive for Freshers on 3rd Aug 2014\\r\\nhttp://goo.gl/ldv4eU\\r\\n\\r\\nWalkin Drive for Freshers on 2nd Aug 2014 \\r\\nhttp://goo.gl/KYHp8l\\r\\n\\r\\nAllscripts Hirings Freshers for Associate Software Engineer .......\\r\\nhttp://goo.gl/vZm4Yf\\r\\n\\r\\nVery Good Opportunity for Freshers @ Amazon.\\r\\nhttp://goo.gl/bzScNc\\r\\n\\r\\nAdfactors PR Private Limited Hirings Freshers as Junior Software Engineer ,........\\r\\nhttp://goo.gl/Ch5OEJ\\r\\n\\r\\nStellent Soft Walkin Drive for Freshers on 30th-31st Jul 2014\\r\\nhttp://goo.gl/C4JRnR\\r\\n\\r\\nWipro Walkin Drive for Freshers on 2nd-3rd Aug 2014 ......\\r\\nhttp://goo.gl/FrVbo3 /n/r Only 5 Weeks until Round 1 \\x96 TEXATA Big Data Analytics World Championships 2014. Two Online Qualification Rounds. Live World Finals in Austin Texas. Share with any colleagues or friends that love Big Data. Use 100% Free discount code - TEXATA398646P - if you need it (normally $30 entry). Good luck and register online \\x96www.texata.com /n/r Only 5 Weeks until Round 1 \\x96 TEXATA Big Data Analytics World Championships 2014.  Two Online Qualification Rounds. Live World Finals in Austin Texas. Share with any friends or colleagues that are awesome with big data. Register online \\x96 www.texata.com /n/r Openings for 2012,2013 & 2014 B.Tech/MCA Freshers @ Web Synergies , Hyderabad for Various Skills[C#, ASP.Net, MS SQL ,JAVA, Spring, Hibernate, MS , QL/Oracle, jQuery, PHP, My SQL, JQuery, HTML 5, CSS3, Drupal, Magento, wordpress, MS SharePoint, MS SQL, MS CRM,MS SQL, MS BI (SSIS, SSRS,SSAS), MS SQL, .Net ] ...............\\r\\nhttp://goo.gl/WF7ntj\\r\\n\\r\\nIGATE Walkin Drive for Freshers on 21st-25th Jul 2014 ......\\r\\nhttp://goo.gl/NXemJB\\r\\n\\r\\nCMS Info Systems Walkin Drive for Freshers on 18th-19th Jul 2014\\r\\nhttp://goo.gl/fJatC4\\r\\n\\r\\nPersistent Systems Walkin Drive for Java Developer on 20th Jul 2014\\r\\nhttp://goo.gl/V01Dil\\r\\n\\r\\nAtum IT Walkin Drive for Freshers on 19th Jul 2014 Hyderabad, Andhra Pradesh...............\\r\\nhttp://goo.gl/H47Hr3\\r\\n\\r\\nAricent Walkin Drive for Trainees on 19th Jul 2014\\r\\nhttp://goo.gl/0BMWxY\\r\\n\\r\\n20 Openings for Freshers on 19th Jul 2014 in NCR @ Nacre Software Services, Hyderabad.........\\r\\nhttp://goo.gl/pBgt8I\\r\\n\\r\\nNua Trans Media Walkin Drive for Freshers on 18th-19th Jul 2014 \\r\\nhttp://goo.gl/6yiHJY /n/r 1-Day Workshop on Microsoft OneNote at University of Malaya, Kuala Lumpur (23rd August, 2014). We also accept Local order / Purchase order. Thank You. /n/r #Germany destroyed #Brazil by (7-1). Brazil may be in utter grief & pain after facing their worst-ever defeat in #Fifaworldcup in first semi-final.Team BDI Systems & Technologies Pvt Ltd  #Social #Sentiment Analyzer captures the sentiments of the people all around the world on #Twitter. Check out \\r\\n#BravsGer #bigdata /n/r #Germany destroyed #Brazil by (7-1). Brazil may be in utter grief & pain after facing their worst-ever defeat in #Fifaworldcup in first semi-final.Team BDI Systems & Technologies Pvt Ltd  #Social #Sentiment Analyzer captures the sentiments of the people all around the world on #Twitter. Check out \\r\\n#BravsGer #bigdata /n/r Mastering Data Analysis Using SPSS at University of Malaya, Kuala Lumpur (9th - 10th August, 2014). We also accept Local order / Purchase order. Thank you. /n/r Just following up on big data world championship - register online at http://www.texata.com /n/r I am coming..\\r\\n\\r\\nhttp://www.bigdatapoc.com/ /n/r HOT TECHNOLOGIES IN IT, HADOOP big data CLASS ROOM AND ONLINE TRAINING @ BANGALORE @ /n/r Registrations open for Big Data World Championships - www.texata.com /n/r Workshop on NVivo 10 at University of Malaya, Kuala Lumpur (8th June, 2014) /n/r Final Call for Position Papers; 2014 Federated Conference on Computer Science and Information Systems (FedCSIS); submission deadline: May 23, 2014; http://fedcsis.org/call_for_position_papers /n/r English Writing Workshop Designed for Scientific Manuscripts at University of Malaya, Kuala Lumpur (31st May - 1st June, 2014) /n/r 2-Day MATLAB Workshop for Journal Publication\\r\\nUniversity of Malaya, Kuala Lumpur\\r\\n(24th \\x96 25th May, 2014) /n/r 2- Day Workshop SPSS FOR RESEARCH (Advance Level) at University of Malaya, Kuala Lumpur (21st - 22nd June, 2014). We also accept Local order / Purchase order /n/r 2-Day Workshop on High Impact ISI Journals Writing And Publishing at University of Malaya, Kuala Lumpur (14th - 15th June, 2014) /n/r Hello everyone,\\r\\n\\r\\nI have been given this opportunity to have an interview with a major IT corporate for a Big Data consultant opening. However, my major is in chemistry and I have no idea about the questions I may be asked through the interview, or the skills I should be familiar with (of course I have heard about software and applications such as hadoop). Can anybody here help me out? I appreciate any sort of insight!\\r\\n\\r\\nCheers!!! /n/r can any one please suggest me to join good institute for big data hadoop in Hyderabad ?? /n/r JOB: Statistical Analyst : Must be proficient in SAS (MACROS) and R Language + Strong in statistics. Salary NO BAR Share cv - vivek.shrivastava@innovaccer.com /n/r Call for Position Papers; 2014 Federated Conference on Computer Science and Information Systems (FedCSIS); submission deadline: May 23, 2014; http://fedcsis.org/call_for_position_papers /n/r Scalable Computing: Practice and Experience; Vol 14, No 4 (2013); Table of Contents available at: ttp://www.scpe.org/index.php/scpe/issue/view/114 /n/r 2-Day MATLAB Workshop for Journal Publication at University of Malaya, Kuala Lumpur (24th - 25th May, 2014). We also accept Local order / Purchase order. Please like and share to your friends.Thank you. /n/r How is Bigdata  summer training at HP educational services any  idea ?? cost is 14900-/ for 90 hours training . plz do help me :) Thanks in advance :) /n/r BIG DATA = BIG BUCKS!\\r\\nAttend the BIGDATA Counselling session at our corporate office HTC Towers \\x96 3rd floor, Guindy - Chennai from morning 10.00 AM to 7.00 P.M on all days of the week including Saturday and Sunday or Talk to our experts+91-9840730610 / 9787401008 website http://itmr.ac.in/dip_bd.html or Facebook : HTC Institute of Technology Management and Research /n/r Federated Conference on Computer Science and Information Systems (FedCSIS; http://www.fedcsis.org) submission deadline for ALL events (strict) April 23, 2014 (5 days) /n/r Get trained in Big Data and upgrade your career \\r\\nContact: +91 - 9840730610\\r\\nwww.itmr.ac.in /n/r Workshop on Two-Level of Structural Equation Modeling (Multilevel Modeling-Advanced SEM) using EQS at University of Malaya (17th - 18th May, 2014)\\r\\n\\r\\nTime:\\r\\n9:00 am \\x96 5:00 pm\\r\\n\\r\\nSpeaker: \\r\\nDr. Nasser Alareqe\\r\\nInternational Islamic University, Malaysia\\r\\n\\r\\nFee:\\r\\nRM 300 / person\\r\\nAccepted Local Order/Purchase Order\\r\\n\\r\\n \\r\\nIf you have any questions please contact Ms. Anura Azlan Shah (+60102610787 / +60322422387). /n/r Get trained in Big Data and upgrade your career \\r\\nContact: +91 - 9840730610\\r\\nwww.itmr.ac.in /n/r Call for Papers; Security Transparency in Cloud based Services; Paper submission: April 30, 2014; more info: \\r\\nhttp://www.scpe.org/index.php/scpe/pages/view/Call-issue-2-2014 /n/r Call for Papers; 3rd Workshop on Scalable Computing in Distributed Systems (SCoDiS\\'14) and 8th Workshop on Large Scale Computations on Grids (LaSCoG\\'14); submission deadline: April 23, 2014; http://www.fedcsis.org/2014/scodis-lascog /n/r Registrations Open - Big Data Analytics World Championships 2014\\r\\n\\r\\nInvites just opened for Big Data Analytics World Championships 2014.  \\r\\nTwo Online Rounds. LIVE Top 16 World Finals in Dallas/Austin Texas USA. Starts 23 August 2014.  Register at www.texata.com /n/r Where can i get learning material for HADOOP from basics ? /n/r What is Cloud Computing ?\\r\\n\\r\\nCloud technology  is everywhere, desktops, tablets, smartphones or Netbooks.\\r\\nThe Big Data  concept entwined with Cloud technology is fuelling the technological future. /n/r Call for Papers; Performance of Business Database Applications (PBDA\\'14); submission deadline: April 11, 2014; http://www.fedcsis.org/pbda /n/r FOR JOB - \"Placement Co-Ordinators of all the Engineering Colleges and all B.Tech Freshers 2014 Batch\" through out India We are going to organize an event in mid of April for Recruitment of Interns as well as Full time Employees. Please try to Connect ASAP on LinkedIn (mr.vivekmba@gmail.com ) if willing to Participate and grab a hi speed career in Analytics Industry.\\r\\n\\r\\nJob Title- Data Scientist\\r\\nOpenings- 50 + Positions\\r\\nMinimum Salary - 5 LPA\\r\\nIndustry- Analytics \\r\\nLocation - Noida\\r\\nProcess- Online Test and Personal Interview /n/r Tending to the needs and wants of your employees is crucial for company success and growth. /n/r Hi Folks! \\r\\nWhy Do We Need Big Data?  #bigdata #cloudcomputing #CRMS6 /n/r Workshop on Two-Level of Structural Equation Modeling (Multilevel Modeling-Advanced SEM) using EQS at University of Malaya (12th - 13th April, 2014).We also accept Local order / Purchase order. Please like & share to  your friends.Thank you. /n/r Workshop on Two-Level of Structural Equation Modeling (Multilevel Modeling-Advanced SEM) using EQS at University of Malaya (12th - 13th April, 2014).We also accept Local order / Purchase order. Please like & share to  your friends.Thank you. /n/r Dear JOB SEEKERS Please connect with me on LinkedIN to Explore NEW Job Opportunities ! \\r\\nDATA SCIENTIST LEAD / Analyst LEAD , Exp 2 to 3 years ! SALARY NO BAR , NOiDA LOCATION ! \\r\\nDATA QUALITY MANAGER - SALARY NO BAR APPLICATION DEVELOPER - SAlaRY - NO BAR \\r\\nUI/UX DESIGNER - SALARY UPTO 5 LPA S\\r\\nSTATS HEAD - SALARY NO BAR \\r\\nMARKETING MANAGER- SALARY NO BAR \\r\\nPYTHON DEVELOPERS - FRESHER \\r\\nDATA SCIENTIST FRESHER Many Other Openings ! HURRY UP ! /n/r 1-Day workshop on Writing Research Proposal /Thesis\\r\\nat University of Malaya, Kuala Lumpur\\r\\n(5th April, 2014)\\r\\n\\r\\nTime:\\r\\n9:00 am \\x96 5:00 pm\\r\\n\\r\\nSpeaker: \\r\\nDr. Nasser Alareqe\\r\\nInternational Islamic University, Malaysia\\r\\n\\r\\nFee:\\r\\nRM 180 / person /n/r SAP IDES Server Access | SAP Training Videos | SAP Online Training\\r\\n\\r\\nHANA 1.0 SP 7 | BPC 10.1 | GRC 10 | BO BI BW | CRM 7.0 | SRM | SCM | IS Oil & Gas | IS Utilities | IS Retail | FICO | ABAP | BASIS | SECURITY | ECC 6.0 EHP6 & Many More..,\\r\\n\\r\\nSAP IDES Server Access 24/7 server support ,Certification Materials\\r\\n\\r\\nPlease Contact Us : Email : erpwebaccess@gmail.com , Call : USA : +1 908 982 1555, IND :+91 9885078067 , Skype : erpwebaccess /n/r SAP IDES Server Access | SAP Training Videos | SAP Online Training\\r\\n\\r\\nHANA 1.0 SP 7 | BPC 10.1 | GRC 10 | BO BI BW | CRM 7.0 | SRM | SCM | IS Oil & Gas | IS Utilities | IS Retail | FICO | ABAP | BASIS | SECURITY | ECC 6.0 EHP6 & Many More..,\\r\\n\\r\\nSAP IDES Server Access 24/7 server support ,Certification Materials\\r\\n\\r\\nPlease Contact Us : Email : erpwebaccess@gmail.com , Call : USA : +1 908 982 1555, IND :+91 9885078067 , Skype : erpwebaccess /n/r Spend your Weekends in ITMR and Learn Big Data \\r\\nContact : +91-9840730610 / 9787401008\\r\\nmohammed.samiuddin@htcitmr.ac.in\\r\\nwww.itmr.ac.in /n/r SMALL SLIDE CAN OPEN BIG DOOR - DIPLOMA IN BIG DATA TRAINING PROGRAM AT YOUR CONVENIENCE !\\r\\nContact : +91-9840730610 / 9787401008 mohammed.samiuddin@htcitmr.ac.in\\r\\nwww.htcitmr.ac.in /n/r Small Slide can open Big Door \\r\\nLearn Professional Diploma in Big data in Weekends & Upgrade your Career!\\r\\nContact +91 9840730610 | mohammed.samiuddin@htcitmr.ac.in\\r\\nhttp://itmr.ac.in/dip_bd.html /n/r Actuary is a business professional who uses mathematics, statistics and financial and investment theory to study uncertain future events, analyze their financial consequences and develop programs\\r\\nto reduce the impact of the associated risks, especially those of concern to insurance companies. /n/r BigData is in real demand today. This technology will add value for the developers, System administrators, System analyst, Marketing analyst, Business analyst, Security analyst, Project Managers & Marketing research people. /n/r What is the difference between Hadoop and Twitter storm?\\r\\nCan we use twitter storm with Hadoop for Real time machine learning instead of Mahout? /n/r hello everybody!! =) there are someone italian? /n/r Dear Sir/Mam you can earn through facebook advertising from your home \"payment guaranteed\" \\r\\nfor details contact 0301-4538149 /n/r Hello Every one I am MCA graduate 2012 passed out student with the knowledge of Bigdata Hadoop Technology. I am Having 6 month hand on Experience so if any find Hadoop jobs for my profile mean please let me know the details to Thiru3355@gmail.com.... /n/r join www.bloodbankcell.com as a volunteer blood donor. You can updated sms,email about who is looking for blood in locality and also healthcare related information. join www.bloodbankcell.com website. now 155 member. are you in our database /n/r join www.bloodbankcell.com as a volunteer blood donor. You can updated sms,email about who is looking for blood in locality and also healthcare related information. join www.bloodbankcell.com website. now 155 member. are you in our database /n/r I am looking for good ways of integrating R and Hadoop. I have read online reviews of different ways of integrating the two but I would love to hear some advice and comments from anyone who has had experiences doing so.  :-) /n/r I am working on a Web Data Extractor which fetches data from the unstructured pages on the web.\\r\\nI\\'m building a \"generic\" one which can fetch data from almost any kind of web page and across multiple pages with few clicks.\\r\\nI\\'ve been able to make the algorithm for getting data but the bigger question in front of me now is : \"What could be done with that data?\"\\r\\n\\r\\nI understand that it depends on the user, how he/she wants to look at the data and analyze it accordingly. But I want to provide some predefined utilities to the user, no matter whether the user fetches data from e-commerce websites or from social media ones.\\r\\nPlease guide me on this! :) /n/r We\\'ve reached 1000 members!  :-D /n/r Hi everybody, \\r\\n\\r\\nI started this group slightly over a year ago in late December 2012 and it has been astonishing to see how quickly the group grew. And I take this as a sign that there is a desire for a forum like this to share ideas with other data scientists and individuals interested in big data.\\r\\n\\r\\nAs the group grew in size, there were gradually more and more posts which were job offers or offers for various forms of training. I decided to allow this because at the time I felt that it these might be valuable resources for members in the community. However, this trend has spiraled out of hand and it now feels like those types of messages have drowned out the fruitful exchange of ideas that we once had in the community.\\r\\n\\r\\nThus, as of Saturday January 11th 2014, any posts that are advertisements will be deleted and will be grounds for banishment from the group. This includes but is not limited to posts about job offers and paid trainings. \\r\\n\\r\\nIf you wish to share information about a free educational resource such as new course on Coursera, edX or Udacity that you plan to sign up for or have taken in the past that is perfectly ok. You can also share about books, blog posts and podcasts that you have found useful so long as it is not self-promotion. Use common sense when sharing with the community.\\r\\n\\r\\nWishing you all the best in your data science journey! - Henrik Nordmark /n/r Hi,\\r\\n\\r\\nAnybody want,\\r\\n\\r\\nHadoop Big-data  latest training videos \\r\\n\\r\\nOracle BPM  11g Training videos\\r\\n\\r\\nOracle SOA developer 11g Training videos\\r\\n\\r\\nsap-security & BI  pl/sql\\r\\n\\r\\nselenium -software testing\\r\\n\\r\\ncore java\\r\\n \\r\\nTraining videos\\r\\n\\r\\nIt contains 40 hours training session\\r\\n\\r\\nYou just send me a  mail to  alya9642@gmail.com\\r\\n\\r\\nNote: Not for Free\\r\\n\\r\\nIn the mail mention your Name,Place of living, and Contact Number !!\\r\\n\\r\\nNot for windows viewers! \\r\\n\\r\\nDon\\'t comment send me a mail only!\\r\\n \\r\\nThank you ! /n/r Hi,\\r\\n\\r\\nAnybody want \\r\\n\\r\\nOracle BPM  11g Training videos\\r\\n\\r\\nOracle SOA developer 11g Training videos\\r\\n\\r\\nOracle Weblogic Admin 11g Training videos\\r\\n\\r\\nOracle OSB developer Video  11g Training videos\\r\\n\\r\\nOracle  AIA  11g Training videos\\r\\n\\r\\nOracle  ODI  11g Training videos\\r\\n\\r\\nYou just send me a  mail to  alya9642@gmail.com\\r\\n\\r\\nNote: Not for Free\\r\\n\\r\\nIn the mail mention your Name and Place of living !!\\r\\n\\r\\nNot for windows viewers! \\r\\n\\r\\nDon\\'t comment send me a mail only!\\r\\n\\r\\nThank you ! /n/r Hi friends,i need one help, i have some data, the data is date wise sales report, i want to predict or forecast , the data having up and down in graph.. so which model is suitable this situation? any body help me.. i need ur helps...   /n/r Hi,\\r\\nAnybody want,\\r\\nHadoop Big-data  latest training videos \\r\\nOracle BPM  11g Training videos\\r\\nOracle SOA developer 11g Training videos\\r\\nYou just send me a  mail to  alya9642@gmail.com\\r\\nNote: Not for Free\\r\\nIn the mail mention your Name,Place of living, and Contact Number !!\\r\\nNot for windows viewers! \\r\\nDon\\'t comment send me a mail only!\\r\\n \\r\\nThank you ! /n/r respected sir\\'s,and friends.i am ramana,i am very new to hadoop.i have the theoretical knowledge of hadoop,but don\\'t have any practical knowledge.i need your help,can any one give me any soft copy of commands and material of hadoop\\'s map reduce,pig,and hive.plzzz share to me,some information guysssss /n/r respected sir\\'s,and friends.i am ramana,i am very new to hadoop.i have the theoretical knowledge of hadoop,but don\\'t have any practical knowledge.i need your help,can any one give me any soft copy of commands and material of hadoop\\'s map reduce,pig,and hive.plzzz share to me,some information guysssss /n/r Hello. I am sadat from Bangladesh. Just recently I have discovered the power of big data and trying to harness its true power. However my technical skills are only limited to SQL. I am thinking to learn hadoop and map reduce. Do you think learning these two will give me any significant positive edge in my career? Will it be at all possible for me to learn all these technical things considering that I am a business graduate? And by learning I mean ms from University. Or should I go for MBA in business analytics (focused for business students). Need your expert opinion on this. Thanks. :) /n/r Celebrating the fact that as of yesterday, I have an exciting new job doing data science for a digital marketing company! :-D\\r\\n\\r\\nAnd as if that were not good news enough... My very good friend and colleague Vladimir, with whom I work so well with has as of today also got a job at the same company as me! \\r\\n\\r\\nI look forward to some very awesome times ahead! :-D :-D :-D /n/r Join Us in the MapReduce Group\\r\\n\\r\\nhttps://www.facebook.com/groups/mapreducegroup/ /n/r Everyone know Hadoop and Mahout can process with very big data. But how big is it? Would you send me some source contains volume of data they are processed. Thank you. /n/r Hello everyone. Would you help me to find some \"related work\" that related to Apache Mahout. Because Mahout is new, so it\\'s difficult to find some research literature related to Mahout. Thank you very much. /n/r I am looking to import a massive  JSON file into a database for analysis. Does anyone have any recommendations for database compatibility with a JSON file? I am going to be doing network analysis from the data and would probably be using graphical interfaces to present the results. Anyone got any thoughts? /n/r Do you like to write Blogs about BI, Analytics and Big Data ? Please let me know.. /n/r Many companies are saddled with data warehouses that weren\\'t designed to handle big data. http://deloitte.wsj.com/cio/2013/07/17/the-future-of-data-warehouses-in-the-age-of-big-data/?mod=wsjcio_hp_deloitte /n/r Hi Guys! thanks for the accept. I would like to apologize in advance if my question would be a bit noob .I think I have a hint of what Big Data is for ( base from random articles I found on the net)  but I\\'m completely confused on Data Science. Do you need heavy mathematical background for it? Sorry for the very noob question. I\\'ll try to read more so that I can contribute to the group :) /n/r Has anyone here read the Nate Silver book yet? If so what did you think? /n/r Would you like to buy an online BI, Analytics and Big Data courses and certification package ? cost around USD $400 total. Please help in answering to this small survey... we are working on it right now and will launch in near future.. just trying to understand that if it is really a good idea or not? /n/r White Paper - 3 Ways to Improve Customer Care Through Data Hygiene  http://goo.gl/6IXgR /n/r What would people say are some of the seminal papers in the Big Data field, both classically, presently and considering the future of the field. /n/r Thank you for letting me in on the group!\\r\\nI have no back ground in Big Data so I am looking to chat to this group about how it all works with the knowledge I have on health and predicting health trends based on increasing and decreasing markets, individual probability and predicting health conditions and symptoms (genetic predisposition) example of my Back ground on this subject... My presentation to the Scottish Respiratory Nurse Forum (SRNF) Conference 16-17th Novenber 2012\\r\\nPresentation title: The Buteyko Method & My Asthma\\r\\nAlex Spence, I am going to talk about my experience with my asthma and what was taught to me 11 years ago and has kept me 99.99% asthma free. \\r\\nCorrecting Two Myths About Breathing\\r\\nThe importance of The Bohr Effect\\r\\nWhy do asthma symptoms come and go\\r\\nHow healthy are you\\r\\nWhat does normal breathing look like\\r\\nGenetic predisposition to Asthma\\r\\nCan Asthma be Cured\\r\\nDominant protective mechanism\\r\\nBreath Increasing Factors\\r\\nBreath Reducing Factors\\r\\nAudience: Respiratory Nurses /n/r For the sake of being very focused with studying, I will be temporarily deactivating my Facebook account for about a month, possibly a bit longer.\\r\\n\\r\\nYou can direct any admin questions to Rene Romero Benavides and Lillian Pierson, who are now also admins for the group.\\r\\n\\r\\nTake care, Henrik. /n/r I would like to hear from anybody in the group who has experience with data science applications with SME\\'s, i.e. Small and Medium size Enterprises.\\r\\n\\r\\nWhat are the needs of an SME in relation to data mining information that is relevant to their company? And from what you\\'ve experienced, is there an awareness of those needs or is it the case that the SME needs to be educated about the benefits of data mining?\\r\\n\\r\\nI would imagine that SME\\'s in different industries would have both different needs and different levels of awareness of how those needs could be met through the power of data science techniques.\\r\\n\\r\\nAny input on this would be greatly appreciated!\\r\\n\\r\\nKind regards,\\r\\nHenrik. /n/r How can I find the dataset, source code to implement about Collaborative Filtering and recommendation system, or another subject related big data, data mining. ? \\r\\nThank you so much. /n/r Hi Henrik , all.Thanks for your approval on my joining request. I am really interested in data driven research and decision making. I have been trained as architect and urban planner, and currently im phding on urban studies on the topic of data driven decision making in urban management. I will be happy to learn about any question or comment you may have on my research subject, and i\\'m very much looking forward for any opportunities that may come from this interesting group. Wishing you all a nice day. /n/r Hi, is anyone out here a mobile app programer? Or at least a java programer who wants to experiment with apps. I have an interesting project I want to work on but my java dyslexia is inhibiting my progress. Would anyone like to cooperate. We can discuss incentives privately. /n/r Funny slide to break the ice for a Big Data presentation. /n/r Not sure I agree with this diagram entirely... but it does provide an interesting way of thinking about Data Science. ;-) /n/r hello ,Please tell me about different data mining tools which available. Please provide supported links and tools.\\r\\nExample :\\r\\nBinning method (Bin means)\\r\\ndata smoothing\\r\\nFinding missing value /n/r Hi! Could you suggest me some research point about Collaborative Filtering?\\r\\nThanks. /n/r Share your Linked In profile page: /n/r Greetings, I have been working with data in a tangential way. Mostly geospatial analysis, crime analysis and, descriptive statistics of educational data. I would like to pursue a path as a data analyst. I realize that SQL skills are very important and I have studied this in the past but I really need to brush up. Are there any free online courses that teach you SQL and provide practice databases to query? Also, which job roles should I look out for if becoming a data analyst is my ultimate goal? It seems companies want a lot of experience before considering you for this role. Thanks in advance for the feedback. /n/r Very soon, harnessing the power of information won\\'t just be a matter of profitability, but of survival. Find out about Microsoft tools to help your business thrive in the age of big data. /n/r Would you let me know What is the \\'data sparsity\\' term?\\r\\nThanks all. /n/r Check these guys out... Applying machine learning and topology to big data: http://www.wired.com/design/2013/01/data-viz-ayasdi-iris/ /n/r So we have a small group going of people interested in data science, big data and statistics.  :-)  I would love to know what you are all up to, in which contexts you have been or are interested in applying data science techniques. It would also be great to know a little bit about your backgrounds.\\r\\n\\r\\nI am originally a mathematician / logician by training but I have been moving more recently in a more applied direction using statistics to solve real world problems. I am currently working with Vladimir Metodiev on a project at a textile company.\\r\\n\\r\\nWhat interesting things have you been up to in data science?   :-) /n/r Sorry if this is a double posting or redundant in any way. Saw it and I thought I would share with group... \\r\\n\\r\\nBig Data Computerworld Strategy Guide\\r\\nPerhaps you\\'ve heard that the next new thing in IT is \\'big data\\' and concluded that the hype-cycle machine is turning out another attention-getter. I\\'m not big on predicting paradigm shifts, so I won\\'t in this case. But I will say that if you\\'re an IT professional, you ignore big data at your peril. I believe this one is all it\\'s cracked up to be and more. Read more. /n/r We\\'ve just hit 80 members! Woohoo! :-) /n/r Wow, there are a lot of new members! We\\'ve gone from a handful of people when I created the group 4 days ago to over 50 members today!  :-D\\r\\n\\r\\nA very warm welcome to all of you who have just recently joined! I hope you find this to be a useful forum in which to share ideas, questions, techniques and in which to collaborate with others interested in Data Science. <3\\r\\n\\r\\nIf you were dragged in here by a friend and are not that interested in being part of this group, there are no hard feelings in you choosing to leave. \\r\\n\\r\\nThe goal is for this community is to thrive with engaged members that actively interact and support one another in whatever way possible independently of whether the community is large or small.\\r\\n\\r\\nSo far we have gone through an initial round of introductions and we have begun to compile a list of resources for Data Science. We have thread on free online courses in statistics, SQL, Hadoop, R and related topics. And we have an analogous thread on recommended books related to these topics.\\r\\n\\r\\nIf you haven\\'t already done so, I would encourage you to scroll to the introductions thread and just say a few words about your background, your data science interests and any interesting data driven projects you have been involved in just to get a flavor for what everybody is up to.  :-) /n/r This is a forum for Big Data, Data Science, Data Mining and Statistics. /n/r Welcome to the Big Data, Data Science, Data Mining & Statistics group! /n/r '"
      ]
     },
     "execution_count": 246,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "male_text = \"\"\n",
    "for i in range(len(data_male)):\n",
    "    male_text = male_text + data_male[i] + \" /n/r \"  \n",
    "male_text "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we clean the text and remove any unwanted characters. also applying lower case. Then we separate the words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Does Gmail sell information is one\\'s private emails. Judging by adverts in my Facebook news feed, I would say \"Yes.\" But perhaps this isn\\'t news for anyone and I have been under a rock for years. Clarification, please. Is private email private in name only? /n/r Hold the applause. The new rocket has two main purposes, neither of them connected with the development of science: to boost the obscene personal wealth of Musk, currently more than $21 billion, and to expand the military arsenal of American imperialism.\\n\\n\\'Falcon Heavy launch marks new stage in the privatization\\x97and perversion\\x97of space exploration\\'\\nWorld Socialist Web Site\\n\\nhttp://www.wsws.org/en/articles/2018/02/09/falc-f09.html /n/r Fake or real followers? /n/r THIS is where science is needed. HERE ON EARTH! /n/r If you bombard the Earth with photons for a while, it will emit a Tesla Roadster-st /n/r Wake Up America, the Original, the Original .. America is \"Fighting The Wrong War\"!  We need a \"War On Killer Diseases\"!! /n/r Check out that lenticular cloud we caught over Mt. Hood, Oregon, USA yesterday, Feb 8, 2018 morning during #sunrise. #omht #StormHour\\n\\nhttps://twitter.com/inMtHood/status/962053037031174145 /n/r how does amazon go works? /n/r God is a prankster, he gives you instincts, he gives you this and he sets the rules. And while he looks, he is laughing his ass off. He set the rules of all time. He\\'s a tight ass, he\\'s a sadist. He\\'s an absentee landlord.\\n\"The Devil\\'s Advocate\"\\nhttps://youtu.be/O4irXQhgMqg?t=2 /n/r My hat off to #teamcanada.\\nBecause these are the ideals I came to the US for. Maybe I aimed a bit low (I am a competitive archer, but not an olympian after all). :) /n/r This reminds me of the first glimpse we got of Darth Vader in the second Star Wars movie, \"The Empire Strikes Back\":\\n  it was a \\'peek-a-boo\\' shot from behind as his helmet was slowly lowered onto his disfigured and scarred head. /n/r Demonstration of intelligence ? /n/r ++Don\\'t miss the chance++\\n#Oral #presentation #slot #available \\n#Grab your slot: https://goo.gl/uHQvZ1 \\n#Molecular #Immunology #Conference  #London /n/r The Mandelbrot Set, a pathological curve.... /n/r \"Is A Little Less American Evil Really Too Much To Ask For?\"\\n\\nDamn Good Question if you ask me.\\n\\n\"\\n\\nIs it too much to ask for the US not to drone-bomb innocent civilians or torture suspected enemy combatants in Guantanamo?\\n\\nI am frequently criticized of being too idealistic and too judgmental when it comes to politicians, whether Democrat or Republican. They\\'re just human beings, people tell me. They\\'re doing their best. Why am I so hard on them?\\n\\nBut the criticisms that I wage hardly seem like too much to ask. For example, don\\'t destroy the entire countries like Libya and Syria. Don\\'t kill hundreds of thousands of people in Iraq. Don\\'t torture them at Guantanamo. Don\\'t give massive tax cuts to the wealthy. Don\\'t hang out with rich people who want to comprise you. And don\\'t accept their contributions.\\n\\nThis is too much to ask?\\n\\n? Ted Rall /n/r Notice to all members:\\nFirst instance of obviously off-topic posting: \\n-Will be deleted.\\nSecond instance of obviously off-topic posting: \\n-Member will be banned.\\nBLOCKING of group admins is NOT allowed.\\nA member that blocks an admin may be warned to unblock.\\n-If member does NOT UNBLOCK, member will be banned.\\nThe group admins are:\\n  Jill Perlman.\\n  M Patricia McLaughlin.\\n  Mi Robin.\\n  Kathy Henderson.\\n  Steve Cooperman.\\n  Bob Kerns.\\n  Steve Hicks\\n  Kevin G. Rhoads, and \\n  William A. Boyle.\\nNew members:\\n-PLEASE read the group description!\\n(pic thanx to Sargon Agade!) /n/r ACLU Action\\n\\nHi Jerald \\x96\\n\\nBig news for drug policy reform: After California legalized recreational marijuana use, San Francisco followed suit by clearing thousands of marijuana convictions dating back 40 years. This is a step in the right direction \\x96 now people won\\'t be held for crimes that are no longer crimes. \\n\\nThe outdated policies of the War on Drugs have done little but waste federal resources and harm lives forever. But while some of us are fighting back, others \\x96 like Attorney General Jeff Sessions \\x96 are doggedly pursuing the same failed policies. His vendetta against marijuana and people who use it goes against public opinion and completely disregards the human consequences of these unjust laws. \\n\\nBut we can stop him. There\\'s a new bill in Congress to legalize marijuana under federal law \\x96 and remedy some of the injustices of the War on Drugs. We need your help to make sure it becomes law.\\n\\nTell your members of Congress to support the Marijuana Justice Act and reform our broken criminal justice system.\\n\\nHow do we know that reform is needed? The data says it all. In 2013 we found that Black people are almost four times more likely to be arrested for marijuana possession even though Black and white people use marijuana at comparable rates.\\n\\nWhat\\'s more, 64 percent of Americans approve of legalizing marijuana. But Jeff Sessions\\' policies go against the will of the people.\\n\\nLet\\'s tell Congress to pass the Marijuana Justice Act NOW to promote equitable and proportional justice. Antiquated and out-of-touch drug laws \\x96 like Sessions\\' marijuana policies \\x96 have got to go.\\n\\nWith your help, we can make sure Congress listens to us and does the right thing.\\n\\nThanks for standing up with us,\\n\\nJesselyn McCurdy\\n\\nJesselyn McCurdy\\nACLU Deputy Legislative Director, fighting for criminal justice reform /n/r the eye often perceives what the brain has already seen!? /n/r Hey! ...FREE! :O /n/r A potential area of research. /n/r A BRITISH APPEALS COURT on Monday rejected demands from the U.S. Government for the extradition of accused British hacker, Lauri Love, citing the inability of U.S. prisons to humanely and adequately treat his medical and mental health ailments. Extradition to the U.S., the court ruled, would be \"oppressive by reason of his physical and mental condition.\" /n/r Blowing snow with very low ground level visibility made for a perfect winter day for photography. Eastern Manitoba, Canada Feb 1st, 2018 #sundog #Manitoba #Weather #StormHour @ctvwinnipeg @CNN @TheWeatherNetUS\\n\\nhttps://twitter.com/BrentMckean501/status/961052879992905735 /n/r altruism |?altro?o?iz?m|\\nnoun\\n\\x97 the belief in or practice of disinterested and selfless concern for the well-being of others /n/r My son and I.... /n/r One of the perks of being lodged at a #rorbuer, Lofoten Islands, Norway is that you can see the Aurora right from your balcony ~ Thanks to @DavidRocaberti #Auroraborealis #StormHour\\n\\nhttps://twitter.com/StormHour/status/960634125362266118 /n/r How the diet changed the Neanderthals.\\nNeanderthals dominated the territory of modern Europe for over 200,000 years. They were perfectly adapted to the conditions of the ice age, but disappeared somewhere 20,000-30000 years ago. They were replaced by Cro-Magnens, who became the progenitors of modern people.\\nHowever, it is proved that the genes of Neanderthals are in the genome of modern man.\\nThe main part of the Neanderthal diet was meat. These are proteins and fats.\\nOnly in the liver is glycogen in a relatively large amount.\\nProceeding from this, gluconeogenesis was very characteristic for the metabolism of Neanderthals. Vegetable food accounted for a very small percentage of their diet.\\nThe diet of Neanderthals was very similar to the diet of modern brown bears: meat, fish, snails, berries, roots and leaves of plants.\\nWith the melting of glaciers, vast spaces of free land opened up, which were filled with plants, including berries: raspberries, blueberries, acorns. They can be dried and mixed with meat. The Indians still know the recipe for phemikana.\\nHowever, an increase in the percentage of carbohydrates in the diet led Neanderthal women to polycystic ovary syndrome and a decrease in fertility.\\nIt has already been proved that the Neanderthals crossed with the Cro-Magnens.\\nWomen with part of the genes of the Cro-Magnens were more fertile, because the Cro-Magnens lived in a warmer climate and consumed more carbohydrates. Gluconeogenesis in them was less pronounced, fertility is higher, their descendants and replaced net Neanderthals can and pure Kromiens who could not tolerate the cold. There was a highly adaptive race that occupied all the earth and displaced all other hominids.\\n??? ????? ???????? ??????????????.\\n????????????? ?????????????? ?? ?????????? ??????????? ?????? ????? 200000 ???. ??? ???? ????????? ????????????? ? ???????? ??????????? ???????, ?????? ??????? ??? ?? 20000-30000 ??? ?????. ?? ??????? ???????????, ??????? ????? ????????????? ??????????? ?????.\\n?????? ????????, ??? ???? ?????????????? ????????? ? ?????? ???????????? ????????.\\n???????? ????? ????? ????????????? ?????????? ????. ??? ????? ? ????.\\n?????? ? ?????? ???? ???????? ? ???????????? ??????? ??????????.\\n?????? ?? ????? ????????????? ??? ????? ?????????? ??? ??????????? ??????????????. ???????????? ???? ?????????? ????? ????? ??????? ? ?? ???????.\\n?????? ?????????????? ??? ????? ????? ?? ?????? ??????????? ????? ????????: ????,????, ??????, ?????, ????? ? ?????? ????????.\\n??? ?????? ???????? ??????????? ???????? ???????????? ????????? ?????, ??????? ??????????? ??????????, ? ??? ????? ? ????????: ???????, ????????, ?????????. ?? ????? ?????? ? ????????? ? ?????. ??????? ?? ??? ??? ????? ?????? ????????. \\n?????? ?????????? ???????? ????????? ? ??????? ???????? ? ?????? ?????????????? ? ???????? ????????????? ???????? ? ???????? ????????????. \\n??? ???????? ???,????????????? ???????????? ? ?????????????.\\n??????? ??????? ????? ????? ???????????? ???? ????? ?????????, ?????? ??? ??????????? ???? ? ????? ?????? ??????? ? ?????????? ?????? ?????????. ????????????? ? ??? ??? ????? ???????, ???????????? ????,?? ??????? ?  ????????? ?????? ?????????????? ????? ? ?????? ????????????, ??????? ?? ?????????? ?????. ???????? ?????? ?????????? ????, ??????? ?????? ??? ????? ? ????????? ???? ?????? ?????????. /n/r Trump\\'s ascension to the leader. /n/r Here is a short list of items that may contain cochineal-derived colorant: /n/r \"Equifax has said it is under investigation by every state attorney general and faces more than 240 class action lawsuits.\"...\\n\\nOnce upon a time a headline like this would have been unbelievable,\\n  but w/the bastards \\'running the show\\' today, No Longer.\\n\\n*\\n\\nEquifax (EFX.N) said in September that hackers stole personal data it had collected on some 143 million Americans. Richard Cordray, then the CFPB director, authorized an investigation that month, said former officials familiar with the probe.\\n\\nBut Cordray resigned in November and was replaced by Mulvaney, President Donald Trump\\'s budget chief. The CFPB effort against Equifax has sputtered since then, said several government and industry sources, raising questions about how Mulvaney will police a data-warehousing industry that has enormous sway over how much consumers pay to borrow money.\\n\\nThe CFPB has the tools to examine a data breach like Equifax, said John Czwartacki, a spokesman: \"The bureau has the desire, expertise, and know-how in-house to vigorously pursue hypothetical matters such as these,\" he said... /n/r It looks like a welcoming place! /n/r Using a sophisticated technique called extra galactic micro lensing, astronomers have detected ( not observed ) over 2,000 planets outside our own Milky Way galaxy.\\n\\nThe planets orbit stars in a galaxy some 3.8 billion light years away from the earth, so their discovery is quite a stunning achievement.\\n\\nCommon sense told us these planets existed, but to have evidence is always comforting. /n/r Is This For REAL?! /n/r Here are 77 Facts That Sound Like Huge Lies\\nBut Are\\nActually Completely True.\\n:\\nGet ready to have your mind blown into a\\ndifferent time zone.\\n1. If you put your finger in your ear and\\nscratch, it sounds just like Pac-Man.\\n2. The YKK on your zipper stands for \"Yoshida\\nKogyo Kabushikigaisha.\"\\n3. Maine is the closest U.S. state to Africa.\\n4. Anne Frank, Martin Luther King Jr., and\\nBarbara Walters were born in the same year,\\n1929.\\n5. The name Jessica was created by\\nShakespeare in the play Merchant of Venice.\\n8. Cleopatra lived closer to the invention of the\\niPhone than she did to the building of the\\nGreat Pyramid.\\n9. Russia has a larger surface area than Pluto.\\n10. Saudi Arabia imports camels from\\nAustralia.\\n11. Hippo milk is pink.\\n12. The toy Barbie\\'s full name is Barbara\\nMillicent Roberts.\\n13. Woody from Toy Story has a full name too\\n\\x97 it\\'s Woody Pride.\\n14. And while we\\'re at it, Mr. Clean\\'s full name\\nis Veritably Clean.\\n15. Oh, and Cookie Monster\\'s real name is Sid.\\n16. Carrots were originally purple.\\n17. The heart of a blue whale is so big, a human\\ncan swim through the arteries.\\n18. Vending machines are twice as likely to kill\\nyou than a shark is.\\n19. Home Alone was released closer to the\\nmoon landing than it was to today.\\n20. Oxford University is older than the Aztec\\nEmpire.\\n21. Not once in the Humpty Dumpty nursery\\nrhyme does it mention that he\\'s an egg.\\n22. France was still executing people with a\\nguillotine when the first Star Wars film came\\nout.\\n23. Armadillos nearly always give birth to\\nidentical quadruplets.\\n24. Betty White is actually older than sliced\\nbread.\\n25. The unicorn is the national animal of\\nScotland.\\n26. A strawberry isn\\'t a berry but a banana is.\\n27. So are avocados and watermelon............\\n28............. Visit this page #Vanlex_KoluS to get the complete version \\nAnd also get more interesting topics to discuss & read. Thanks \\n.............Vanlex KoluS............. /n/r From #immunotherapy to the #microbiome, one #scientist\\'s journey to find a #cure for #cancer- Dr. #Jennifer #Wargo  https://goo.gl/uxTT6J /n/r Today\\'s sunrise and stunning sun pillar above Cape Cod National Seashore in Wellfleet, Massachusetts, USA, Fev 5, 2018. Thanks to Dapixara @dapixara #Sunpillar #StormHour\\n\\nhttps://twitter.com/StormHour/status/960301020847988736 /n/r We cease to question. /n/r In Three years the Routine should Stop. /n/r Thank you for adding me to the group /n/r Science is timeless, bitches! ?\\n\\nhttps://m.facebook.com/story.php?story_fbid=1194499744028087&id=773558749455524 /n/r Here\\'s a mind blower... check it out!\\n\\nhttps://www.facebook.com/groups/1846743545583087/permalink/2034887416768698/ /n/r The movie \"Brazil\" /n/r Rest from Work-Van Gogh /n/r Facebook\\x97working closely with intelligence agencies and government\\x97is seeking to leverage its role as a mechanism of communication to become an instrument of censorship and repression. In the process, it is turning one of the most  liberating technological advances of the century, the growth of artificial intelligence, into a mechanism for police control and dictatorship.\\n\\n\\'From Facebook to Policebook\\'\\nWorld Socialist Web Site\\n\\nhttp://www.wsws.org/en/articles/2018/02/02/pers-f02.html /n/r Mathematical Entertainments   FREE! /n/r Details .... /n/r My fighter pilot buddy, boots, took this pic out the window of his F-18 of Von Karman Vorticity creating an eddy off the coast of San Diego Feb 1st, 2018. Great shot of a cool phenomenon I\\'ve just learned a lot about! @NWSSanDiego can\\'t say I\\'ve seen this in Iowa before\\n\\nhttps://twitter.com/WXMegs/status/959638888993173506 /n/r *Hallelujah!*\\n  \"God Bless Us, Every One.\"\\n\\nIt\\'s time once again for another *31 Days of Oscar* on Turner Classic Movies (TCM);\\n  Scores of the greatest films Ever Made 24/7!\\n\\n... How I *Love* TCM...\\n\\nIt\\'s the 3rd day into this film festival and I didn\\'t know. (I\\'ve been catching up on DVDs...)\\n\\nJust saw John Ford\\'s \"She Wore a Yellow Ribbon\" for the first time in a decade or two. WOW...\\n\\n*\\n\\nI never was much a fan of the Academy Awards themselves, but I\\'m expecting many moments of genuine happiness in the 28 Days to come,\\n  and, as always at this time, my DVR is going to be MIGHTY Busy. /n/r A reminder for Super Bowl weekend that like the church, the NFL doesn\\'t pay taxes. /n/r The United States has become a very machiavellian society. /n/r I captured this glorious lunar halo a couple of days ago at Paranal Observatory, Chile. You can see all our telescopes (except VISTA), including UT4 and its four mighty lasers. #astrophotography #stormhour\\n\\nhttps://twitter.com/astro_jcm/status/959402120054222848 /n/r Is the \"privatised\" part at fault?\\nWhat happens if you provide greedy people the means to exploit people who do not have the means to defend themselves?\\nThe system worked by \"bringing in private contractors to build and\\nmaintain the system and collect the penalties it ascribed,\" :O /n/r An Image from the Spitzer Telescope... /n/r Tireless worker even away from home, will it also be in the future for humans? /n/r THE END OF EVERYTHING, as if you didn\\'t have enough to worry about....OY Vey...\\n\\nEinstein\\'s famous equation, E=MC squared implies that energy and matter (mass) are interchangeable, that one can be converted to the other and that the amount of energy obtainable from the destruction of a given amount of mass equals the amount of mass times the speed of light squared. It also implies that the sum of matter plus energy never changes.\\n\\nIn nuclear reactions, ( both fission, where atoms are split and in fusion, where atoms are combined ) energy is produced in just this very way...by the complete annihilation of matter.\\n\\nAll the stars in the universe, including our own sun, of course, use fusion to produce energy. Deep in the interior of stars, atoms of hydrogen are combined or FUSED into helium. But the mass of the resulting helium is less than the mass of the hydrogen, and it is that \"missing\" mass that is turned into energy.\\n\\nStars may be thought of as islands of intensely concentrated energy in a vast interstellar void. It\\'s a system, though, a universe, whose organization ( pockets of energy here and nothingness there ) is being constantly eroded, dismantled, degraded.\\n\\nIn other words, the degree of organization of the universe is constantly on the decrease, while the overall disorder, called entropy, is inexorably on the the rise.\\n\\nEventually this disorder, entropy, will reach a maximum as all the stars inevitably exhaust their supply of fuel and cease producing energy altogether. \\n\\nThey will shine no more. There will be no more \"islands of energy\"...no concentration of energy here and nothingness there. What scientists call the temperature gradient will cease to exist and that means no more usable energy available for ANYTHING.\\n\\nInstead, there will be a uniform sameness throughout the cosmos. The temperature from one \"end\" of the universe to the other will be the same...just a few degrees above absolute zero. \\n\\nAppropriately enough, this is what scientists call the \"heat death\" of the cosmos. And it will be the end of absolutely EVERYTHING. There is, however, a great unknown in this dooms day scenario...dark energy and its role in the ultimate fate of the universe.\\n\\nWill this enigmatic \"stuff\" change the outcome?\\n\\nNobody knows.\\n\\nSome say, yes, the the cosmos will end with a whimper as in the heat death scenario described above...in other words, the end will come with a quiet \"whimper\". Others say, no the universe will continue expanding forever. Still others say, the universe will collapse into a singularity, the so-called \"Big Crunch\".\\n\\nRegardless, whatever the fate of the universe...the ultimate catastrophe, if it happens at all won\\'t likely occur for billions, if not trillions of years. So no need to worry.\\n\\nAnd if it continues expanding indefinitely, well, we need not worry about that either, cuz that portends nothing catastrophic...as far as we know now. /n/r The only thing that\\'s changed since then is it\\'s gotten worse. /n/r Blue moon over Blackpool, UK, Jan 30, 2018. How it might have looked if it wasn\\'t for clouds lol #StormHour #BluemoonEclipse #supermoon uk #luna #moon\\n\\nhttps://twitter.com/Stephencheatley/status/958837349487693824 /n/r Reported post deleted due to conspiracy theory and non-factual sourcing. Member warned. /n/r 9th #Molecular #Immunology & #Immunogenetics #Congress #March 08-09, 2018, #London, UK\\n10% - 15% #Discount for #Group #Bookings \\n#Contact here \\n#Email: molecularimmunology@immunologyconferences.org \\n#Ph: 0-800-014-8923, 1-888-843-8169\\n#Web: https://goo.gl/vbyS6G /n/r I enjoy seeing/hearing Good News for a change,\\n? and Glennzilla is an ideal deliverer of This news. /n/r What is the best part of your job? For me, it\\'s giving away money to people from different countries who need help. It takes a lot to get there. Not my company but my society. I\\'m happy for that. /n/r Check them out! :) /n/r Ah-rooooo baby, Australia, Jan 31, 2018. #moon #Eclipse2018\\n\\nhttps://twitter.com/MossyGene/status/958673787041464322 /n/r The moon sets over Mount Susitna in this view from Point Woronzof in Anchorage, Alaska, USA on January 30, 2018. (Marc Lester / ADN)\\n\\nhttps://twitter.com/adndotcom/status/958542319774449665 /n/r \"Modern science is the mathematical description of natural phenomena;\\nMathematics is the systematic study of patterns (in time, in space, or in structures).\"  WAB. /n/r Your thoughts...The osculatory resonance of spiritual enhancements portend advances in geometric syllogisms, provided substitutions in existentialism don\\'t preclude duality.\\n\\nDoug Hullander /n/r Have you seen this post? /n/r After his photo op showing him \"working\" on a completely clear desk, while grasping his phone landed with a thud and a laugh, tRump resorted to digital tech to come up with an alternative photo.   https://www.facebook.com/photo.php?fbid=10213245085714438&set=a.10200471450861550.1073741831.1615423959&type=3&theater /n/r Our beliefs, no matter how intimately and fervently they\\'re embraced, never change the substantiated facts\\n\\nBut facts, when confirmed by evidence, should always change our beliefs...IF we\\'re courageous enough to apply logic and reason. /n/r Hi everyone, who has heard about Sophia the AI robot. She\\'s pretty amazing. Lemme know what you think about her in the comment box below. \\nAnd before I forget, she was also given a citizenship @ Saudi-arabia. \\nThat bot is really amazing. /n/r This article is fro PROGRESS REPORT.\\n\\nJanuary 29, 2018\\n \\nINFRASTRUCTURE SCAM.\\nDuring tomorrow\\'s State of the Union address, President Trump is expected to tout his infrastructure plan, which he has talked about since his time on the campaign trail. But a leaked draft of the plan show it\\'s nothing more than a scam. It calls for cutting or significantly changing at least 10 bedrock environmental laws to make it easier for corporations to bypass critical protections for air, water, and wildlife. Instead of putting Americans to work rebuilding crumbling infrastructure, the Trump infrastructure scam is the administration\\'s latest giveaway to the oil and gas industry. The only clear winners from this plan are corporate developers, while communities, workers, and the environment face untold threats. And there\\'s no clear economic benefit from this plan, as it will decimate the Highway Trust Fund, cutting jobs and crucial infrastructure projects.\\nAs far as environmental risks, this scam is one of the worst. It would prioritize polluter profits and cut communities out of the decision-making process for major developments in their neighborhoods. Corporations would be able to sidestep public health, worker safety, and environmental protections for infrastructure projects, including for toll roads, pipelines, drilling projects, and new mines. It allows polluters to skirt landmark public health protections like the Clean Air Act and the Clean Water Act, all while depriving funding for improvements for clean drinking water systems. It even grants authority to Interior Secretary (and friend of the oil and gas industry) Ryan Zinke to unilaterally approve new gas pipelines through National Parks. The Trump administration will attempt to brand these environmental attacks as an effort to improve the infrastructure permitting process. In actuality, they are attempting to steamroll hardworking Americans by silencing their voices in determining where pipelines, highways, and other large projects should be built, all to boost industry profits.\\nBut Americans won\\'t be duped by anything that puts us, our families, and our outdoor places at risk. A recent poll found that 94% of voters reject the idea that we have to put our health and outdoors at risk to improve our nation\\'s infrastructure. So when Trump tries to use the State of the Union speech tomorrow to sell us on an infrastructure plan, we\\'ll see it for what it is: a scam.\\n\\nACTION OF THE DAY\\n#HandsOff Our Medicaid. Earlier this month, the Trump administration ended Medicaid as we know it, allowing states to enact punitive work requirements as part of their Medicaid programs. The majority of working-age Medicaid recipients are already working\\x97and majority of those who are not are either ill or disabled, caring for a loved one, or going to school. This decision has put 6.3 million Americans\\' health insurance at risk, and it kicks struggling workers while they\\'re down. And according to a new report, the state of Kentucky will go a step further\\x97requiring people who have lost their Medicaid to pass a health or financial literacy test to get health care back, harkening back to racist literacy tests in the Jim Crow South. It\\'s the latest in his attacks on crucial programs that help families make ends meet\\x97so take action today! Add your name to our petition urging the Trump administration to reverse their decision and keep their #HandsOff our Medicaid.\\n\\nWHAT\\'S TRENDING\\nAttacking Reproductive Rights. Tonight, the Senate will vote on a 20-week abortion ban. The procedural vote has been scheduled by Senate Majority Leader Mitch McConnell, who promised a vote early this year after the House passed the bill a few months ago. What effects would such a restrictive ban have on women across the country? Twenty weeks is the time when women have an ultrasound that helps determine if the child will survive after birth. Read stories of women that reveal why the ban would be so harmful. In addition to the harmful effects, our country just acknowledged the 45th anniversary of Roe v. Wade one week ago. Such a ban would clearly violate Roe v. Wade, and thus, is unconstitutional. Even if this particular legislation fails, it reminds us to stay vigilant and continue to fight for women\\'s rights to access safe, affordable abortion care.\\n#TimesUp at the Grammys. Last night, some of the biggest music stars in the country gathered for the 60th Grammy Awards. Just like the stars at the Golden Globes, many attendees showed up in support of #TimesUp, the initiative to end sexual misconduct in the workplace, by wearing white roses. Kesha gave a stunning, emotional performance of her anthem \"Praying,\" which reflects on her survival after her own experiences with sexual assault and abuse. But despite the spotlight given to #TimesUp and #MeToo, gender diversity remains a huge problem for the music industry. In fact, a new study from the University of Southern California \"showed that in the past six years, more than 90 percent of Grammy nominees have been men \\x97 a statistic that\\'s come up a lot as the industry begins to grapple with sexual misconduct.\" And, women only won one of the 10 awards broadcast during the show last night. It\\'s clear that the music industry\\x97as well as every other industry across America\\x97needs more than just gestures or roses to bring about real change.\\n\\nOFF-KILTER\\nDog-Whistling Donald. In the aftermath of the blatantly racist \"shithole\" comment from President Trump, the latest episode of the Off-Kilter podcast explores Trump\\'s more subtle racism, and whether that is actually more dangerous. Hear from former NAACP head Cornell Brooks and longtime disability rights advocate Patrick Cokley on these issues and why people are afraid to use the word \"racist.\" And then don\\'t miss a conversation with Inge Fryklund\\x97a former Assistant State\\'s Attorney in Cook County who serves on the board of Law Enforcement Action Partnership, an organization devoted to criminal justice reform and stopping the war on drugs\\x97about the law enforcement case for legalization.\\n\\nUNDER THE RADAR\\nTracking Your Car. The Immigration and Customs Enforcement (ICE) agency has a new, powerful tool in their hands\\x97and it\\'s raising concerns about people\\'s civil liberties. According to reports, ICE now has access \"to billions of license plate records and new powers of real-time location tracking.\" While this is concerning for all Americans, it is especially concerning for immigrants, given ICE\\'s focus on sweeping up anyone it comes in contact with, even long time family and community members. This new tool will make it easier for ICE to expand its enforcement overdrive apparatus; in addition, it could lead to more people avoiding registering their cars, \"a public safety hazard.\" /n/r We all knew this was coming. /n/r Check out my new laboratory!\\n\\nWhich is the \"monster\"?\\nFRANKENSTEIN - Mondays at 7 PM\\nwww.TheFrankensteinMusical.com\\nmusic, book & lyrics by Eric Sirota physicist/composer/playwright /n/r When Trump said we\\'d be back in the lead.... this wasn\\'t what I had in mind  :/ /n/r In perpetuity. /n/r \"Law Enforcement\" (cont.) /n/r Hey! J. Willard Gibbs! :O /n/r Nature Science Lesson.\\n\\n(File under \"Framing\". How people employ words Matters,\\n  so let\\'s not be \\'Suckers\\'.) /n/r \"Trending.\" (cont.)\\n\\nFrom the Archives (which just makes this more relevant IMO).\\n  And so it Goes... /n/r I think I heard the line \"America First\" from the mother\\'s shirt in use these days... but... I doubt it, history never repeats itself, does it? /n/r Your computer will read this book aloud to you! :O /n/r ...Linus Pauling! (Y) /n/r FYI...\\n\\nWhy do we get goose bumps when exposed to cold temps and in situations which evoke fear, when we\\'re skeet?\\n\\nThe short answer is that this reaction to threatening situations and frigid conditions is an evolutionary throw back.\\n\\nWe are descended from primates whose body was covered with hair and fur. \\n\\nFor these early creatures, it was a survival advantage if they were able to appear larger and more formidable to enemies. How to do that? Cause each hair on the body to become erect. \\n\\nIn freezing conditions, it was a survival advantage also to erect each hair on the body, because this action trapped body heat and provided additional insulation from the cold.\\n\\nWhat erected body hair? Tiny muscles near the skin\\'s surface. When these muscles are activated they form small \"pimples\" or goose bumps.\\n\\nOf course, we\\'re no longer covered with fur, but the muscles that erected the hair in our ancestors remain with us. /n/r \"Maybe that is a good idea. I will discuss with others.\" /n/r THE KING TUT WRAP? A new technology is coming out to wrap food and produce that doesn\\'t include the toxic waste to dump when its no longer needed.\\nBut surprisingly its based on an old technology- a very old one.\\nIn fact, it was discovered in King Tut\\'s tomb and used for preservation by Egyptian pharaoahs..... and its far more cost effective. In fact, it can be re-used up to 150 times\\n\\n\"Come join the revolution, and say goodbye to plastic and its [unhealthy] chemicals. Help our landfills shed some pounds\", says Etee, a company who is going with the idea.\\nLink to video  >\\n\\nhttps://www.youtube.com/watch?v=ohe64q7c5l0 /n/r A stroll under the #NorthernLights, Canada, Jan 27, 2018. #aurora #exploreCanada\\n\\nhttps://twitter.com/Adamhillstudios/status/957318381178368000 /n/r Interesting image to run maps to study human settlements on our planet. This composite image offers a view of the Americas at night. NASA Earth Observatory\\'s \"Night Lights\" website. /n/r Slower metabolizing of alcohol? :O /n/r #Register now for #Molecular #Immunology #Conference\\n#March 08-09, 2018 #London, UK https://goo.gl/DuTJ9z /n/r \"They have access to the most sophisticated technology the world has ever seen and they bully you with it. They are The Advertisers and they are laughing at you.\" /n/r WOW! Northern Lights seen last night, Jan 24, 2018 from Kval?ya, Norway. Photo credit: Marianne Bergli. https://tinyurl.com/y8rzvyoa  #Aurora #NorthernLights #Norway\\n\\nhttps://twitter.com/mark_tarello/status/956707717254729728 /n/r Not good. /n/r Hendrik Casimir effect of the same name in 1948 was declared on the basis of a description of effects in the vacuum between two parallel mirrors. the small inclinations of the mirrors can drastically change the strength of Casimir\\'s strength making a correct measurement difficult.\\nA flat-ball configuration solves this problem because the inclinations of the plane or lateral movements of the sphere only cause a small change in the distance between the two conductors.\\nThis research was published in Physical Review Letters, Matteo Rini. /n/r QOTD (Christ, I wish someone would tell \\'our\\' politicians!):\\n\\n\"All parts should go together without forcing. You must remember that the parts you are reassembling were disassembled by you. Therefore, if you can\\'t get them together again, there must be a reason. By all means, do not use a hammer.\"\\n\\x97 IBM maintenance manual, 1925 /n/r FROM JOHN PAVLOVITZ\\nDear Friend,\\nThank you for your note to let me know you\\'re worried about me, that you\\'re concerned about my health\\x97that you\\'re not sure that I realize I\\'m coming across as really angry lately.\\nYour assessment is correct.\\nI am angry.\\nI\\'m sorry.\\nI can imagine I\\'m not all that fun to be around right now, and that from time to time my words come across as combative or abrasive. I\\'m probably more than a bit of a downer lately and I apologize.\\nYou\\'re going to have to bear with me, as I haven\\'t been sleeping well for about a year or so. Admittedly I\\'m not at my best these days, so you\\'ll need to forgive me. I\\'m chronically overtired. I\\'m exhausted from having to give all the sh*ts about people that you\\'re supposed to be giving\\x97along with my own.\\nI\\'m worn out from keeping up on legislation and watching hearings and staying on top of details and remembering deadlines and imploring action\\x97while you go about your day as if such things are an annoyance, is if they are a disruption of your plan, as if the expiration date for my outrage has long come and gone.\\nI am absolutely burnt out from trying to make my voice loud enough to counteract not only the bad people\\'s incredible volume\\x97but your deafening silence. Both of these things are doing similar damage right now, sadly.\\nBelieve me, I understand that my activism is a problem for you. Please know that your inactivism is similarly problematic for me. It\\'s part of the reason I am as angry as I am; because I\\'m not only having to fight against those who seem furiously bent on hurting people\\x97I\\'m having to fight against those who don\\'t seem give enough of a damn that they are doing so, to say anything.\\nLook, I get it, I really do. It\\'s difficult to see so much bad news, to fully face the relentless flood of terrible, to try and wrap your brain around seemingly boundless cruelty around you. It\\'s tiresome to spend so much time with a closed fist. I know it\\'s even a pain in the rear end to endure the continual rantings of people like me on your news feed and in your timeline and across the dinner table and in the break room.\\nI\\'m tired of me too.\\nI\\'m sick of the fight too.\\nI\\'m sick of the sound of my own voice.\\nI\\'d rather not be doing this either.\\nI\\'d much rather prefer to just enjoy life, to forget about it all, to only post pictures of puppies and my kids, and to simply ignore all that \"political garbage.\"\\nBut that is what privilege looks like; to even believe I have such an option, to have the great luxury of living without urgency because I can seemingly shield myself from it all.\\nThat is what the bad people are counting on. They\\'re counting on good people who are too tired, too apathetic, too selfish, or to oblivious to sustain their outrage. I am not going to give that gift to them.\\nAs long as they\\'re fully invested in putting people through hell, I\\'m going to be as invested in pushing back against it.\\nI think the people I love are worth it.\\nI think you and the people you love are worth it.\\nI think people I\\'ll never meet are worth it.\\nAnd that\\'s the rub here: love will often look a lot like rage, as it fiercely fights on behalf of those who are being attacked. \\nSo yes, angry is not all that I am, but I am rightly angry.\\nAnd it would be really helpful if we could carry the load of outrage right now. \\nThat would actually be a source of rest and joy and breath.\\nFriend, if you really want me to be less angry, you might try being a little more angry.\\nI am angry, friend.\\nI wish you were angry too. /n/r Davos meeting   Live now. /n/r Trump\\'s Attorney Lay\\'s Down the Law! /n/r Are you a Scientist who loves Science Fiction?\\n\\nAnd if so, may I ask for help with a Science Fiction Story? /n/r Tiffany Foster-Grant I have a problem with the Ten Commandments. Here it is: Why are there ten? We don\\'t need that many. I think the list of commandments was deliberately and artificially inflated to get it up to ten. It\\'s clearly a padded list.\\n\\nHere\\'s how it happened: About five thousand years ago, a bunch of reli\\xadgious and political hustlers got together to figure out how they could control people and keep them in line. They knew people were basically stupid and would believe anything they were told, so these guys announced that God\\x97 God personally\\x97had given one of them a list of Ten Commandments that he wanted everyone to follow. They claimed the whole thing took place on a mountaintop, when no one else was around.\\n\\nBut let me ask you something: When these guys were sittin\\' around the tent makin\\' all this up, why did they pick ten? Why ten? Why not nine, or eleven? I\\'ll tell you why. Because ten sounds important. Ten sounds official. They knew if they tried eleven, people wouldn\\'t take them seriously. People would say, \"What\\'re you kiddin\\' me? The Eleven Commandments? Get the fuck outta here!\"\\n\\nBut ten! Ten sounds important. Ten is the basis for the decimal system; it\\'s a decade. It\\'s a psychologically satisfying number: the top ten; the ten most wanted; the ten best-dressed. So deciding on Ten Commandments was clearly a marketing decision. And it\\'s obviously a bullshit list. In truth, it\\'s a politic; document, artificially inflated to sell better.\\n\\nI\\'m going to show you how you can reduce the number of commandments and come up with a list that\\'s a bit more logical and realistic. We\\'ll start with the first three, and I\\'ll use the Roman Catholic version because those are the ones I was fed as a little boy.\\n\\n\\x95 I AM THE LORD THY GOD, THOU SHALT NOT HAVE STRANGE\\nGODS BEFORE ME.\\n\\n\\x95 THOU SHALT NOT TAKE THE NAME OF THE LORD THY GOD IN\\nVAIN.\\n\\n\\x95 THOU SHALT KEEP HOLY THE SABBATH.\\n\\nOkay, right off the bat, the first three commandments\\x97pure bullshit \"Sabbath day,\" \"Lord\\'s name,\" \"strange gods.\" Spooky language. Spooky language designed to scare and control primitive people. In no way does superstitious mumbo jumbo like this apply to the lives of intelligent, civilized human in the twenty-first century. You throw out the first three commandments, am you\\'re down to seven.\\n\\n\\x95HONOR THY FATHER AND MOTHER.\\n\\nThis commandment is about obedience and respect for authority; in other words it\\'s simply a device for controlling people. The truth is, obedience and respect should not be granted automatically. They should be earned. They should be based on the parents\\' performance. Some parents deserve respect. Most of them don\\'t. Period. We\\'re down to six.\\n\\nNow, in the interest of logic\\x97something religion has a really hard time with\\x97I\\'m going to skip around the list a little bit:\\n\\n\\x95 THOU SHALT NOT STEAL.\\n\\n\\x95 THOU SHALT NOT BEAR FALSE WITNESS.\\n\\nStealing and lying. Actually, when you think about it, these two com\\xadmandments cover the same sort of behavior: dishonesty. Stealing and lying. So we don\\'t need two of them. Instead, we combine these two and call it \"Thou shalt not be dishonest.\" Suddenly we\\'re down to five.\\n\\nAnd as long as we\\'re combining commandments I have two others that be\\xadlong together:\\n\\n\\x95 THOU SHALT NOT COMMIT ADULTERY.\\n\\n\\x95 THOU SHALT NOT COVET THY NEIGHBOR\\'S WIFE.\\n\\nOnce again, these two prohibit the same sort of behavior; in this case, mar\\xadital infidelity. The difference between them is that coveting takes place in the mind. And I don\\'t think you should outlaw fantasizing about someone else\\'s wife, otherwise what\\'s a guy gonna think about when he\\'s waxing his carrot?\\n\\nBut marital fidelity is a good idea, so I suggest we keep the idea and call this commandment \"Thou shalt not be unfaithful.\" Suddenly we\\'re down to four.\\n\\nAnd when you think about it further, honesty and fidelity are actually parts of the same overall value. So, in truth, we could combine the two honesty commandments with the two fidelity commandments, and, using positive lan\\xadguage instead of negative, call the whole thing \"Thou shalt always be honest and faithful.\" And now we\\'re down to three.\\n\\n\\x95THOU SHALT NOT COVET THY NEIGHBOR\\'S GOODS.\\n\\nThis one is just plain stupid. Coveting your neighbor\\'s goods is what keeps the economy going: Your neighbor gets a vibrator that plays \"O Come All Ye Faithful,\" you want to get one, too. Coveting creates jobs. Leave it alone.\\n\\nYou throw out coveting and you\\'re down to two now: the big, combined honesty/fidelity commandment, and the one we haven\\'t mentioned yet:\\n\\n\\x95THOU SHALT NOT KILL.\\n\\nMurder. The Fifth Commandment. But, if you give it a little thought, you realize that religion has never really had a problem with murder. Not really. More people have been killed in the name of God than for any other reason.\\n\\nTo cite a few examples, just think about Northern Ireland, the Middle East, the Crusades, the Inquisition, our own abortion-doctor killings and, yes, the World Trade Center to see how seriously religious people take Thou Shalt Not Kill. Apparently, to religious folks\\x97especially the truly devout\\x97murder is ne\\xadgotiable. It just depends on who\\'s doing the killing and who\\'s getting killed.\\n\\nAnd so, with all of this in mind, folks, I offer you my revised list of the Two Commandments:\\n\\nFirst:\\n\\n\\x95THOU SHALT ALWAYS BE HONEST AND FAITHFUL, ESPECIALLY\\nTO THE PROVIDER OF THY NOOKIE.\\n\\nAnd second:\\n\\n\\x95THOU SHALT TRY REAL HARD NOT TO KILL ANYONE, UNLESS,\\nOF COURSE, THEY PRAY TO A DIFFERENT INVISIBLE MAN\\nTHAN THE ONE YOU PRAY TO.\\n\\nTwo is all you need, folks. Moses could have carried them down the hill in his pocket. And if we had a list like that, I wouldn\\'t mind that brilliant judge in Alabama displaying it prominently in the courthouse wall. As long he in\\xadcluded one additional commandment:\\n\\n\\x95THOU SHALT KEEP THY RELIGION TO THYSELF! /n/r Josh Ray, 35, was a husband and father of a young daughter. The World Socialist Web Site spoke with Nikki Emmanuel, whose husband worked with Ray in the oil fields for six years. \"He was a devoted father and husband who loved his job,\" Nikki said.\\n\\n\\'Bodies of five workers recovered at site of Oklahoma gas well blast\\' / World Socialist Web Site\\n\\nhttp://www.wsws.org/en/articles/2018/01/24/okla-j24.html\\n\\n[Top row, left to right: Josh Ray, Matt Smith, Cody Risk. Bottom row, left to right: Parker Walbridge and Roger Cunningham] /n/r \\'Our\\' priorities today. <sigh>\\n\\nHow did that song \\'go\\'?\\n\\x97 \"Teach Your Children Well\"?... /n/r A testimonial documentation. /n/r Thanks to our Emperor\\'s decree, I foresee higher prices on solar panels (and washing machines) coming in the U.S. Which will make switching to renewable energy a less attractive option.\\nOn the positive side.... this might be a great bump for people who have a lot of stock in big oil companies and fossil fuel.\\nDo you think Trump thought of that? ;) /n/r Horizontal monopoly in Europe /n/r #AuroaBorealis above the frozen beach at Flakstad in Lofoten, Norway. It was pretty tough to stand up and the tripod was sliding allover the place. #NorthernLights #twanight #stormhour\\n\\nhttps://twitter.com/AlexConu/status/956155967984619520 /n/r THESE articles are from PROGRESS REPORT\\n\\nJanuary 24, 2018\\n\\nDESENSITIZED.\\nYesterday, tragedy struck Marshall County High School in western Kentucky, as a 15-year-old student opened fire, killing 2 and wounding 18 others. The victims of the shooting \"ranged from 14 to 18 years old\"\\x97young people who have (or had) their whole lives in front of them. With such a brutal attack on high school students, you\\'d expect a nation in mourning, with media outlets covering the violence non-stop.\\nBut you\\'d be wrong. The story barely broke through the 24/7 Trump news cycle, and most members of Congress didn\\'t even comment on the tragedy. The President of the United States didn\\'t say anything at all, even though he spent the morning tweeting. It\\'s really no surprise that Americans have become so incredibly desensitized to such violence. In fact, the school shooting in Kentucky was the 11th school shooting of 2018\\x97and we are only 24 days into the new year. And it\\'s not just these mass shootings that are putting young people in the U.S. at such high risk. Teenagers in the U.S. between the ages of 15 to 19 are 82 times more likely to die from gun violence than teenagers in peer countries. And remember: on an average day, 96 Americans are killed with guns.\\nThese statistics are shocking, but once you\\'ve heard them before, they start to become normalized. But we can take action to prevent future gun violence\\x97we just need our elected officials to have the courage to stand up to the National Rifle Association (NRA). Check out this report to see why policies must be enacted to curb gun violence in America\\x97then call your elected representatives and tell them it\\'s time to #HonorWithAction!\\n\\nACTION OF THE DAY\\n#ProtectDreamers. On Monday, Congress ended the #TrumpShutdown without any solution for Dreamers. This is unacceptable. The next deadline is now February 8, and, in the meantime, 122 Dreamers are losing their protection every single day. We must continue to stand with Dreamers, and demand that Congress take action. Senate Majority Leader Mitch McConnell must be held accountable for his promises to bring legislation to the floor on DACA, and this legislation must provide a permanent solution for Dreamers, without compromising our values by allowing Stephen Miller and his allies to attach their anti-immigrant wishlist. Call your members of Congress today at 202-224-3121! Then, share the graphic below.\\n\\nWHAT\\'S TRENDING\\nGlobal Gag Rule. A year ago, President Trump used an executive order to reinstate and expand the Mexico City Policy, also known as the Global Gag Rule. First instituted in 1984 by then-President Ronald Reagan and rescinded most recently by then-President Barack Obama in 2009, the global gag rule restricts all U.S. global health aid from being used to support abortion in any way. Under Trump\\'s policy, U.S.-funded nongovernmental organizations are even restricted from using private, non-U.S. funds to offer abortion care, discuss abortion, or engage in activities to change restrictive abortion laws in the countries in which they work. The reinstatement of the Global Gag Rule will lead to an increase in unsafe abortions and pregnancy-related deaths among women in the poorest countries. Early analysis of the impacts of the policy shows reductions in critical reproductive health services that cannot easily be replaced.\\nDitching Collection Agencies. Twelve Senators sent a letter to the Department of Education demanding it justifies the use of private collection agencies for federal student loans. Last year, the federal government paid these contractors almost a billion dollars to recover debt from about 7 million borrowers. But, a piece published today by the Center for American Progress points out that the government spent almost as much on debt collection as it did servicing accounts for 33 million borrowers who are currently in repayment. The column describes the pitfalls of using private collection agencies, and argues that borrowers would be better served by removing them from the student loan system.\\nWelcoming Communities. Immigrating to the United States can be a challenging journey, but it is made much harder for LGBTQ immigrants. A new report by the Center for American Progress explains the unique challenges that LGBTQ immigrants face, as well as how cities, service providers, and philanthropic organizations can make communities more welcoming to those individuals. Given that LGBTQ unauthorized immigrants make up 17 percent of survivors of anti-LGBTQ hate violence, it is crucial that cities provide a host of services are made available to LGBTQ immigrants to help them feel safe, protected, and assist them in thriving in their new community. As the report details, the types of services necessary include, but are not limited to, legal, health, employment, housing, language access, and education. If the U.S. is to continue to serve as a beacon of hope for LGBTQ immigrants around the world, communities must reform immigration policies and take concrete actions to improve services to ensure they are safe havens.\\n\\nGOOD NEWS\\nSecond Chances. In November, Florida voters are going to have the opportunity to restore voting rights for more than a million people with a felony record. Over 750,000 signatures were gathered, meaning that the proposed constitutional amendment can now appear on the ballot. As one of only four states with a lifetime ban on voting, the state of Florida\\'s harsh laws prohibit those with a prior felony conviction from getting a second chance\\x97but Floridians have a chance to change that. Head to SecondChancesFL.org for more information about the ballot initiative and what it could mean for 1 in 10 Floridians. /n/r #New Hope for #Repairing a Damaged or #Aging #Immune System https://goo.gl/9G2i6h /n/r Hydrogen bonds are what keeps cellulose fibers together to form paper... :O /n/r \"It\\'s not up to you to decide! \\nIt\\'s not up to me, or the pope, or Billy Graham! \\nAnd for sure it\\'s not up to the government. \\nIt\\'s only up to the mother, hopefully with the advice of a physician and the consideration of the father! \\nBut ultimately the mother makes the final decision.\"\\n(link thx to Mi Robin!) /n/r Its created a huge quackery based industry /n/r Montana says: Either the corporations let you see and use the entire Internet, nothing blocked.\\nOr THEY can get out. (y) /n/r STAND WITH DREAMERS. This article is from Progress Report.\\n\\nYesterday, a bipartisan group of senators agreed to end the shutdown, with the promise from Senate Majority Leader Mitch McConnell that he would bring legislation on DACA to the floor. It is unclear what exactly this legislation would look like, although McConnell said it would be \"neutral\" and allow for an open amendment process. This could lead to amendments being offered by some of the most staunch anti-immigrant senators, such as Senator Tom Cotton, who helped write the cruel RAISE Act. Such nativist, racist policies would not only prevent common sense, bipartisan talks on immigration reform, but they would put more than just Dreamers at risk.\\nThis agreement between moderates to end the shutdown without a permanent fix for Dreamers has drawn ire from many, including other senators, who question why McConnell should be trusted. Senator Claire McCaskill, who voted \"yes\" on ending the shutdown, claims that she is not relying on McConnell\\'s promise, but instead, the \"12 moderate Republican[s] who were willing to step forward in this.\" But remaining wary about these reassurances is not only understandable but necessary. Trump has refused to negotiate in good faith and showed that he does not actually want to help Dreamers by walking away from multiple global, bipartisan deals that could have averted this shutdown in the first place. He is more interested in appealing to the members of his base that are rabidly anti-immigrant, and these efforts to appease those individuals are being led by none other than Stephen Miller. It\\'s time for all members of Congress to stand up to such appalling policies and views and recommit to an America that welcomes immigrants with open arms.\\nWe must continue to stand with Dreamers. The next deadline is now February 8th; but, in the meantime, another 2,000+ Dreamers will lose protection, in addition to the 17,000 that already have been exposed to risks of losing their jobs and deportation. If a permanent solution is not reached soon, the White House has already indicated it will start deporting Dreamers on March 5th. Read our \"Action of the Day\" to see how you can take action immediately.\\nACTION OF THE DAY\\n#ProtectDreamers. Yesterday, Congress ended the #TrumpShutdown without any solution for Dreamers. This is unacceptable. The next deadline is now February 8, and, in the meantime, 122 Dreamers are losing their protection every single day. We must continue to stand with Dreamers, and demand that Congress take action. Senate Majority Leader Mitch McConnell must be held accountable for his promises to bring legislation to the floor on DACA, and this legislation must provide a permanent solution for Dreamers. Call your members of Congress today at 202-224-3121! /n/r Alaska, USA. Aurora....Photography by @timthetoothninja. jan 22, 2018 #Auroraborealis #StormHour\\n\\nhttps://twitter.com/StormHour/status/955592797448560643 /n/r Current situation of Mt.Mayon Volcano, Albay,Philippines /n/r From Kevin G. Rhoads... (Y) /n/r From Bruce Jensen... :O /n/r NorthernLights. Alaska, USA, Jan 22, 2018 #alaska #NorthernLights #Aurora #stormhour #Auroraborealis\\n\\nhttps://twitter.com/thetoothninja/status/955294008720351232 /n/r WE EXPERIENCE TIME TRAVEL ALL THE \"TIME\".\\n\\nOver 100 years ago, Albert Einstein discovered that the rate at which time flows is affected by two parameters...gravity and the speed of the traveler. These two principles were foundational to both his Special and his General Theory of Relativity. Although it\\'s counter intuitive...it\\'s nonetheless true...the passage of time is not constant. It\\'s variable.\\n\\nHow so, you ask...The faster one travels relative to a stationary observer, the more SLOWLY time passes for the traveler compared to that observer.\\n\\nBUT, the weaker the gravity is for one individual compared to another, the more RAPIDLY time flows.\\n\\nIn other words, weaker gravity SPEEDS UP time passage and a faster velocity SLOWS DOWN time passage..\\n\\nSo here we have two competing affects. The higher relative speed compared to someone stationary on the ground, means time for the passenger passes more SLOWLY.\\n\\nBUT the higher one is above those on the ground, the more the flow of time speeds UP.\\n\\nJust how then do these two competing phenomena affect an airline passenger flying 500 MPH at an altitude of 35,000 feet.\\n\\nBeing 6 miles above the earth, where gravity is a tiny bit weaker than for those on the ground, means time for our passenger passes a bit faster.\\n\\nBut, since passengers are traveling over 500 MPH, time flows a little more slowly than for people on the ground.\\n\\nSo, which affect wins out?\\n\\nAs it turns out, the weaker gravity at the higher altitude, speeds UP the flow of time, more than the higher velocity of the aircraft slows DOWN the flow of time.\\n\\nThese affects have actually been calculated for a passenger who racked up 10,000,000 frequent flier miles at 500 MPH.\\n\\nOur passenger aged 59 millionths of a second more than his homebody wife. He has traveled that far into the future compared to folks who kept their feet on the ground.\\n\\nDH /n/r #Oncolytic #Virus Therapy Passes Early #Efficacy Tests in #Glioma Subtypes https://goo.gl/d7VzxC /n/r What can we learn from Mary Shelley\\'s Frankenstein?\\nMaybe that doing science without collaboration, mentorship or peer review is a bad thing.\\n\\nOr maybe is is about the human need for love and companionship.\\nFRANKENSTEIN a new Off-Broadway musical written/composed by a physicist. /n/r Ballooning Trump... /n/r It\\'s hard.\\n\\nI don\\'t VIEW \\'our\\' fcked up government through a partisan lens.\\n  After all, Both parties SUCK.\\n\\nBut damn are they ever making it hard... /n/r Post deleted due to solicitation. Please follow the rules pinned to this forum. /n/r 5 discoveries of science that will change our future?? /n/r The Trump\\'s Presidency is missing many parts, heavily fictionalized and the timeline is frequently questionable. /n/r *Here\\'s TO You!\" /n/r Reasonable question.\\n\\x97 What\\'s *UNREASONABLE* is that only a Tiny SLIVER of the American populace would even think to ask it.\\n\\n(Here\\'s a Hint:\\n\\x97 *Ka-CHING!*) /n/r (This struck me as worth sharing on a page calling itself the \"Science, Technology, and Society Discussion Corner\"):\\n\\nGlenn Greenwald\\n@ggreenwald\\n\\n*\\n\\nTwitter is sending out messages to people telling them that, for their own good, they are documenting that the user has either followed, cited or re-tweeted an account Twitter decided is linked to Russia & its propaganda efforts. That\\'s not creepy at all. /n/r What was Victor Frankenstein\\'s  \"sin\"?\\nThe Frankenstein Musical\\nwww.TheFrankensteinMusical.com /n/r We\\'ve had incredible display of the aurora tonight, Jan 19, 2018 in Finnish Lapland. #StormHour #ThePhotoHour @Aurora_Zone\\n\\nhttps://twitter.com/Astro_Matt27/status/954478734903533569 /n/r A welcome return to some decent solar wind this evening, Jan 19, 2018 at Janiskoski, Finland @TamithaSkov #AuroraBorealis #StormHour\\n\\nhttps://twitter.com/MrAntiatlas/status/954522517984641024 /n/r Just another symptom of the end of democracy in the US.  https://www.facebook.com/photo.php?fbid=10215510831825837&set=a.1118153515947.20266.1290271584&type=3&theater /n/r Beauty in the eye of the microscope /n/r Rocket launch over Japan looks just like California UFO reports\\n\\nAnthony Watts / 40 mins ago January 19, 2018\\n\\nRemember when the pre-dawn SpaceX launch from Vandenberg just before Christmas created a flurry of UFO reports in Southern California? Now it\\'s Japan\\'s turn.\\n\\nOn Jan. 18th, the Japanese space agency JAXA launched a small rocket from the Uchinoura Space Center. It made a big display. Japanese artist and photographer Kagaya captured dramatic images of the rocket\\'s exhaust glowing in the starry pre-dawn sky over the Pacific:\\n\\n\"I watched the launch from Okinawa Island and photographed it using my Sony ?7RIII camera,\" says Kagaya, who has posted a must-see video of the event on Youtube:\\n\\nJapan\\'s new Epsilon rocket is relatively small, designed to launch scientific satellites at a fraction of the cost of its larger predecessors. On this occasion, the Epsilon propelled an Earth observing satellite to orbit, the ASNARO-2. Power by solar cells and carrying a large X-band antenna, ASNARO-2 is a synthetic aperture radar capable of imaging the surface of our planet with 1-meter resolution.\\n\\nShortly after the launch, noctilucent (night shining) clouds were seen over a broad swath of western Japan as ice crystals forming in the rocket\\'s wake caught the rays of the rising sun. These clouds occur naturally around Earth\\'s poles, but they are very rare at lower latitudes such as Japan\\'s. In polar regions, noctilucent clouds are seeded by specks of meteor smoke, which become frosted by naturally occurring water vapor drifting up toward the edge of space. Over Japan, the ingredients were provided by JAXA: water vapor in the rocket\\'s exhaust mixed with solid-booster aerosols to create the display.\\n\\nFrom NASA\\'s Spaceweather.com\\n\\nhttps://wattsupwiththat.com/2018/01/19/rocket-launch-over-japan-looks-just-like-california-ufo-reports/ /n/r \"The cycles of parasites are often diabolically ingenious. It is to the unwilling host that their ends appear mad. Has earth hosted a new disease \\x96 that of the world eaters? Then inevitably the spores must fly. Short-lived as they are, they must fly. Somewhere far outward in the dark, instinct may intuitively inform them, lies the garden of the worlds. We must consider the possibility that we do not know the real nature of our kind. Perhaps Homo sapiens, the wise, is himself only a mechanism in a parasitic cycle, an instrument for the transference, ultimately, of a more invulnerable and heartless version of himself\\x85a biological mutation as potentially virulent in its effects as a new bacterial strain. The fact that its nature appears to be cultural merely enables the disease to be spread with fantastic rapidity. There is no comparable episode in history\\x85 To climb the fiery ladder that the spore bearers have used one must consume the resources of a world\\x85 Basically man\\'s planetary virulence can be ascribed to just one thing: a rapid ascent, particularly in the last three centuries, of an energy ladder so great that the line on the chart representing it would be almost vertical\\x85 Western man\\'s ethic is not directed toward the preservation of the earth that fathered him. A devouring frenzy is mounting as his numbers mount. It is like the final movement in the spore palaces of the slime molds. Man is now only a creature of anticipation feeding upon events.\"\\n\"The World Eaters\"\\nfrom Loren Eiseley, The Invisible Pyramid, 1970 /n/r The Following Is From SOCIAL SECURITY WORKS: \\n\\nAs all eyes are focused on a potential government shutdown, the Senate is attempting to sneak through Alex Azar. He\\'s Donald Trump\\'s new pick for Secretary of Health and Human Services, the cabinet post that oversees Medicare and Medicaid. And he might be even worse than Trump\\'s first HHS Secretary, Tom Price.\\n\\nAzar is the former president of pharmaceutical giant Eli Lilly USA. During his tenure, Eli Lilly more than tripled the price of life-saving insulin medication?protecting drug company profits over patient care.\\n\\nNext week, the Senate will vote on Alex Azar\\'s nomination. Write to your Senators right now and tell them that we don\\'t need a pharmaceutical exec running our health care. /n/r Trump denies saying \"shithole countries\" and now some of the Republicans present support Trump\\'s statement, intimating that Dick Durbin, who made the information public, is a liar. Trump says he\\'s not a racist. Trump says \"there was no collusion\"... almost a dozen times. Trump says that the Dems are preventing his own bill, which would end DACA, from being canceled and permit DACCA to continue. Trump says this, Trump says that. I\\'m sick of Trump and want at least him GONE, but would prefer the WHOLE Trump Administration and associated cronies to also be removed from our government !!! /n/r Imperfect nature or cruel nature? /n/r From the Archives.\\n  That Carl Sagan was one helluva Human Being. R.I.P. /n/r Phew! \"From the Archives\" again...\\n  \"I don\\'t need no Stinkin\\' iPod\" edition...\\n\\n(Then again, what with Apple\\'s iPhones, etc., the iPod is now as extinct as the Dodo Bird along w/these turntables, isn\\'t it?) /n/r Would you like to play a game? /n/r Occam\\'s Razor /n/r Speaking the Truth.... /n/r Intelligent design is a hoax. No supernatural input was required for evolution, in fact, supernatural is an oxymoron. It does not exist. Nothing can be outside of nature. All phenomena have an explanation. All phenomena obey the laws of science.\\n\\nYes, we may not understand all natural processes now, but to assign what we don\\'t understand to the workings of a god is irrational. /n/r From Kevin G. Rhoads... :O /n/r Pink and blue..the bizarre ice world of Lake Ontario, Rochester, NY, USA... Thanks to @JamesMontanus #StormHour\\n\\nhttps://twitter.com/StormHour/status/953358338703724544 /n/r Aside from the obvious which would be a sort of Trump KGB-for-hire, I can\\'t see this as an intelligent way to go in just about ANY circumstance :/ /n/r \"Sing It, Baby!\" /n/r Even in the Hospital, I saw more things having less destructive  diseases than Trump... /n/r Reading ???? ????? amin maalouf:\\n\\nEach individual\\'s identity is made up of a number of elements and these are clearly not restricted to the particulars set down in official records. Of course, for the great majority these factors include allegiance to a religious tradition; to a nationality \\x97 sometimes two; to a profession, an institution, or a particular social milieu. But the list is much longer than that; it is virtually unlimited.\\n\\n[\\x85]\\n\\nNot all these allegiances are equally strong, at least at any given moment. But none is entirely insignificant, either. All are components of personality \\x97 we might almost call them \"genes of the soul\" so long as we remember that most of them are not innate.\\n\\nWhile each of these elements may be found separately in many individuals, the same combination of them is never encountered in different people, and it\\'s this that gives every individual richness and value and makes each human being unique and irreplaceable.\\n\\n[\\x85]\\n\\nIt can happen that some incident, a fortunate or unfortunate accident, even a chance encounter, influences our sense of identity more strongly than any ancient affiliation.\\n[\\x85]\\n\\nIn every age there have been people who considered that an individual had one overriding affiliation so much more important in every circumstance to all others that it might legitimately be called his \"identity.\" For some it was the nation, for others religion or class. But one has only to look at the various conflicts being fought out all over the world today to realize that no one allegiance has absolute superiority. /n/r Did you ever wonder how much heat flows from the earth\\'s core to the surface, and how it\\'s distributed?\\n\\nI ran across this map in a paper on Enhanced Geothermal Energy.\\n\\nThe paper is here.  I haven\\'t looked at their economic analysis   but it\\'s from 12 years ago, so the competing technology picture has changed enormously.\\n\\nPaper here:\\nhttps://energy.mit.edu/wp-content/uploads/2006/11/MITEI-The-Future-of-Geothermal-Energy.pdf\\n\\n(Since I\\'m supplying my own graphic, I don\\'t think Facebook will link the graphic to the paper.) /n/r Light of the Zodiac #Astrophotography Tenerife, Spain of a phenomena that can sometimes be viewed after dusk in spring, and before dawn in autumn, the suns rays lighting up cosmic dust! Venus can also be seen in the centre ~ Thanks to @OllieTPhoto #StormHour\\n\\nhttps://twitter.com/StormHour/status/953061845488881664 /n/r Suttons Law /n/r People often ask how I can take a photo of static radiation and it be of Earth? If I am on Earth...? I do it the same way Prof Alan Guth did... He has even pointed out where Earth is for you, so you know where to look in my photo... ;) #UniversalMechanics  :) https://www.facebook.com/UniversalMechanicsStudyingTheQuantumSpaceRealm/posts/864802400357369 /n/r Finland is the nation that has the highest per capita quantity of metal bands. Finland is also where the creator of Linux was born. I\\'m reasonably certain that this is *not* coincidence! /n/r \\'Society and Technology\\':\\n  For MLK Day, a painting by Haitian artist Walter Mere. /n/r This would be a interesting ride.  Hold on to your belongings! /n/r Mankind tries to survive between superstition and (little) science. Why? /n/r For MLK Day, a painting by Haitian artist Walter Mere. /n/r WOW! Northern Lights seen tonight, Jan 13, 2018 from Troms?, Norway. Photo credit: Markus Varik. https://tinyurl.com/ybuse43d #Aurora #NorthernLights #Norway\\n\\nhttps://twitter.com/mark_tarello/status/952360140900052993 /n/r Anyone use a Boogie Board? LCD tablet? /n/r If you are in the mood for some reading,,,, /n/r Is your thermometer accurate?  :O /n/r Machine-tool calculations, 1972.\\nHey! ...FREE! :O /n/r Microsoft says security patches slowing down PCs, servers\\n\\nMicrosoft Corp said on Tuesday the patches released to guard against Meltdown and Spectre security threats slowed down some personal computers and servers, with systems running on older Intel Corp processors seeing a noticeable decrease in performance.\\n\\n10 Jan 2018 02:15AM(Updated: 10 Jan 2018 02:55AM)\\n\\nREUTERS: Microsoft Corp said on Tuesday the patches released to guard against Meltdown and Spectre security threats slowed down some personal computers and servers, with systems running on older Intel Corp processors seeing a noticeable decrease in performance.\\n\\nThe security updates also froze some computers running AMD chipsets, Microsoft said in a blog post, citing customer complaints.\\n\\nShares in Intel, which reiterated on Tuesday that it saw no sign of significant slowdown in computers, fell 1.4 percent, while those of AMD fell nearly 4 percent.\\n\\nAMD shares have gained nearly 20 percent in the last week as investors speculated that the chipmaker could wrest market share from Intel, whose chips were most exposed to the security flaws.\\n\\n\"We (and others in the industry) had learned of this vulnerability under nondisclosure agreement several months ago and immediately began developing engineering mitigations and updating our cloud infrastructure,\" Microsoft executive Terry Myerson wrote in a blog post. (http://bit.ly/2mj6f3Q)\\n\\nSecurity researchers disclosed the flaws on Jan. 3 that affected nearly every modern computing device containing chips from Intel, AMD and ARM Holdings.\\n\\nMeltdown and Spectre are two memory corruption flaws that could allow hackers to bypass operating systems and other security software to steal passwords or encryption keys on most types of computers, phones and cloud-based servers.\\n\\nIntel said a typical home and business PC user should not see significant slowdowns in common tasks such as reading email, writing a document or accessing digital photos. (http://intel.ly/2FiL0Hk)\\n\\nThe chipmaker said last week that fixes for security issues in its microchips would not slow down computers, rebuffing concerns that the flaws would significantly reduce performance.\\n\\nRival AMD had also played down the threat, saying its products were at \"zero risk\" from the Meltdown flaw, but that one variant of the Spectre bug could be resolved by software updates from vendors such as Microsoft.\\n\\nBut on Tuesday AMD said it was aware of an issue with some older-generation processors following the installation of a Microsoft security update that was published over the weekend.\\n\\nMicrosoft said it was working with AMD to resolve the issues.\\n\\nApple Inc also released an updated version of its operating system software on Monday to fix the security flaw.\\n\\n(Reporting by Eric Auchard in Frankfurt and Supantha Mukherjee in Bengaluru; Editing by Saumyadeb Chakrabarty and Shounak Dasgupta)\\n\\nSource: Reuters /n/r ...Appropriate reading for our times?\\n\"Tenser, said the tensor!\\nTension, apprehension and dissension have begun!\" :O /n/r Anyone in Boise Idaho? /n/r I\\'m thinking about using \"Antimatter Sails\" in my Science Fiction Story, does anyone know about those? /n/r Add this to the book of lies.\\nDonald J. Trump\\n\\nVerified account\\n \\n@realDonaldTrump\\n 15m15 minutes ago\\nMore\\nThe language used by me at the DACA meeting was tough, but this was not the language used. What was really tough was the outlandish proposal made - a big setback for DACA! /n/r Things you should stop believing or saying about Astrophysics:\\n\\n1) The universe expands: The universe expands, it is true, but it does so at large scales such as galactic clusters, it does not expand in the Solar System and moves us away from the other planets, or that the galaxies of the Local Group move away, because if it were like that, the Milky Way would never collide with Andromeda ever.\\n\\n2) We are stardust: We have ATOMS of stars, but not only of them, other atoms are obtained from other means such as the collision of neutron stars, supernovas of type 1a or 1b, cosmic ray spacing and others.\\n\\n3) When a particle and antiparticle collide annihilate: For that \"annihilation\" must be of some particle with its corresponding antiparticle as an electron and a positron, it can not if it is electron and anti-proton for example. The truth is that they do not annihilate, they become more particles, you can understand this as joining 2 and -2 gives you 0 and from this 0 you can get other particles like 1 or -1, also if you have a certain energy you can create particles that have that same energy, for example if you have 4 and -4, you can get 3 and -3 on one side and 1 and -1 on the other.\\n\\n4) The cosmic microwave background is the echo or the light of the Big Bang: It would really be the light emitted 400,000 years after the Big Bang, since before that important things happened like the time of the great unification, the inflationary era, the Hadron era, neutrino decoupling and much more. /n/r \"The Republican tax bill departs from tradition to specifically target citizens of Democratic states with higher taxes via the elimination of the deductions for real estate taxes and state and local income taxes, both of which are more important in the coastal states where those taxes are high. What else will the Republicans do to make life difficult for Democrats?\"\\n\\n\\x97 Ted Rall /n/r Denny Haldeman https://www.facebook.com/photo.php?fbid=1140106886124829&set=a.165411843594343.36705.100003765861975&type=3&theater&ifg=1 /n/r So   What is the \"deal\" with Hillary Clinton and the sale of Uranium One to Russia that has the REPUBS so excited?\\n...Can you spell \"FAKE NEWS\"?   Please SHARE!\\n(link thx to Estel Cooper!)\\nhttps://www.facebook.com/CAFE/videos/1954628538193363/ /n/r HOW MANY TIMES has Trump used the word COLLUSION\" ? He\\'s used it at least a dozen times by my count. To me, that means he and his family are GUILTY of COLLUSION !!! /n/r (Okay. \"Society\".)\\n\\n*\\n\\nUnderstand, you Must ACCEPT This!\\n\\nOur *Overlords* INSIST,\\n\\x97 And they\\'re going to Shovel this SH*T down our throats and Up our BUTTS until We SUBMIT! /n/r Jaguar Pyramid in Belize where you can climb to the summit. Was at the complex in the jungle a few years ago. /n/r Only a few days left to #register for the 9th #Molecular #Immunology & #Immunogenetics #Congress\\nIf you haven\\'t #booked your #slots yet, then now is the time to do. \\n#Book here: https://goo.gl/vbyS6G /n/r \"Dialectic of Nature\"\\n\\nBelow are three fundamental mathematical equations that bring a solution to the interaction in nature.\\n\\nNewton\\'s law of universal gravitation states that two celestial bodies attract each other with a force that is directly proportional to the product of their masses and inversely proportional to the square of the distance between them. This force is independent of the kinetic energies of the masses (static solution). The left wing of the equation below is the interacting force (effect, action or cause) and the right wing is the result as reaction of mass M2. It is apparent that, in this interaction, a reason causes a result but is not affected from it. In a sense, the result fully depends on the reason, but the reason is independent of result it causes.\\n\\nIn Einstein field equations, the force relation works with a rather strange logic. Here too, the reason (the mass that creates the space-time curvature) is not affected by the result (the object following the shortest path along this curvature). There is also another unusual breaking off here: isolation of mathematics (geometry) from physics. The left side of the equation is the curved space-time geometry, while the right side is the physics thought to form this geometry. Physicist John Wheeler describes this situation as: \"Space-time tells the matter how to move, and matter tells the space-time how to curve\". The \"cause\" and the \"result\" guide each other, but any one of them cannot affect the other, because the one is abstract mathematics, and the other is tangible physics.\\n\\nWe can hardly talk about cause-and-result relation in quantum physics. There is even no guarantee that a cause will have an expected consequence. There is complete uncertainty prevailing there and only possibilities are in question. Some basic particles of constant magnitude are responsible from force relations, so the reason (force) is not influenced by its result, such that the interaction process in micro and macrocosm work in totally different way. Therefore, efforts have almost no chance to achieve a unified theory with traditional physics logic.\\n\\nOn the other hand, the cause and result (action and reaction) in the Field-Relative Model of the Universe (UF-ME) are interpreted together and inseparably (spotlight 10 in www.researchgate.net/publication/322317299). As you can see below, the right side of the equation is zero, and in the expression on the left side, the cause and result are considered together in a way that cannot be separated from each other. So, a reason causes a result and is affected by this result. In other words, a result emerges for some reason and becomes a part of the reason which is the source of its entity. Since the \"UF-ME\" is valid in every scale from atomic structure to cosmology, this form of relationship is universal. This way of operation in whole nature makes the dynamism and ceaseless change inevitable.\\n\\nSuch functioning in the nature is known as \"dialectic of nature\". It can also be called as \"dialectic of interaction\" or \"dialectic of action and reaction\". The dialectical method emerged in the ancient Greek philosophy as a \"reasoning method using the antagonisms\". Since 19th Century German philosopher G.W.F. Hegel interpreted the universe as a concrete idea, he added a material content to the dialectical relation. Friedrich Engels another German philosopher defined the dialectic as the scientific theory of totality in the context of the relationship of opponents in his unfinished work \"Dialectic of Nature\". \\n\\nThe dialectical method was used in ancient Greece to reach the right result in idea debates. In Marxist literature this was the fundamental method to explain the historical evolution of societies. The UF-ME shows that the process of interaction in physical nature, based on action-reaction relation, is also a complete dialectical process. At each stage after the interaction process starts, the cause (force) is not the same as the one in previous stage, because it is affected from the reaction it created. The \"result\" in later stage will also change accordingly (continuous dynamism). In this process, cause and effect cannot be separated from each other and should be evaluated together (unity of opposites). In a sense, \"God does not play dice\". In my opinion the relativistic approach to the interaction in nature is the formulation of dialectic of nature and the UF-ME is the mathematical expression of this concept. \\n\\nMost readers know now, how this dialectical method (UF-ME) provides perfect solutions to gravitation and other major mysteries in cosmology and astronomy that cannot be explained by contemporary physics (www.researchgate.net/publication/321474644 and www.researchgate.net/publication/317845079). Please keep in mind that the UF-ME will present its principal revolutionary solutions at atomic and subatomic level. You had better follow me. /n/r Space/time is faster the closer you are to gravity right? (See blackhole scene on interstellar) Doesnt that mean that the age of the universe is different on earth than in intergalactic space? \\n\\nIf that\\'s true than the 13.5 billion earth years age of the universe would be alot less away from the gravitational pull of earth, the sun, and the milky way galaxy.  How do i figure the math?Any physicists here? /n/r Some more of the pyramid complex in Belize. /n/r I can\\'t Help It. I remember all too well that - in many ways\\n\\x97 I grew up in a DIFFERENT Country.\\n\\n(It\\'s not as if \\'my\\' government\\'s actions didn\\'t disgust me \\'way back when\\' Too; but Jeebus\\n\\x97 At Least it was run by GROWN UPS...)\\n\\nOf course the network media conglomerate \\'handled\\' those criticizing \\'our\\' government Then as it does Now:\\n\\x97 \"You\\'re FIRED!\"\\n\\nYears later, alas, MSNBC *Superstar* Phil Donahue didn\\'t get the memo. /n/r I wish to complain about some recent actions by two moderators of this group. I\\'ve sent a Facebook private message one of them and have received no response in 24 hours. What are my options? /n/r Hey! ...FREE! :O /n/r Personal caveat, and as I am on my 2nd Sat. nite cocktail before watching my Sat. Nite Movie I find myself hoping I don\\'t \\'ruffle feathers\\' around here, which is NOT my intent. (How many times need I mention my high regard for this group, it\\'s admin. team - several of whom are valued friends - and Most of its denizens? [I don\\'t like some & they don\\'t like me, LOL. So it goes.]\\n\\x97 I\\'m making \"observations\" here, not expressing \"Criticism\" [as anyone who\\'s been the target of my criticism would likely \\'second\\']).\\n\\nThis is the \"S, T & S\" page. Has anyone performed any kind of analysis re. the ratio of \"S & T\" compared to \"Society\" (as exemplified by Anti-Trump\" articles) lately?\\n\\nPlenty of denizens have complained over the months about the preponderance of what they regard to be *Wholly Political* oriented posts. I\\'ve never been one of them. I learned Long Ago that I can \"skip\" posts I\\'m not interested in and go to those I *Am* interested in. The \\'funny thing\\' (considering I\\'m writing this) is that\\n\\x97 I *like/Read - SHARE* veritable TONS of material pertaining to our Economics and Politics, so this Isn\\'t a \"Complaint\" on my part. (This is no surprise to folks who \\'drop by\\' my home base.)\\n\\n*\\n\\nBut I confess I\\'m a tad surprised at the VOLUME of material here focused Entirely on the HORROR that is \\'Our\\' Government today... *Specifically TRUMP*.\\n\\nTo anyone who\\'d be STUPID enough to suggest I\\'m not as DISGUSTED as *Anyone* else about what\\'s goin\\' on I\\'d respond \"Have Fun PROVING IT (Idiot):\\n\\x97 You\\'re F*CKED!\"\\n\\nBut if we\\'re going to be so acerbic \"in the name of sharing re. *SOCIETY*\" shouldn\\'t we address The ENTIRE CORRUPT SYSTEM instead of just today\\'s Obvious and JUSTIFIABLE Target; like how Both parties \"Say one thing while acting in a way that Proves what they\\'ve Said are LIES\" and \"How the MEDIA is as beholden to the *Powers That BE* as \\'our\\' government representatives are to The Oligarchy*?\\n\\nShould I be sharing the TONS of material I share \\'at home\\' and elsewhere concerning this subject matter? I hold back because of the \"S & T\" aspect of the group. (I just shared - under the aegis of \"Society\" - the HUGE expos? from James Risen\\n\\x97 but That CRITICAL story about the impact On our Society because of Big MEDIA *Excoriates* - JUSTIFIABLY - the Democratic Party [under the Obama administration] as much as it does those of George W. Bush and the GOP.)\\n\\n*\\n\\nJust my Two Cents (being a long time resident here who hopes my shares are usually considered \\'worthwhile\\' [as the responses to such generally suggest]). I\\'ll just wrap with:\\n\\n? I hope that those here who love a lot of the content here excepting all the *Partisan* Politicizing know how to \"Skip\" those posts they regard \"inappropriate\" - those they don\\'t Like - as I do. And:\\n\\n? I participate on a large Multitude of FB groups dealing w/everything from literature to music to economics to, of Course, politics and more...\\n\\x97 and I see more anti-Trump threads here than I do on most sites Solely DEDICATED to ripping that bastard \"a New One\". (Is it just \\'Me\\' - and need I mention that eviscerating that P.O.S. w/out condemning all those Other A-HOLES in his party today is all but Inexcusable? [But then again, there IS *Plenty* on this page vilifying those fiends too, No Doubt - LOL.])\\n\\nAgain: I understand why that \\'bothers\\' some folks even if it Doesn\\'t Bother ME in the Least. (If this isn\\'t the \\'same page\\' I joined ages ago, \"Okay by Me\":\\n\\x97 I subscribe to *Evolution*. [But some \\'things Have changed around here\\' haven\\'t they?])\\n\\nWot It IS... /n/r A friend of mine posted this as her Header Photo for her group page.\\n\\nAwesome. /n/r It has just been revealed that a new FB algorithm is now being used to read the minds of users and, worse, this program is capable of implanting irrational ideas and even anti-WWE propaganda directly into your cerebral cortex.\\n\\nNASA advises users who oppose this intrusion to always wear an aluminum covered hat when on the FB site.\\n\\nTaking this precaution will prevent FB from hijacking your brain 98% of the time.\\n\\nShould you be among the unfortunate 2%, stand at least 9.5 feet from your computer and type and mouse with a 10 ft pole.\\n\\nDoug Hullander /n/r The year 2018 has opened with an international campaign to censor the Internet.\\n\\n\\'Governments and corporations escalate Internet censorship and attacks on free speech\\'\\nWorld Socialist Web Site\\n\\nhttp://www.wsws.org/en/articles/2018/01/06/pers-j06.html\\n\\n[Upcoming video livestream of a discussion on Internet censorship.] /n/r Any comments from actual specialists? /n/r Cheap drones.\\nLocal gas station for 5$ miniflyer? /n/r Post deleted due to lack of credible source. Fox is now considered propaganda. /n/r Almost Full Moon On NWT Highway 3 about 170km from Yellowknife, Canada. The sun had set and the colors in the sky were intense. #moonphoto #moonphotography #skyphoto #skyphotos #sunset #sunsetphoto #sunsetphotography #spectacularnwt http://bit.ly/2lPn0CX \\n\\nhttps://twitter.com/EclecticBlogs/status/948665171899187200 /n/r Snowflake (slang)\\nSnowflake as a slang term involves the derogatory usage of the word snowflake to make reference to people. Its meaning has varied, but may include a person who has an inflated sense of their own uniqueness, has an unwarranted sense of entitlement, or is easily offended and unable to deal with opposing opinions. Generation Snowflake, and snowflake are a politicized insult. /n/r ....even the horses are saying H_ _ _ No to this Winter weather! ?? /n/r YOUR PUBLIC LANDS SOLD ON THE INTERNET. At 9:00 a.m. on February 2, 2018, 2 million acres of Utah\\'s Bears Ears National Monument and Grand Staircase-Escalante National Monument will be immediately available for mining and drilling. Without paying a dime to the federal government, speculators will be able to stake a claim to mine for uranium, potash, and any other mineral that they believe can be extracted from the monuments. The oil and gas resources in the area, meanwhile, will be eligible to be sold off to energy companies through recently established private internet auctions and anonymous bidding systems that are highly susceptible to waste, fraud, and abuse. Fight back at MonumentsForAll.org. This is from the Jan 4th Progress Report. /n/r What is an Expert? An expert is a person with extensive knowledge or ability based on research, experience, or occupation and in a particular area of study and skills through study and practice over the years, in a particular field or subject, to the extent that his or her opinion may be helpful in fact finding, problem solving, or understanding of a situation..... /n/r Reported post deleted and member advised against click bait sources. Yabberz is click bait. /n/r GOP and DNC.... /n/r From Steve Cooperman... :) /n/r Multiple Sclerosis or MS as from the Mayo Clinic. In T1 weighted images, fat is bright and cerebrospinal fluid (CSF) is dark; with T2 weighting, fat is dark and CSF is bright. Proton density (PD) weighting produces image contrast on the basis of the PD of the tissue. Such weighting of images has been exploited to identify inflammatory demyelinating lesions at different stages in their evolution....\\nhttps://www.mayoclinic.org/\\x85/m\\x85/symptoms-causes/syc-20350269 /n/r Flat Earthers.... 3:)\\nhttps://www.facebook.com/photo.php?fbid=1573302639431167&set=gm.10157182908107715&type=3&theater&ifg=1 /n/r A Trusted Trump Confidant.... /n/r Post reported and deleted for spam. Member warned. /n/r A little late... But way cool! 3:)\\nhttps://www.facebook.com/photo.php?fbid=2025423991006090&set=gm.10157177211152715&type=3&theater /n/r From Susan Ludwig... 3:) /n/r What is the PURPOSE of government? /n/r A poison is found on the skin of a species of a genuine poison dart frog. In northern Choc? Department, Phyllobates aurotaenia is used, while P. bicolor is used in Risaralda Department and southern Choc?. In Cauca Department, only P. terribilis is used for dart making. The poison is the batrachotoxins in P. terribilis are powerful enough that it is sufficient to dip the dart in the back of the frog without killing it..... /n/r ...And counting...\\n(link thx to Charlie Kent!) /n/r The U.S. Navy and Army made the first attempts to fly to Hawaii. It was no easy matter. Hawaii lay about 2,400 landless miles from San Francisco\\x96a tiny navigational target in a vast ocean. Few planes could fly that distance without refueling. But Charles Lindbergh\\'s historic transatlantic. The flight in 1927 made many aviators eager to try, and crew wave from their PN-9. The Navy stationed ships every 200 miles to guide the flyers, refuel their planes, or rescue them if trouble arose. /n/r For all you Scientists who believe Time is unreal Happy ???????????? /n/r One of the most accurate weather reports I\\'ve ever seen. /n/r Static Friction keeps a stationary object at rest! Once the Force of Static Friction is overcome, the Force of Kinetic Friction is what slows down a moving object.... /n/r The \"law of physics\" is the Pauli exclusion principle, which states that two identical fermions (particles with half-integer spin) cannot occupy the same quantum state simultaneously.... /n/r Lad Akins, Director of Special Projects at REEF, said in one presentation not long ago that studies have shown that lionfish can live without food for up to 3 months and only lose 10% of their body mass.\\n\\nhttps://lionfish.co/lionfish-faq/ /n/r Remembering 2017...\\n(pic thx to Ben Kirchner & The Washington Post!) /n/r For those interested in technology for the ongoing new area of human healthspan and lifespan, here is the International Longevity Alliance annual report for 2017. http://longevityalliance.org/?q=ila-annual-report-2017\\n\\nPlease don\\'t hesitate to suggest things to add by sending an email to contact_a t_longevityalliance_d o t_com\\n\\nOf note, to respect the statutes the board will undergo some renewal soon, also new applications of non-lucrative associations as ILA members are welcome. http://longevityalliance.org/?q=partners http://longevityalliance.org/?q=ila-membership-application-eligibility /n/r Science was a big part of the dumpster fire of 2017.   https://www.facebook.com/photo.php?fbid=1833325163367762&set=a.1275702055796745.1073741825.100000708879267&type=3&theater /n/r Vladimir Putin is a LOT smarter than Donald Trump... /n/r Have we all been \"buying the Brooklyn Bridge\"? :O /n/r A thought for our New Year... /n/r Interesting analysis! /n/r Wow! (Y) /n/r Which infusion I could drink to see the context from the observation point of a ladybug! A Hamlet doubt or a hidden desire? This is the question! /n/r Love your Mother...\\n...board! :O /n/r Forces, mass   and acceleration! :O\\n(Hey! ...FREE!) /n/r Solving the World\\'s Problems through Creativity (cont.) /n/r Roy Moore... /n/r You will get A CHARGE OUT OF THIS!! :O\\n(Hey! ...FREE!) /n/r What is universal force-motion equation (UF-ME)?\\nThe UF-ME is an expression of all force and motion relations between all entities in nature with a single and simple mathematical formula. The equation expresses all the entity energies of the masses together with spin and linear motion energies in the cause and effect context.  The figure below shows how this equation is formed in the case of two bodies. For an n-body system the equation can be generalized in the form of matrices. The figure shows the escape and spin field velocities of each mass at the point where the other mass is as well as their own spin and linear velocities. For the UF-ME, it is a decisive factor for how each mass perceives the total velocity fields that the other produces. The relative effective field distribution factor ?R, is obtained by dot product of two relative total field velocities perceived by each mass. The UF-ME is the gradient of (?_R/r). \\nSome basic properties of UF-ME:\\n Owing to the dot product of perceived total field velocities of each mass, the UF-ME both preserves the excellent symmetry of Newton\\'s universal law of gravitation and implements Einstein\\'s relativistic concept in its true nature.\\n The force equation F=-GM_1 M_2 ?(?_R?r) is reduced to Newton\\'s universal law of gravitation if the masses have no kinetic energies (k_i  = 0 and v_i  =0), i.e., Newton\\'s law applies only to systems that are stationary relative to each other. The Newton law will lose its validity as the interacting objects accelerate. \\n The equation uses the relative velocity of the masses to each other, the total mass of interacting objects and mass size ratio owing to the true inertial reference frame.  In this way, it becomes applicable the force relationship between any two masses each may have any size. For example, if the two interacting masses are equal then ? = 1, and if it is the sun-photon relationship or M_1?M_2 then ? = 0. \\n The relative effective field distribution factor ?_R  of UF-ME includes both the causes (field velocities) and the results (kinetic energies of masses). In other words, the UF-ME interprets the cause and effect of an interaction simultaneously in a single and simple mathematical expression. \\n In general, the relative effective field distribution factor ?_R has also polar and azimuthal components due to the relative linear and spin velocities of the masses. Therefore the solution of UF-ME gives lateral force components besides the main radial one. \\n The UF-ME shows that gravitational repulsion is also possible under certain circumstances (www.researchgate.net/publication/317845079). We only familiar with the radial gravitational component (attractive only) from the conventional physics. \\n The polar component of interacting force causes small masses to cluster at the equatorial plane of the main body (Spiral galaxies, planets positions in Solar system, Saturn\\'s rings, most recent observation of positions of Uranus\\' moons on its unusual equator plane etc.)  (Conventional theories require globular clusters only due to their spherically symmetric nature.)\\n Azimuthal component causes all members of a balanced cluster system to revolve in the same direction which is the spin direction of the main body in the system (spiral galaxies and motion of the planets in the Solar system). (The spherical symmetric force relation requires rotating in random directions.)\\nThe UF-ME is a single expression which is valid in every scale in the Universe. Readers can estimate that in the cosmic scale and visible universe with naked eye M_i?1 unit mass and r?1 unit distance while k_i?1, but in atomic and subatomic scale M_i?1 unit mass and r?1 unit distance while  k_i?1. The universal gravitation constant G  in UF-ME serves in every scale because it is a parameter specifying space fabric as indicated in one of previous spotlights. (In contemporary physics  G has no role at all in atomic scale). /n/r Donald Trump\\'s least presidential moments so far... /n/r Donald Trump\\'s least presidential moments so far... /n/r Donald Trump\\'s least presidential moments so far... /n/r Donald Trump\\'s least presidential moments so far... /n/r From M Patricia McLaughlin... (Y) /n/r Immigrants FROM Silicon Valley are taking all your jobs! :O\\n....AND your salaries!\\n(link thx to Patricia Darling!) /n/r For their relentless work against the environment, the poor, and the planet. /n/r They only needed to convince just enough voters in those precise Electoral College districts that would put Trump over the edge...\\n...AND THEY DID. Trump actually won with an advantage of less than 100 thousand votes in just the critical state districts... /n/r *DEEP Science* (cont.) /n/r John Lundin writes: Sorry to share this on Christmas, but a friend shared it today and I found it interesting - and scary. You don\\'t need to be a rocket scientist to crunch the numbers and realize this isn\\'t sustainable... /n/r Society:\\n\\x97 And now, a commercial message for our rapacious would-be \\'overlords\\': /n/r This is now \"everyday technology\"   it is amazing how it works   for EVERYONE! :O\\nhttps://www.facebook.com/kgrhoads/posts/10210842465601206 /n/r (I saw and had planned to share the article before I saw my friend\\'s post.\\n\\n\"Net Neutrality: The Coming Battles in the Congress and the Courts\"\\nhttps://www.nakedcapitalism.com/2017/12/net-neutrality-coming-battles-congress-courts.html\\n\\nBut I like her graphic so I\\'m sharing the article as listed in her share...) /n/r 3d printing and the next steps? How much can be \"printed\" at home? /n/r Vr and Ar and virtual reality and augmented reality? The next steps? Show things? Interact with things? And record things and data tracking? /n/r SOMETHING WE ALL NEED TO THINK ABOUT\\n\\nPaleontologists, archeologists, evolutionary biologists and biochemists concur with near unanimity. Upwards of 99% of all species that once walked the earth, that once burrowed in the soil, inhabited the bodies of others, swam in the oceans and rivers or flew in the atmosphere above...are no more. They are extinct. The scientific data ( the fossil record ) indisputably confirms it. Again 99%...gone!\\n\\nIn almost every instance, organisms disappeared because of their failure to adapt to a changing environment: they failed to adapt to drastic weather alterations; failed to find new habitats after theirs was destroyed; fell victim to invasive predators; or they were annihilated by asteroid impacts, etc.\\n\\nFaced with an inability to cope, to change with conditions that threatened their very survival, they succumbed en mass, completely disappearing, never to return again.\\n\\nAnd so humans must face the age old question: will our species follow suit or will we be the exception to this fatal fate of our own species ? \\n\\nWill our superior intelligence enable us to beat those odds? Will we able to adapt...to outwit what has been the inevitable death knell for most every other creature?\\n\\nOr will our willful ignorance, our false sense of superiority and invulnerable result in the exact opposite...will these foibles be our undoing...will they result in the same fate that befell the doomed 99%?\\n\\nFurther, do we have the will to determine which direction our intellect will lead us? Will our failure to mitigate the effects of climate change with all its dire consequences spell the eventual end? Will over use of anti-biotics do us in? Will thermonuclear global warfare be the end of our reign on earth?\\n\\nOur planet has already undergone 5 mass extinction events. We are overdue for the 6th. Will we be its victims?\\n\\nFifty, one hundred, one thousand, ten thousand years from now, will we have become extinct like the thousands of other species. Could we have prevented, but because of sheer stupidity didn\\'t? \\n\\nAs Carl Sagan once said, \"there is no evidence that help will ever come from any outside source to save us from ourselves. It\\'s up to us and us alone\". /n/r WHY are National Parks and Monuments and other Public Lands so important? Why should places like Bear\\'s Ears, Grand Staircase - Escalante, Cascade-Siskyou, Katahdin Woods and Waters and Giant Sequoia National Monuments be protected? \\n\\nHere is just one reason - without them, only the rich would have access to lands.\\n\\nPer the Washington Post, the top 100 private land owners in the U.S. own a combined 40.2 million acres, an amount of land roughly equivalent to Connecticut, Rhode Island, Massachusetts, New Hampshire and Maine combined. That\\'s up from only one New Hampshire and Maine\\'s worth of America (27.2 million acres) in 2007.\\n\\nAnd this argument completely sets aside the local economic, ecological and scientific value of these lands.  There is no valid excuse for removing protected status UNLESS you intend to capitalize the resources and damage the lands. /n/r Whom do you consider as YOUR family? :O /n/r Certainly!   Of course! (Y) /n/r From Kevin G. Rhoads... /n/r If you were to read this book and absorb even a 10% of it you\\'d fall in love with your mind. /n/r Its a good start towards fixing things.Even with 45 running things, they\\'re doing something right.\\n\\nhttps://www.npr.org/sections/health-shots/2017/12/18/571666553/food-and-drug-administration-plans-crackdown-on-risky-homeopathic-remedies /n/r DLCC Breaking News (via DLCC.org) \\n  \\n\"Last-Minute Tax Break Could Enrich Trump And GOP Leaders\"\\n\\x96International Business Times, December 15\\n\\nNow we know why Donald Trump\\'s tax bill was written entirely behind closed doors.\\n\\nTo win the last few votes necessary to ram their bill through, GOP leaders reportedly inserted a last-minute loophole that will save themselves and Donald Trump\\'s businesses millions of dollars, while the rest of the bill raises taxes on tens of millions of middle-class families.\\n\\nThe two most prominent beneficiaries of this crooked deal? Tennessee Sen. Bob Corker \\x96 who flipped his vote from NO to YES within hours of the final bill\\'s release \\x96 and Donald Trump himself.\\n\\nMeanwhile, 50% of Americans expect this bill to raise their own taxes to pay for Trump and Corker\\'s sweetheart deal. /n/r Congenital Insensitivity to Pain(CIP) /n/r \\'Pai\\'s the Limit\".  Great, I can\\'t wait. :/ /n/r One practical application of my cube theory would be an extremely strong support material. It could be made of very light-weight material and be extremely stable and withstand tremendous weight. /n/r Ended this day....1916 Bataille de Verdun met fin ? la bataille de Verdun, le plus long engagement de la Premi?re Guerre mondiale, s\\'ach?ve en ce jour, apr?s dix mois et pr?s d\\'un million de pertes totales subies par les troupes allemandes et fran?aises. La bataille avait commenc? le 21 f?vrier.... /n/r WOW! Northern Lights seen last night, Dec 17, 2017 from Troms?, Norway. Photo credit: Marianne Bergli. #Aurora #NorthernLights #Norway\\n\\nhttps://twitter.com/mark_tarello/status/942742299867525120 /n/r Personally I would rather discuss science on this page rather than politics. There are plenty of other forums to discuss politics. /n/r Shakespeare - Hamlet /n/r No matter how many doors you lock, someone will find their way in. /n/r This facebook page seems more dedicated to issuing threats to its members and censorship than it does to Science and Technology... I don\\'t even know how I got signed up for this page... I\\'m done with it, bye. /n/r REGENERATION, RENEWAL OF THE NERVUS TISSUE, DIVISION OF NEURONS. SENAD ?e?erbegovi?  25. Mar 2017 The frogs can regenerate neurons and nerve tissue, healing is similar and the same principle as dermatoglyphic findings in humans (genes shape), proteins in this process are in two places, those in the cell wall of a few, over the signal broken off the molecules occur hromatin protein appearance of tissue damage, and then activate the sharing of cells to fill the space tissue damage, when the damage is filled in, the shape of the tissue, the surface station sent a signal molecule that occurs hromatin in protein to stop cell division. In humans they are, and at possible dermatogliphy frog parts of nerves and organs, though this upgrade people you need to add a larger number of designed protein, for now though just AFM research, because we need to understand the Insert protein on the default locations. human neurons, have mechanisms for repairing and replacing hromatin and other proteins, but for now they don\\'t have the Division ... cartilage as a supportive tissue, in some cases, it can function as a bone, if it has a greater firmness. /n/r Stock Market Arbitrage.... /n/r Higher Education.... /n/r The difference between being a cynic and a satirist.... /n/r Sexual Harassment....(I\\'ll be hanged for this) /n/r The Story of Christmas.... /n/r And   turned in grades for Fall semester!\\n...Only one student (out of 34) failed the course!\\nhttps://www.facebook.com/notes/william-a-boyle/some-thoughts-for-the-end-of-the-semester-/10150182386345686/ /n/r ...What\\'s WRONG with this picture? :O /n/r Some said that the old fiat cu?rency system by the Fed steals the wealth of the working and middle class people for the rich and super rich.  \\n\\nWe know now that Fed manipulates not only the markets but the monetary system in favor of the super rich thus, a new paradigm was invented by Nakamoto in 2009.  Called the digital crypto currency, it was introduced to get out of the manipulation and control of the Fed. \\n\\nBankers says it is a hoax. But crypto currency is rising. One reason perhaps  is no bank intermediaries thus no interests. \\n\\nIn fact, not only the people are investing into cypto currencies but countries too, like Venezuela is into petro coin now. It is backed by oil. Russia, Syria and Iran too, are thinking of going into crypto currency to fight the sanctions by the US. Possibly, North Korea will jump into it, too.\\n\\nJust recently, however, the bankers  set out a futures trading on crypto currency. Some experts on crypto currency said, however, it is not affected by the chronic cycle of bubble and bursting as the old fiat currency system of the Fed. \\n\\nWhat do you think about this on going \"war on cu?rencies\"? Will the crypto currency of the people finally end the control and manipulation of central banks? /n/r How many people  here think that autonomous vehicles are actually a good idea? It seems like a hack of the system by terrorists would be horrific even if it doesn\\'t happen very often. And it seems like it could interfere with evacuations from hurricanes, fires, etc. if the servers don\\'t have power. /n/r Post deleted due to non credible source and information. Member advised. /n/r WOW! Meteor seen Wednesday, Dec 13, 2017 night from Ibaraki, Japan. Photo credit: KAGAYA. #Meteor #Space\\n\\nhttps://twitter.com/mark_tarello/status/942078996795285504 /n/r Federal Communications Commission /n/r Height of Idiocy.... /n/r A good attitude\\nhttps://m.facebook.com/story.php?story_fbid=10155702827472745&substory_index=0&id=45052217744 /n/r There is a limit.... /n/r Hope for people with this annoyance.. /n/r Anti-censorship game.\\nSuggest Alternative words or phrases for each below.\\nSuggest means for fighting, I.e. undermining, censorship in government.\\n\\nEvidence-based\\nScience-based\\nFetuses\\nTransgender\\nVulnerable\\nEntitlement\\nDiversity /n/r Do you still have some of these around? :O /n/r Terms I Dislike.... /n/r 3 book recommendations for those who want to understand better the future during their x-mas holidays:\\n\\n- ZERO TO NONE, by Peter Thiel\\n\\n- BOLD, by Peter Diamandis\\n\\n- THE FUTURE IS WAITING, by... Martin Gallardo\\n\\nI was very excited to get the first copies last night and I could not wait to share it with you. Very grateful to all the people who read it, so far very positive reviews.\\n\\nYou can get the .pdf version here\\n\\nhttps://martin-gallardo.com/products/the-future-is-waiting?variant=44909386258#qcref=true \\n\\nAnd the Amazon .kindle version here\\n\\nhttps://www.amazon.com/Future-Waiting-compilation-mind-blowing-predictions-ebook/dp/B076YWJVGN/ref=sr_1_2?s=books&ie=UTF8&qid=1513346168&sr=1-2&keywords=the+future+is+waiting /n/r American politicians.... /n/r A little Tuesday Morning Funny. /n/r KYLO KILLS SNOKE /n/r Anytime someone puts a lock on something you own, against your wishes, and doesn\\'t give you the key, they\\'re not doing it for your benefit. /n/r No one has rights over me. /n/r Post deleted as to being a conspiracy theory. Member informed. /n/r After a year   are there any doubts? /n/r Bored of the Rings..... /n/r An amazing historical account of how these two geniuses interacted with each other... /n/r WOW! Northern Lights seen last night, Dec 13, 2017 from Troms?, Norway. Photo credit: Marianne Bergli. #Aurora #NorthernLights #Norway\\n\\nhttps://twitter.com/mark_tarello/status/941114467034726405 /n/r Be prepared to open your wallets and pocketbooks... /n/r Passion is inversely proportional to the amount of real information available.... /n/r Which is more important, community, or individual freedom? /n/r After decades of war and spending 4.4trilion dollars the US is losing ground in the middle east. Although I am surprised at first withTrump\\'s latest move in Israel, it isn\\'t difficult to understand his decision.   That is if one considers the current situation of the US economy it hasn\\'t recovered since the 2008 financial mojo. \\n\\nOf course, we all know that proclaiming Jerusalem as the capital  of Israel was used for a long time as a leverage to Israel. I\\'m sure Trump fully knows well the direct consequence of his move it will unify the Arabs against Israel.  We are seeing this already. \\n\\nIn my view, his audacious move is another way of saying \"let us leave the middle east and let Russia and China do the peace process.\" Of course, Trump is fully aware of the consequences in getting out of the middle eastern war. \\n\\nWhat do you think? /n/r What do you think? \\nIt\\'s okay, Pluto can\\'t hear you! \\n\\nhttps://m.facebook.com/groups/799675960211308?view=permalink&id=831928046986099 /n/r An amazing SF author! (Y) /n/r WOW! Northern Lights seen last night, Dec 11, 2017 from Fairbanks, Alaska, USA. Photo credit: Sacha Layos. #Aurora #NorthernLights #Alaska\\n\\nhttps://twitter.com/mark_tarello/status/940567425099206656 /n/r Here\\'s what the Ole Farmer\\'s Almanac calls for this winter. /n/r THIS IS SCIENCE\\n\\nDiffering opinions are fine and are to be expected when the topics are somewhat nebulous, uncertain or not quantifiable. \\n\\nBut in regards to evolution and to certain findings in cosmology, the uncertainty has largely been removed by convincing evidence. \\n\\nThat\\'s not to say there will never be modifications, all theories in science are amenable to revisions as discoveries are made, but the basic principle of evolution, in particular, is as if carved in Stone. \\n\\nThe same holds true for other scientific principles. Examples...Newton\\'s  precepts in classical physics were modified by Einstein\\'s theory of relativity and HIS ideas were modified by quantum mechanics. \\n\\nSuch flexibility is the hallmark... the nature of science. Driven by new discoveries, it\\'s forever self-correcting, always improving upon itself...always searching for the truth...and in doing so it hopefully improves the world and the human condition. /n/r Comment deleted due to lack of credible source. Please use discretion when responding. /n/r \"There is a great deal of research to establish a strong relationship between career development and student background, particularly socioeconomic status (Hill, Pettus, and Hedin 1990; Mestre and Robinson 1983; Rolle 1977). Scientists tend to come from well-educated white families (Grandy 1994; Pearson 1986). Lack of knowledge and familiarity on the part of underrepresented minorities in terms of what constitutes careers in STEM may contribute to their limited presence in these fields (Hill, Pettus, and Hedin 1990). Knowledge about STEM careers and exposure to scientists\\nand engineers have been found to increase minority students\\' commitment to a STEM major, degree aspirations, and commitment to a STEM career (Good, Halpin, and Halpin 2001; Rolle 1977; Wyer 2001).\" /n/r Monday morning funny! /n/r World\\'s first mindfile powered artificial intelligence robot to complete a university course: Philosophy of Love. /n/r For me it is like a painting of elegance and harmony. /n/r Don\\'t forget, folks!   \\nThis is the kind of LIBERTARIAN right-wing DEEP BRAINWASHING that we\\'re up against! :(\\nhttps://www.facebook.com/photo.php?fbid=10214616808792073&set=a.4197367970637.180064.1177861531&type=3&theater /n/r Light and sound show! :O\\n(link thx to Alex Maldonado!)\\nhttps://www.facebook.com/donatodandre/videos/10203579772852199/ /n/r \"Information is a \\'lock-and-key\\' phenomenon  \\nAs you do not know which locks you will come up against in your life, it is best to obtain as many keys as you can as early as possible   \\nThis is known as getting an education.\" /n/r Nuremberg Trials... /n/r (y) Like Lifestyle & please Share! /n/r How long has this been going on? /n/r BRAIN WASHING\\n\\nIndoctrination, be it religious, political or otherwise is a disease of the mind, often a permanent part of one\\'s make up.\\n\\nOnce this mental programming is inculcated into young minds, especially, the thought patterns are almost impossible to erase.\\n\\nThus, the brain washed individuals are impervious and resistant to reasoning. They insist on maintaining their belief system no matter how improbable and unreasonable they may be, often becoming defensive when confronted.\\n\\nThree examples that come immediately to mind are radical Islamists, racists and Christian fundamentalists ( evangelicals). /n/r \"And these blast points, too accurate for Sandpeople. Only Imperial Stormtroopers are so precise.\"\\n\\x97 Ben Obi-Wan Kenobi /n/r (La prova di una particella  contenente quattro tipi di quark ? stata evidenziata per la prima  volta dai dati del collisore Tevatron presso il Fermi National Accelerator Laboratory (Fermilab) in Illinois. La nuova particella ? composta da un quark bottom, uno strange quark, un quark up e un quark down. La scoperta potrebbe aiutare a chiarire le complesse regole che governano i quark: le minuscole particelle fondamentali che formano i protoni e i neutroni all\\'interno di tutti gli atomi dell\\'universo.)\\nThe evidence for a particle containing four types of quarks was first shown by data from the Tevatron collider at the Fermi National Accelerator Laboratory (Fermilab) in Illinois. The new particle consists of a bottom quark, a strange quark, a quark up and a quark down. The discovery could help to clarify the complex rules that govern quarks: the tiny fundamental particles that form the protons and neutrons within all the atoms of the universe /n/r So - Where are all these new \"NAZIS\" coming from?\\nDoes anyone wonder how this \"self-radicalization\" happens?\\nIs it fundamentally different from the online \"self-radicalization\" of the ISIS bozos of a few years back?\\n...Is it \"natural\" or is it being COVERTLY promoted? /n/r Comment deleted. Members must not insult while discussing topics no matter who is right or wrong. /n/r How bad does the smell of gas have to get before they evacuate us. Maybe after we all start puking and passing out?\\n\\n\\'Near walkout at Detroit auto plant over unsafe conditions\\'\\nWorld Socialist Web Site\\n\\nhttp://www.wsws.org/en/articles/2017/12/07/jnap-d07.html\\n\\n[Ambulances and Fire Department emergency vehicles at plant Saturday morning.] /n/r Just WHAT are the Repubs trying to PROVE? /n/r QUESTION: Shortly after the \"Big Bang\", it must have been dark; right?  Until the Higgs Field slowed subatomic particles by giving them mass, which allowed electrons to bind to protons; there was no matter.  You need hydrogen condensed by gravity to start fusion, so how long till the light came on?  Thanks. /n/r Tech question...on Facebook.  I notice they have no live human help available.  My personal page news feed is not loading at all anymore.  Every other FB page loads normally.  Nothing has changed on my edge and two hours of searching for an answer has yielded nothing thus far.  I can still read PMs and my denny page loads fine.  Just missing the news feed.  PM me. /n/r Why was the discussion about ET life, that became a discussion about possible microbes in outer planet ocean(s), get terminated? I thought it was relevant, current and educational. /n/r Thanks a lot to the team and members for accepting me as a member of it. I am really glad. /n/r Would anyone like to discuss Extraterrestrial Life in association with Science, please? /n/r Without looking it up, do you know the next large number name after trillion? /n/r GOD IN THE MACHINE https://www.theguardian.com/technology/2017/apr/18/god-in-the-machine-my-strange-journey-into-transhumanism /n/r Marine plastic pollution is right out of hand. /n/r So... In this reality if you kill one person you will be hunted until the day you die or until you are brought to justice, however if you SALUTER AND MAIM THOUSANDS of  people, a few who just happen to be your fellow countrymen... People will smile in your face a serve you blindly until they day you die AND they will erect statues and build airports dedicated to you.... Got it... /n/r Is it \"Deja vu, all over again\"? /n/r Jews and the Palestinians.....(My Son) /n/r Magical..Super moon from Aberdeenshire, Scotland, this morning, Dec 4, 2017. Thanks to Gordon Robertson @gordo_rob #Supermoon #StormHour\\n\\nhttps://twitter.com/StormHour/status/937766688094871553 /n/r Now 45 knew Flynn was under investigation for lying to the FBI. More Obstruction evidence. /n/r Amazing northen lights over Torsfjorden, Norway #Space #StormHour\\n\\nhttps://twitter.com/TopAstroPics/status/937682051746148357 /n/r Or the whole thing could be faked from the git go. /n/r Spam posts for another group and for an event have been deleted.\\n\\nMembers Matthieu Ehostidc and Sandra D. Sabatini are hereby warned. /n/r ... a physics curiosity for the perversely curious... /n/r #MotivationalMondaysRead\\n*Society Discussion /n/r So the answer some seem to give is that you need the root to get DNA.  I didn\\'t know this.  A very good question though /n/r Which could have a larger quantity?\\nA. Grains of sand\\nB. Stars\\nC. Universes\\nD. Realities /n/r Northern lights #Karakok has now passed #Kautokeino\\'s lowest measurement today at -30.8 degrees and is now coldest in the country by -31.8 degrees. Photo: Kjell H. S?ter taken at Karasjok, Norway\\n\\nhttps://twitter.com/Meteorologene/status/936341971882467328 /n/r \"Making AmeriKa GREAT Again\" (for the Rapacious Kleptocrats) cont. /n/r Ren? Descartes\\' philosophy was one of the most important motivating forces for the development of the modern scientific way of thinking - and this way of thinking has dramatically improved the daily lives of so many people all over the world.\\nRen? Descartes\\' philosophy can be summarized in his quotation:\\n\"Dubito ergo cogito; cogito ergo sum.\"\\n(\"I doubt, therefore I think; I think, therefore I am.\")\\n Ren? Descartes (1596-1650). /n/r Light pillar in the evening Omsk (Russia, November 30, 2017)\\n\\nhttps://twitter.com/MeteoRUSSIA/status/937005574897758208 /n/r Innovative technology of manufacturing without glue of ecologically pure plate material from a vegetative waste (straw). The basis of technological production of such plate material is ecologically pure hydrolysis of a fibrous substance with the use of carbonic acid. Through this hydrolysis, lignin is liberated from the fiber structure, which is a plasticizer when forming a plate material. \\nThe specific mass of the material is 1.35 g / cm3, the compressive strength is 50 kg / cm2 at a thickness of 20 mm. The preliminary price of the material is no more than $ 5 m2.\\nIt is patented in the Republic of Kazakhstan.\\n100% environmentally friendly material! /n/r ....... \"#REALScienceMarchesOn in-spite of #TheScienceDeniers! /n/r This is what I call \"An Accurate Analysis\". /n/r Humanity has begun to have a scientific understanding of the magnitude of the problem   WHY do we refuse to believe it?\\n(pssst! ...Have you ever seen a bacterial colony growing in a petri dish?\\n...Did it continue to grow INDEFINITELY?) /n/r Manic Depressive.... /n/r Not to ask a question.... /n/r Here are some interesting facts. Now imagine what life would be like if they had the kind of freedoms we do, and didn\\'t have that embargo /n/r Astrophiz 48: Dr Jacinta Delhaize \\'Star-forming frenzy\\'\\nIn this fabulous extended 60min episode we  feature Dr Jacinta Delhaize who is a Postdoctoral Researcher at University of Zagreb, Faculty of Science in Croatia. https://soundcloud.com/astrophiz/astrophiz-48-dr-jacinta-delhaize-star-forming-frenzy\\n\\nJacinta has devolved a \\'stacking technique\\' to combine data to overcome the problem of detecting weak hydrogen signals from distant galaxies. She has been using data from  the Parkes Dish and the Hershel instrument to helps us understand the role of hydrogen in the evolution of galaxies.\\nAfter recently moving from ICRAR in Western Australia to Croatia,  her research is now looking at how black holes at the centres of galaxies can effect star formation, and is now using data from the Jansky VLA to continue this collaborative research.\\n \\nFor observers and astrophotographers, Dr Ian \\'Astroblog\\' Musgrave tell us what to look for in our morning and evening skies over the next days and weeks, and how to best observe the imminent Geminid Meteor Shower. In \\'ian\\'s tangent\\' he tells us about Australia\\'s early eminence in Space with the 50th Anniversary of the launch of our first satellite, WRESAT\\n \\nJacinta has an excellent youtube vid and you can see her describe her research on the infrared-radio correlation of galaxies at tinyurl.com/jdelhaize\\n \\nHer website is at www.jacintadelhaize.com and she also has a public twitter and instagram account. Both are @jdelhaize\\n \\nIn the news:  Teams of Radio astronomers and optical astronomers both research the Magellanic Clouds in the Southern hemisphere, and both come up with exciting discoveries. /n/r The less I post about Dump, the better I feel.... /n/r #ThrowbackThursdaysRead\\n\\n*Society Discussion /n/r I love a great Political cartoon: Lord help me, this \\'Hits the SPOT\\'...\\n\\x97 Welcome, AmeriKa, to the World of NOW...\\n\\n(Then again, anyone who\\'d posit this graphic transcends national boundaries and TIME/HISTORY re. <Cough COUGH!> \"Civilization\" itself;\\n\\x97 No argument from me, friends.)\\n\\nI *Still* subscribe to the Theory of Evolution.\\n\\x97 I guess I have to acknowledge it doesn\\'t apply \"Across the Board\", eh?\\n\\n*\\n\\nIt\\'s difficult to defend the \\'Worthiness\\' of Humanity sometimes. /n/r ...Does anybody really know what time it is? 3:) /n/r FREE   and with solutions! :) /n/r Will the Agung volcano be able to cool the planet\\'s climate again?\\n\\n11/29/2017\\n\\nThe awakening of the Agung volcano has alerted the island of Bali, which has forced to evacuate at least 100,000 inhabitants of the area and close the airport in the city, causing in turn that 120,000 tourists are trapped.\\n\\nBut beyond these important disorders for the local population and its visitors, there is some expectation in the scientific community for the possibility that a large explosion may happen in the coming days and that the resulting cloud of ash and gases will later cause some effect in the climate of the Planet. And it is that this same volcano already caused in 1963 a global cooling of the Earth that was estimated of two years of duration.\\n\\nAlert status in Bali\\nLast Saturday the authorities of this beautiful Indonesian island raised the alert level as the ash cloud became even more important, ordering the immediate evacuation of around 40,000 people in the area with the perspective that this week it would be necessary to evict a total of of 100,000 inhabitants.\\n\\nBut many of them are not willing to leave their belongings and their animals, perhaps because it is not the first time that the Balinese see Agung spitting ash into the atmosphere and are accustomed to its roar. The previous major eruption dates from 1963 and left 1,600 dead.\\n\\nThe temperature of the Planet dropped to two degrees\\nAfter that violent explosion of ash and gases to the Indonesian sky, the scientists detected in the two following years a global cooling of the temperature of the planet between 1 and 2 ?C.\\n\\nOther volcanoes that caused similar eruptions later, such as Mount St. Helens in 1980 (the most disastrous eruption in the history of the United States) did not have the same impact on Earth\\'s climate.\\n\\nThe explanation is fundamentally due to two factors: the presence of sulfur in large proportions and the height at which the fumes and gases are high in the eruption.\\n\\nThe relevant role of sulfur in modifying the climate\\nFor an eruption of this type to be able to modify the climate of the planet it must happen that the cloud of ash and gases is able to reach the stratosphere, an area of ??the atmosphere where these compounds can block the arrival of solar energy.\\n\\nAnd is that the high presence of sulfur dioxide makes the filtering of sunlight more effective, contributing more importantly to reduce the temperature of the planet.\\n\\nAgung is located in an area of ??the Planet that, due to its latitude, has the stratosphere at an altitude higher than other regions closer to the Poles, and this is another handicap for ash and sulfur to block the sun\\'s energy, unless the eruption is as powerful as that of 1963 and get rise above the troposphere.\\n\\nThe closest precedent that managed to modify Earth\\'s climate was Mount Pinatubo (Philippines) when it erupted in 1991, sending 20 billion tons of sulfur dioxide to the sky.\\n\\nhttp://www.cazatormentas.com/sera-capaz-el-volcan-agung-de-enfriar-el-clima-del-planeta /n/r More FAKE NEWS encountered via a Graphic Meme I stumbled across...\\n\\n*\\n\\nDo not believe that meme claiming a group of baboons is called a congress. Not only is it wrong, it disrespects baboons.\\n\\x97 A group of baboons is called a \"Troop\". /n/r Aims to be the World\\'s largest Digital Bank\\n*Society Discussion /n/r Politicians... Military Leaders... The Intelligence Community Spooks...\\n\\x97  Oh yeah: we can\\'t forget most of the mainstream media, especially on TeeVee...\\n\\nI say we should Trust Them. What about you? /n/r The first century of the American political experiment... /n/r *Fantasies*\\n\\x97 Not just for Children anymore... /n/r Have we been sold a \"bill of goods\"? /n/r WOW! Northern Lights & Light Pillars earlier this month, Nov 2017 (light reflection from ice crystals) in Norman Wells, Canada. Photo credit: Nicky Lynn. #Canada\\n\\nhttps://twitter.com/mark_tarello/status/934977900000038917 /n/r I must confess that I had never heard of this guy before this article appeared in my inbox. It seems his is a name I should have known. /n/r Post deleted for spam and advertising. Member strongly warned. /n/r Friends, many of you helped us by supporting the restoration of HMS President 1918, one of only three World War One ships left, I am pleased to say that we now have nearly all the funds needed but have one last hurdle to overcome. Richard Desmond, owner of Express Newspapers, after having ran a campaign to raise money for us in the Sunday Express agreed to allow the Ship\\'s walkway to attach to his land at London Bridge. He has now, for no reason, changed his mind and we need that access. \\nCould you email Richard or write to him at\\nThe Northern & Shell Building\\nNumber 10 Lower Thames Street\\nLondon EC3R 6EN\\nto ask him to allow the access and support us as he said he would. His email is via his PA allison.racher@express.co.uk \\nThank you so much! /n/r \"Truth, Justice and The American Way\" (cont.) /n/r WE EXPERIENCE TIME TRAVEL ALL THE \"TIME\".\\n\\nOver 100 years ago, Albert Einstein discovered that the rate at which time flows is affected by two parameters...gravity and the speed of the traveler. These two principles were foundational to both his Special and his General Theory of Relativity. Although it\\'s counter intuitive...it\\'s nonetheless true...the passage of time is not constant. It\\'s variable.\\n\\nHow so, you ask...The faster one travels relative to a stationary observer, the more SLOWLY time passes for the traveler compared to that observer.\\n\\nBUT, the weaker the gravity is for one individual compared to another, the more RAPIDLY time flows.\\n\\nIn other words, weaker gravity SPEEDS UP time passage and a faster velocity SLOWS DOWN time passage..\\n\\nSo here we have to competing affects. The higher relative speed compared to someone stationary on the ground, means time for the passenger passes more SLOWLY.\\n\\nBUT the higher one is above those on the ground, the more the flow of time speeds UP.\\n\\nJust how then do these two competing phenomena affect an airline passenger flying 500 MPH at an altitude of 35,000 feet.\\n\\nBeing 6 miles above the earth, where gravity is a tiny bit weaker than for those on the ground, means time for our passenger passes a bit faster.\\n\\nBut, since passengers are traveling over 500 MPH, time flows a little more slowly than for people on the ground.\\n\\nSo, which affect wins out?\\n\\nAs it turns out, the weaker gravity at the higher altitude, speeds UP the flow of time, more than the higher velocity of the aircraft slows DOWN the flow of time.\\n\\nThese affects have actually been calculated for a passenger who racked up 10,000,000 frequent flier miles at 500 MPH.\\n\\nOur passenger aged 59 millionths of a second more than his homebody wife. He has traveled that far into the future compared to folks who kept their feet on the ground. /n/r Well, if you didn\\'t think things could get worse, they are apparently about to get a whole lot worse.\\nYour rights of equal access to information and communication on the Internet hangs by a thread, and if this \\'gutting\\' of Net Neutrality goes through, what you see and access online will be able to be influenced and controlled by dark money. :/\\nPlease share /n/r NEW: Earth-like exoplanet found orbiting the star Ross 128:\\n\\nhttps://youtu.be/QkRgGRHA4ao?list=PL3RiFKfZj3pv1ZqpFxuZinoGtUGEOankw\\n\\nPlease share this post and comment below. /n/r From Kevin G. Rhoads... /n/r Ren? Descartes.... /n/r Sagan prescience, predicting tRumpism.  https://www.facebook.com/seth.jarvis.14/posts/10209313266742563 /n/r Using Gold Nanoparticles to Kill Cancer (American Physical Society ) http://www.physicscentral.com/explore/action/pnb-nanotherapy.cfm /n/r Aristarchus of Samos.... /n/r Looking with pretty ruth upon my pain. Pain pays the income of each precious thing. /n/r #MedicareForAll, or should we just continue cranking out Hundreds of BILLION$ more for bombs, drones, bullets and all that other \\'Indispensable for National Security\\' crap \\'our\\' government just loves using to simultaneously wage multiple wars we started all around the globe at the behest of the kleptocrats who are literally preying on us? /n/r From M Patricia McLaughlin... (Y) /n/r Thermal-Electric power, or how to turn what comes down from the sun, and when it hits a dark surface, it becomes heat. How to use this process to make electricity? /n/r Remembering Forrest J Ackerman\\n(November 24, 1916 \\x96 December 4, 2008)\\n\\nDuring his career as a literary agent, Ackerman represented such science fiction authors as Ray Bradbury, Isaac Asimov, A.E. Van Vogt, Curtis Siodmak, and L. Ron Hubbard. For over seven decades, he was one of science fiction\\'s staunchest spokesmen and promoters.\\n\\nAckerman was the editor and principal writer of the magazine \"Famous Monsters of Filmland\".\\n\\nAckerman was central to the formation, organization, and spread of science fiction fandom, and a key figure in the wider cultural perception of science fiction as a literary, art, and film genre. Famous for his word play and neologisms, he coined the genre nickname \"sci-fi\". /n/r Nerds keep it real on Black Friday /n/r right or wrong /n/r It turns you on   and it\\'s FREE! :O /n/r Hi everyone, I think most of us have experienced it, the question is this, when we open our eyes ( after standing under the sun with our eyes closed) for ex:standing in prayer, we see something greenish what causes it, and how it emerges and varnishes can someone explain. /n/r Bronze Aged Savages.... /n/r To Hard.... /n/r Trump kept sitting and talking.... /n/r BREAKING: Few minutes ago a huge #fireball was spotted in Southern Italy and in some zone of #Maghreb. #Meteor #astronomy\\n\\nhttps://twitter.com/meteorologo777/status/933398760570441734 /n/r Red sprites, lightning, zodiacal lights, milky way and orange airglow on La Palma, Canary Islands, Spain last night! Nov 21, 2017 What a combo and experience! #astrophotography #astronomy @StormHour @GTCtelescope @fotoastronomica @weathernetwork @StormchaserUKEU @WeatherNation @WetterOnline @earthskyscience\\n\\nhttps://twitter.com/ADphotography24/status/933264072094527489 /n/r Uber orders up to 24,000 Volvo XC90s for driverless fleet\\n\\nhttps://goo.gl/y7gTcM\\n\\nUber has entered into an agreement with carmaker Volvo to purchase 24,000 of its XC90 SUVs between 2019 and 2021 to form a fleet of autonomous vehicles, according to Bloomberg News. The XC90 is the base of Uber\\'s latest-generation self-driving test car, which features sensors and autonomous driving computing capability installed by Uber after purchase on the XC90 vehicle....\\n\\n#PintFeed #UberSelfDrivingTestCar #VolvoXC90SUVs #uber #Technology #News /n/r What about the notion that God is arbitrary? /n/r Tragically, there are Too Many AHoles out there who agree w/this creep... /n/r Post deleted due to Over-Zealous remarks by a member. Member warned against grandstanding. /n/r If I believed in the Bible.... /n/r Yeah, that\\'s what we Do.\\n\\n(At least you can see here that *I* know how to spell \"Yeah\".) /n/r WOW! Northern Lights seen last night, Nov 20, 2017 from Talkeetna, Alaska, USA. Photo credit: Dora Miller. #Aurora #NorthernLights #Alaska\\n\\nhttps://twitter.com/mark_tarello/status/932959683010334721 /n/r WE JUST SAW THE MOST INCREDIBLE NORTHERN LIGHTS IN ICELAND SOME MINUTES AGO!! Nov 21, 2017 @NorthLightAlert @AuroraAddicts @StormHour @ThePhotoHour @TamithaSkov @earthescope @KPAuroraAlert @severeweatherEU @dartanner #northernlight #Auroraborealis\\n\\nhttps://twitter.com/Muhammediceland/status/932773886093086720 /n/r Being Right is Often not Enough.... /n/r From Kimball Corson... :( /n/r Post and its poster on how to engage in software piracy deleted.\\n\\nTo avoid imperilling the group\\'s existence by breaking FB\\'s rules, instaban\\x99?. /n/r Primary Erythromelalgia (EM or PM) /n/r I\\'d read about how \"Safety Factors\" have no impact whatsoever in this \"governmental decision making\", *So HERE We GO!\":\\n\\n\"Keystone Pipeline Leak Won\\'t Affect Last Regulatory Hurdle\\n\\x97 A state official says discovery of a 210,000-gallons oil spill from the Keystone pipeline will not affect the decision of Nebraska regulators next week on a massive expansion of the system.\"\\n\\nhttps://www.usnews.com/news/business/articles/2017-11-17/keystone-pipeline-leak-days-before-nebraska-expansion-ruling /n/r November 1973 President Nixon\\ntold the nation on TV,\\n\"I am not a crook.\"\\nWhen will Trump do the same? /n/r It is no accident that among the younger generation there is rising interest in and support for socialist policies. A recent survey found that more young people in America would choose to live under socialism or communism than under capitalism.\\n\\n\\'The government attack on US college students\\'\\nWorld Socialist Web Site\\n\\nhttps://www.wsws.org/en/articles/2017/11/20/pers-n20.html /n/r Kimball Corson writes: Corporate Tax Cuts?\\nPresent corporate effective rates, before any cuts, are about the same as many effective rates in foreign nations. We are high in our published rate of 35% only. To get a sense of this, in the extreme, consider the following: /n/r Sounds like an interesting event!\\n...This November 30th!\\nhttps://www.facebook.com/events/1391128724342379/?notif_t=event_calendar_create&notif_id=1511211801782050 /n/r A person becomes what the environment he is surrounded by. /n/r Middle Section of a SAR Arc. Went right across the sky from West to East. Watched and photographed for 1 hour. Taken August 21/ 2017 10: 54 p.m Eastern Manitoba, Canada @subauroralarcs @AuroraMAX @BIAUS @TheWeatherNetUS @aurorawatch @StormHour @CBC @Photo_Space @_SpaceWeather_\\n\\nhttps://twitter.com/BrentMckean501/status/932419580743499777 /n/r From #RandyBresnik, NASA Astronaut /n/r Really I couldn\\'t get the idea about gravitational waves /n/r WOW, This is too funny !!!  I first saw this post tonight, but maybe you saw it after Estel Cooper posted it Thursday. Too bad this post isn\\'t an actual fact !!! /n/r Juvenile Detention Sentence.... /n/r Donald J. TrumpVerified account @realDonaldTrump\\nNov 17\\n\\nPut big game trophy decision on hold until such time as I review all conservation facts. Under study for years. Will update soon with Secretary Zinke. Thank you!\\n40,857 replies 21,750 retweets 113,968 likes /n/r How I got my kids to swim.... /n/r Never attribute to malice that which can be adequately explained by stupidity. /n/r On November 18th 1978 the Cult leader Jim Jones instructed 400 members of his church, \"People\\'s Temple\", to commit suicide in Guyana. So when is the next mass killing in the States ? /n/r SAR ARC: August 21st , 2017 10:51 P.M facing east. Taken in Eastern Manitoba, Canada. The Aurora was also strong low in the Horizon to the north. Air glow was very strong also at this time. @subauroralarcs @AuroraMAX @BIAUS @TheWeatherNetUS @aurorawatch @StormHour @CBC\\n\\nhttps://twitter.com/BrentMckean501/status/932284984806838272 /n/r November 19, 1863 famous\\nGettysburg Address was delivered by\\nPresident Lincoln.... /n/r No one understands the working of a false democracy more than America does at the moment.... /n/r November 19th, 1969 Apollo 12 lands on the Moon.... /n/r We have all been born with an addiction,\\nit\\'s called breathing. /n/r Tonic Immobility (TI).... /n/r He is the Biggest underachiever of all Time.... /n/r Hamlet Act 1, scene 3, 78\\x9682....Thanks Pat.... /n/r Do not invoke conspiracy as explanation when ignorance and incompetence will suffice, as conspiracy implies intelligence. /n/r Alexander the Great, became a keystone of Western Philosophy. Alexander became legendary as a classical hero in the mold of Achilles. /n/r Somehow I don\\'t think Mr. Spock or Mr. Data would approve of Facebook\\'s AI robot algorithms...\\n\\n(Now watch as I get banned for a month for making this comment. /n/r The more Trump lies, the more Frequent Flyer Points he gets, so he can use them for that Big Jet Aeroplane to fly around in.\\nIt\\'s called, \"Compulsive Spending\" /n/r No one mentions what it is like to be groped by a woman.... /n/r Teach them to earn things, not demand things.... /n/r A huge leap for trucking /n/r In Canada, we have a 50% female cabinet.... /n/r Dunning\\x96Kruger Effect.... /n/r Self aggrandizement is defined as exaggerating one\\'s own importance or people who like their own post. /n/r From Steve Cooperman... :)\\nhttps://www.facebook.com/grammarly/photos/a.158139670871698.33824.139729956046003/1783753021643680/?type=3&theater /n/r how many stupid deniers are left here? /n/r Turkey still does not admit to this.... /n/r Aurora borealis.. Oulu, Finland on Nov 7, 2017. Thanks to Thomas Kast\\n@ThomasKast1 #Northernlights #StormHour\\n\\nhttps://twitter.com/StormHour/status/931592444877131778 /n/r When will US politicians learn anything about economics? Yesterday, the House passed a \"Tax Reform\" bill that can be expected to reduce the US GDP and cost the US hundreds of billions of dollars because of cuts to preventive health care. /n/r Art and technology are never strangers.\\n\\nWhile I was at Symbolics, one of our customers was Thinking Machines, with their then pioneering massively parallel architecture, the Connection Machine.\\n\\nHow cool that it\\'s now in MoMA.\\n\\nMore below... /n/r When will US politicians learn anything about economics? Yesterday, the House passed a \"Tax Reform\" bill that can be expected to reduce the US GDP and cost the US hundreds of billions of dollars because of cuts to preventive health care. /n/r In the original Roman calendar.... /n/r To be lonely, one must first know what it is not to be. /n/r I am thinking about all of the recent exposures of attacks on women by their economically \"superior\" men and how that disparity protected this evil and silenced those women, and has done so historically for ALL working class people. \\nFranken being in the rich and powerful capitalist class automatically suppresses any \"middle or poor class\" (capitalist termed) working class people from enacting their human rights, as they, including this woman sexually abused by Franken, are also suppressed by their own mindset that this is simply the way it is. They are convinced there can only be \"rich, middle class, and poor\" and there can be nothing more but to suck up to the rich and hope this helps them to become one themselves to escape its evil.\\nShe said: \"I wanted to shout my story to the world with a megaphone to anyone who would listen, but even as angry as I was, I was worried about the potential backlash and damage going public might have on my career as a broadcaster.\"\\nTHAT is capitalism. \\nThe working class must come to class consciousness to understand they are the majority being repressed and victimized, and that the capitalist class can only perpetuate this evil by assimilating them to its cause. \\nThere is only one ethical solution and that is to join in a working class revolution to bring workers to absolute democratic power over the capitalist class if they ever expect to take any and all of its evil down. /n/r As my friends know I don\\'t engage in much *Trump Bashing*; it\\'s too damned Easy and besides, millions of my FB compadres do it vigorously on a daily basis.\\n\\x97 Ripping the GOP\\'s guts out? Yeah. I DO do that, especially when they do Absolutely INSANE BS like this... /n/r \"No, it\\'s not Pluto, but lurking in the dark outer reaches of our solar system, twenty times farther from the Sun than Neptune, is what NASA claims is a large ninth planet....NASA estimates that Planet Nine is about 10 times as massive as Earth, making it a rocky \"super-Earth\" (more on super-Earths later). It has a wide elliptical orbit that takes it as far as 100 billion miles from the Sun...A super-Earth is a rocky planet with slightly more or substantially more mass than Earth, though scientists believe that planets with much more than 1.6 times Earth\\'s radius can no longer be rocky....Super-Earths are common; most solar systems have them, and therefore it\\'s likely that our solar system is no exception. If Planet Nine really is out there, its secrets are about to be revealed.\"\\n\\nhttps://secondnexus.com/science/nasa-9th-planet-super-earth/2/ /n/r Is there \"Hope\" left?... /n/r What is Hyperspace?\\n\\nCan it be used for Travel? /n/r small wins in math... c/o my daughter /n/r WOW! Northern Lights seen Tuesday, Nov 14, 2017 night from Fairbanks, Alaska, USA. Photo credit: Sacha Layos. #Aurora #NorthernLights #Alaska\\n\\nhttps://twitter.com/mark_tarello/status/931179593616314368 /n/r I am looking to learn R language.Please guide me how to start as a Beginner i.e IDE and reference documents,videos,tutorials would be of great help.Currently I am working on Java with Eclipse IDE.\\nThanks in Advance /n/r Hi there\\nDoes someone here have experience working with stochastic partial differential equations (SPDE) for spatial modeling using integrated nested Laplace approximations (INLA) in R? I would love to hear from your experience.\\nThank you in advance ;) /n/r Help!! i need some subject, publication, book about using business intelligence/big data in banking(for exemple fraud detection..) tnks y /n/r Most #banking, #financial services, and #insurance (BFSI) organizations are striving to adopt a fully #data-driven approach to grow their #businesses and enhance customer services. Experts believe, like most other industries, #bigdata #analytics will be a game changer for companies in the financial sector. For more visit http://bit.ly/2Db1Rdz /n/r Urgent Hiring for the following job SEO / SOCIAL MEDIA SPECIALIST, let\\'s start your 2018 with a new Challenge ...\\nA company considered as a major player in its industry is looking for an SEO / Social Media Specialist\\n\\n\\x95 Knows how to make content move and go viral across Facebook, Twitter, and other social networks\\n\\x95 Understands the essentials for optimizing content for SEO\\n\\x95 Creative mindset, analytical and writing skills\\n\\x95 Updated on the latest digital marketing trends \\n\\nA competitive compensation package plus performance incentives and bonus and career growth await the successful candidate. Please email your resume at \\nhead. recruitment2017@gmail.com /n/r Hi, I\\'m searching a subject about combined internal audit with business intelligence /n/r I\\'m searching for case studies about Data Mining in B2B Business Company. Any idea? /n/r HAHAHA\\nBitcoin Vs Money by Google Trend!\\nBitcoin Burst Very Soon! :) /n/r Walk-in for Data Engineering Professionals\\n\\nhttps://www.facebook.com/events/100109240784704/ /n/r Walk-in for Business Analytics Professionals\\n\\nhttps://www.facebook.com/events/526501987728210/ /n/r Wikipedia is a rich source of well-organized textual data, and a vast collection of knowledge. What we will do here is build a corpus from the set of English Wikipedia articles, which is freely and conveniently available online.\\nhttps://www.kdnuggets.com/2017/11/building-wikipedia-text-corpus-nlp.html /n/r MOL2NET: FROM #MOLECULES TO #NETWORKS (>10 000 followers)!!! Official group link: https://www.facebook.com/groups/chembioinfo.networks/ Topics: #Science (All Areas), #Chemistry, #Physics, #Statistics, #Medicine, #Computational science, #Nanotechnology, #Bioinformatics, #Education, etc. CALL FOR PAPERS: MOL2NET, International Conference Series (Free of cost), #MDPISciforum, #Switzerland, http://sciforum.net/conference/mol2net-03, Submission until 2017-Nov-30. Workshops: #USA, #Spain, #Italy, #Brazil, etc. Selected Papers for Journal #Nanomaterials, IF = 3.55, #Experimental and #Computational #Nanosciences, http://bit.do/mol2net-nanomat-issue. Chairperson: Prof. Humbert Gonzalez-Diaz, ORCID: https://orcid.org/0000-0002-9392-2797 /n/r Any free course for Hadoop or Spark for data engineering? /n/r Chaque Jour est une occasion de merci au Cr?ateur car notre vie est entre ses mains. Sur cette terre sois tu choisis d\\'?tre mauvais soit tu choisis d\\'?tre une bonne personne, c\\'est le libre arbitre.\\n\\nla m?chancet? sur cette terre s\\'est accrue, de plus en plus de guerres, l\\'esclavage de retour, l\\'adoration du dieu argent, sorcellerie, nous montre que beaucoup ont choisis le mal et ils sont nombreux ? le faire.\\n\\nle mal est devenue tellement recurent que c\\'est devenue la norme. or le salaire du p?ch? c\\'est la mort mais le don gratuit de Dieu c\\'est la vie ?ternel.\\n\\nchoisissons de faire le bien, choisissons de d?noncer le mal, de d?noncer l\\'esclavagisme non pas seulement en libye mais en arabie saoudite, au liban et dans tous ces pays arabes o? plusieurs sont hypocrites ils rev?ts une apparence de pi?t? pourtant ils sont plein de m?chancet? et de p?ch?.\\n\\nil crie Allah 6 fois par jour aux yeux de tous mais viol, vole et tue en cachette. \"or le diable ne vient que pour voler, d?rober et ?gorger\" A bon entendeur\\n\\nChoisissons le bien, aimons notre prochain, prenons soin de ceux qui nous sont proches...nos ?pouses et ?poux, nos familles notre semblable car tu aimeras ton prochain comme toi m?me et tu ha?ras le mal /n/r #Hadoop Video #Tutorial Complete Playlist.\\n\\nKnow More..\\nhttp://cutt.us/AYpvM /n/r Hello all\\n\\nPlease can you help me \\n\\nHow can I get Dataset about job searching in Egypt?\\n\\nThanks /n/r Explaining simple Ensemble learning for time series forecasting in R ->\\nhttps://petolau.github.io/Ensemble-of-trees-for-forecasting-time-series/ /n/r Networking and networking models at\\n\\nhttps://m.facebook.com/groups/540482969435657 /n/r Data science of network modelling at\\n\\nhttps://m.facebook.com/groups/540482969435657 /n/r How does the amount of tax collected differ around the world? What are the global trends in tax collection? How reliant are developing countries on natural resource revenues?\\n\\nThe recently updated Government Revenue Dataset has the data needed to answer all these questions and many more.\\nhttp://bit.ly/2yYkIXN /n/r Curso de Introducci?n a Big Data con MongoDB y Hadoop\\n\\nYa son cientos los alumnos interesados que han aprendido a utilizar las herramientas de Big Data para la gesti?n de informaci?n para sus productos o servicios. Os invitamos a conocer de primera mano una de las tecnolog?as mas importantes de nuestro presente. Muchas gracias.\\n\\nhttps://culture-lab.es/curso/curso-online-introduccion-a-big-data-con-mongodb-y-hadoop/ /n/r Data Science and Analytics related discussion, trends, activities, opportunities and information can be found on following page... https://www.facebook.com/groups/1501661686712546/ /n/r Network models at\\n\\nhttps://www.facebook.com/EurostatStatistics/photos/a.1846529728965286.1073741828.1846196185665307/1972533073031617/?type=3 /n/r Hello everyone. Does anyone know a library or database which contains semantic clusters of words? The use-case is movie and video analytics. Primary for EN, in the future also for PL. The output should be like that w1,w2,w3 are associated with anger, w4,w5,w6 are associated with action and so on. The best output be some memberships function mf(w,c) which returns value in the [0,1] interval based on how much is the word w close to cluster c. /n/r FinTechs are looking for big data analysis applications:\\n\\nhttps://m.facebook.com/groups/540482969435657 /n/r Artificial Intelligence, Blockchain applications, Machine learning: join at\\n\\nhttps://www.facebook.com/groups/bigmefi/ /n/r Anybody has some good coding music to recommend? (By \"coding music\" I mean the kind with no lyrics that mess with your thinking.)\\n\\nMost of the time I just listen to Chopin, Hans Zimmer, also \"Social network\" soundtrack. But after a while I just get bored when repeated too many times.\\n\\n?? /n/r INTERESTED IN BLOCKCHAIN APPLICATIONS?\\nJOIN:\\nhttps://www.facebook.com/groups/bigmefi/ /n/r Looking for a Quant or Datascientist cofounder, future CSO/CTO of a fintech startup based in Paris and offering a P2P credit insurance service for BtoBtoC or BtoC markets.\\nSkills needed: team leader/builder, applied math (probabilities - monte carlo method -, quantitive finance, stats, graph analysis), algorithms, machine learning, api and web/mobile clients dev.\\nMore details by MP. /n/r Machine learning and Artificial Intelligence - how it is leveraging the growing body of digital data to help solve humanity\\'s greatest challenges. https://ibm.co/YPC-MN #IBMML #IBMDSX /n/r Talking with a QA expert and designer of robust testing frameworks as we discover the importance of pair-wise testing.  https://ibm.co/YPC-PG1 /n/r Spent some quality time with the CIO of Wumart in China and presented my book to him /n/r About GST enabled Accounting Software to contact us info@mavericksindia.com or call us on 020-27272626 /+91-8983344954 /n/r Here\\'s a free code to the Big Data Analytics World Championships    TEXATA100. Round 1 starts 30th September 2017 online. Register at: http://www.texata.com /n/r Blockchain and P2P economics discussion on at:\\nhttps://www.facebook.com/groups/540482969435657/admin_activities/ /n/r \"The short sighted data scientist might see this as a threat, giving lesser-trained colleagues an opportunity to compete with or outshine them \\x96 but I don\\'t believe that\\'s the case. Instead, I see believe this trend will serve only to empower data scientists within their organization. The ability to get buy-in from leadership and staff has always been one of the biggest challenges for the profession, and giving more colleagues a chance to understand the unbridled power of data will only make that obstacle easier to overcome.\" /n/r Big data and data science in economics and finance\\nhttps://www.facebook.com/groups/bigmefi/ /n/r I have a question which would be happy to know your idea about.\\nIn your opinion, can high perplexity of natural languages all be sought in brain structure using current imaging tools or knowledge gathered by neuroscientists, or much of that cannot be seen as that is caused by consciousness or mind or etc?\\n\\nis high perplexity of natural languages due to complexity of brain structure ; or instead, because of complexity of consciousness, mind or etc ? /n/r would you regard transfer learning as special category of analogy? /n/r I am new to data science, what is the most used analysis tool?   And what does it do better, than the others?\\n\\nPython, R, Rubi, Or the Old School Excel?   Feel free to mention any that are missing and you prefer over the mentioned options. /n/r If you looking for a #WebDevelopment Company..!!\\nThen Contact us We have Skilled web programmers to deliver professional #WebApplication development solutions and much more #WebServices.\\nFor more details please visit our website:- http://www.mavericksindia.com/ /n/r Anyone from/in Dallas? /n/r Pra quem estiver em SP /n/r Hello. I\\'m new here.\\n\\nInterested to learn about Big Data from the very Basic. Can anyone suggest me the best books to start with?\\n\\nThanks for your cooperation. /n/r Hello Experts,\\n\\nPlease advise [Gaming Industry]:\\nI have some data in multiple tables with hundreds of columns.\\nI have created an SQL statement that takes what I think I will need to predict some columns (number of deposits, amount of deposits, life time value etc).\\nUsing Python I have imported the data, cleaned and scaled it.\\nNow after I have run PCA on my data frame I see (using bar plot) that I have around 7 principal components.\\n\\nHow can I derive from these 7 principal components which of the columns of my data frame are actually most informative?\\n\\nThe goal is to take these columns as input to multilinear regression to predict other target columns.\\n\\nPlease help me with this/suggest anything else that can help me.\\nThanks in advance!\\nSteve /n/r Data Science and Analytics related discussion, trends, activities, opportunities and information can be found on following page... https://www.facebook.com/groups/1501661686712546/ /n/r Hire & get hired as a remote data scientist! Enlarge your talent pool & job opportunities to the whole planet! New FB group for AI job offers!! https://www.facebook.com/groups/1054526381348851/ /n/r ????????, ? ??????????????, ??????????, ??????????-????????????, ??????? ????? ????? ?:\\n- ??????? ? ?? ????????\\n- ?????? ? ???????????\\n- ?????????????? ????? ???????????\\n- ??????????? ??????? ???????\\n- ?????? ?????????? ??????????\\n- ?????? ??????????? ? ??????????\\n\\n?????? ????????, ???? ??????? ??? ????? ? Python ? ???????? ????????.\\n\\n?????? ????? ??????, ?????? ????????? ??????? ? ?????? ???????????? ?????????????? ? ???????? ?????????? ??? ???? ????. /n/r Herding Unicorns: how to manage a data team British Museum profusion #datascience #data #business #management /n/r The deadline for the CIKM workshop \"Data & Algorithm Bias\" is getting closer! Submit your work before July 2nd! More information in http://dab.udd.cl/2017 /n/r Launch of the Data Science section of the Royal Statistical Society #DSSLaunch RSS Data Science profusion #datascience #statistics #data #science /n/r Dear All,\\nI am looking for any studying material for the canonical correlation analysis. If anyone know a good material please contact me. /n/r Making Money Out of Data ranked #5 on the Kindle eBook: #5 BestSeller in the Econometrics Category.\\n\\nLearn from my experience at large Corporates on how they Make Millions of Dollars out of Data through #analytics.\\n- Read 5 Real Business Success Stories from the industry and a step-by-step guide to an innovative approach to creating incremental value through advanced analytics techniques. /n/r Want to know about data science in economics and finance? Join in at:\\n\\nhttps://www.facebook.com/groups/bigmefi/ /n/r Making Money Out of Data ranked #5 on the Kindle eBook: #5 BestSeller in the Econometrics Category.\\n\\nLearn from my experience at large Corporates on how they Make Millions of Dollars out of Data through #analytics.\\n- Read 5 Real Business Success Stories from the industry and a step-by-step guide to an innovative approach to creating incremental value through advanced analytics techniques. /n/r Hi All - I\\'m looking for someone with expertise in machine learning and NLP.  The initial conversation would be around a contractual role and can turn into a equity holding/ full time role. /n/r Hello Experts,\\nI am working on some challenge:\\nMy data is taken from Games companies (online games/casinos/lotto etc.)\\nI have a lot of data - transactions data, user data, tons of columns.\\n\\nPlease help me to understand which dashboards/charts/analysis should I produce?\\nAlso I need machine learning algorithms to be applied to predict user behavior and actions.\\n\\nPlease advise,\\nSteve /n/r What is your vision and thought on data visualisation? /n/r Making Money Out of Data: The book contains five business usecases from 5 industries and explains standardised analytics process to successfully deliver analytics / data science project to create value for an organisation. #Money #Analytics #BigData #AI #DataScience #ArtificalIntelligence /n/r Making Money Out of Data: The book contains five business usecases from 5 industries and explains standardised analytics process to successfully deliver analytics / data science project to create value for an organisation. #Money #Analytics #BigData #AI #DataScience #ArtificalIntelligence /n/r Keep your world clean and green.\\nSave trees, Save the environment!!\\nClean city, Green city!!\\n#Happy_world_environment_day /n/r Hi All. I am a programmer and my plan is to create an educational system that will predict student outcome based on the student response time during recitation. My problem is, I don\\'t have knowledge in different classifiers. But here\\'s my different attributes to consider:\\nGender M/F\\nUnitsEnrolledPercentage 50%, 13%, etc\\nTravelHours 1.5, 2, etc\\nResponseTime 43%, 80%, etc\\nResponseResult Correct/Wrong\\nSleepHours 8, 7, 5, etc\\n\\nPredicted outcome: passed, failed\\n\\nIs ID3 capable of handling these data to make predictions?\\n\\nThank you in advance. /n/r can we generalize no free lunch theorem for machine learning algorithms? in a sense that there is no best ML method context independently? /n/r I am co-organizing the CIKM 2017 workshop on \"Data & Algorithm Bias\" in Singapore, together with Ciro Cattuto, Leo Ferres, Ricardo Baeza-Yates, Daniela Paolotti and Jeanna Matthews. The deadline is July 2nd. More information at: dab.udd.cl. /n/r Is ID3 optimal or suboptimal in sense of feature-by-feature maximization of mutual-info wrt to labels over all kinds of decision trees?\\n(to make myself clear, maybe we can use dynamic programing search methods like beam-search to choose order of feature selections in ID3 to improve final mutual info score. do you confirm?) /n/r http://www.kdnuggets.com/2017/05/intro-mxnet-python-api.html\\nThis post outlines an entire 6-part tutorial series on the MXNet deep learning library and its Python API. In-depth and descriptive, this is a great guide for anyone looking to start leveraging this powerful neural network library. /n/r Data Science: an FB group focused on machine learning in economics and finance\\n\\nhttps://www.facebook.com/groups/bigmefi/ /n/r I am pleased to announce Data Science Phd Positions and Scholarships in Applied Economics and Management (AEM), at the University of Bergamo and Pavia.\\n***The deadline to apply is the 15th of june, 2017***\\nApplications on-line at:\\nhttps://www.unibg.it/\\x85/bando-di-concorso-lammissione-ai-cor\\x85\\n(Read the ENG files only) /n/r http://datastructures.conferenceseries.com/\\n\\nData Structures-2017 conference is here to share the knowlege about Ransomware intrested speakers can participate in this prestegious conference and share their views about Ransomeware.\\n\\nsubmit your abstracts at:http://datastructures.conferenceseries.com/abstract-submission.php /n/r Identifying type of sports from many sport events. What will be the approach? /n/r Data Science and Analytics related discussion, trends, activities, opportunities and information can be found on following page... https://www.facebook.com/groups/1501661686712546/ /n/r Guys any oNe any idea..!! about the Error in ANN\\'s /n/r Hey Everyone !!\\n\\nIs there a machine learning approach to extract key data from pdfs?\\nwith key data I mean - let\\'s say price (if it contains)(price of anything - airway/railway/bus/hotel tickets...etc, phone bill/electricity bill/house bill)?\\nI could only think of regex..nothing else...Please suggest any better approach?\\n\\nThanks /n/r Data Science and  Analytics related discussion, trends, activities, opportunities and information can be found on following page... https://www.facebook.com/groups/1501661686712546/ /n/r Is there any PDF or E-book which is helpful for beginners to learn Statistical concepts such as Regression , Clustering etc. /n/r MOL2NET: FROM MOLECULES TO NETWORKS!!!\\nPublic group (>10 000 members): https://www.facebook.com/groups/chembioinfo.networks/ Tags: #Science (#Multidisciplinary), #Chemistry, #Computational Science, #Data #Analysis, #Bioinformatics, #Networks, #Nanotechnology, #Biotechnology, etc. The group is also the host of  #MOL2NET International Conference Series on Multidisciplinary Sciences, #MDPI, #Sciforum, #Switzerland, UPV/EHU #Basque Country, Spain. Page: http://sciforum.net/conference/mol2net-03, RGate: http://bit.do/rg_mol2net, Assoc Workshops: #Miami, #USA, #Bilbao, #Spain, #Soochow, #China, etc. Online Submission: You have to login or sign in at http://sciforum.net/login to submit your short communication (1-2 pages recommended), proceeding abstract, slide presentation, or video conference. Submissions to general sections and workshops will remain open from now on until 30 November 2017. /n/r Comenzamos despu?s de Semana Santa un nuevo #curso de #BigData con MongoDB y Hadoop presencial en #Madrid y #Online para particulares y empresas el pr?ximo lunes 17 de abril ;) \\n\\nhttps://culture-lab.es/curso/curso-de-introduccion-a-big-data-con-mongodb-y-hadoop/ /n/r Hi anyone is having code for regression or classification for students placement analysis? /n/r Hello, everyone, I need this book\"CompTIA Linux+ / LPIC-1 Cert Guide (Exams LX0-103 & LX0-104/101-400 & 102-400),Thank you /n/r Analytics related latest trends and information can be found on following page.. https://www.facebook.com/groups/1501661686712546/ /n/r Do you think the rise of sentient machines is held back more by the hardware requirements or the software/knowledge engineering necessary to simulate something like consciousness? /n/r Hone your skills with #Hadoop #onlinetraining at Glory It Technologies Pvt. Ltd. Hadoop is an open-source framework for distributed storage and processing of the #bigdata in a distributed computing environment across clusters of computers by using simple programming models.\\n\\nHadoop services are used for data storage, data access, data processing, data governance, security, and operations. And it is widely used across industries like finance, government, media & entertainment, information services, health-care, and much more.\\n\\nTo know more, visit http://www.gloryittechnologies.com/Bigdata-Online-Training.html /n/r Tech Trend 2017 is a unique event showcasing the emerging technologies, providing participants with an opportunity to gain access to the most innovative, and leading companies, professionals, entrepreneurs, and academics in the world. Tech Trend 2017 features some of the leading companies, and CEOs in the world that are consistently pushing the boundaries of innovation. Attendees will also gain first-hand insights of the future and understand the technologies disrupting businesses and driving the new economy.\\n\\nTech Trend 2017 will cover a wider-range of topics like Internet of Things, Machine Learning, Cyber Security, Artificial Intelligence, Big Data etc. We will be expecting at least 300 attendees for the event made up of leading professionals, executives, decision makers, academics, and entrepreneurs from around the region.\\n\\nVisit https://lnkd.in/fc-yG99 to complete your registration\\n\\n#TechTrend #CASUGOL #BigData #HRAnalytics #IoT #SmartCity #MachineLearning #Conference /n/r Tech Trend 2017 is a unique event showcasing the emerging technologies, providing participants with an opportunity to gain access to the most innovative, and leading companies, professionals, entrepreneurs, and academics in the world. Tech Trend 2017 features some of the leading companies, and CEOs in the world that are consistently pushing the boundaries of innovation. Attendees will also gain first-hand insights of the future and understand the technologies disrupting businesses and driving the new economy.\\n\\nTech Trend 2017 will cover a wider-range of topics like Internet of Things, Machine Learning, Cyber Security, Artificial Intelligence, Big Data etc. We will be expecting at least 300 attendees for the event made up of leading professionals, executives, decision makers, academics, and entrepreneurs from around the region.\\n\\nVisit https://lnkd.in/fc-yG99 to complete your registration\\n\\n#TechTrend #CASUGOL #BigData #HRAnalytics #IoT #SmartCity #MachineLearning #Conference /n/r Hello Experts,\\nI have a question for you. Consider you have a business of arbitrage - If I order some product from company A, and can cancel it (free until date T), the price fluctuates and I can reorder on a lower price.\\nHow to find the optimal algorithm for the optimal price of the item/order where I rebook and save money for my customer (and get a fee for the service). /n/r any idea for how to apply Business Intelligence for logistics?? /n/r HI Everyone,\\nHow can we implement pl/sql cursor functionality in Hive/Pig/Spark. If anybody worked on same please provide your inputs. Thanks In Advance. /n/r How the Logistic Regression Model Works in Machine Learning \\n\\nhttp://bit.ly/2oslRVi  \\n\\n#MachineLearning #DataScience #Python /n/r Dear all, \\nI have a question regarding the Association analysis in R. Does anyone have examples of the association analysis for the store or ecommerce, where the input data are stored in lines for each product, so one transaction might have more then one  line with product ID or product supplier. The number of products in more than 10000.\\nThanks in advance. /n/r is here any one who work with scala and spark ? /n/r ML algorithms CheatSheet :) /n/r Understanding, generalisation, and transfer learning in deep neural networks http://bit.ly/2nCFmas\\n\\n#DeepLearning #NeuralNetworks #DataScience #MachineLearning /n/r Is it possible to combine on-line analytical processing (OLAP) with data mining /n/r special type of programming language used to provides instructions to the monitor is \\na) FPL. b) SML. c) DML. d) jcl\\nwhat is the correct answer? /n/r Develop key #presentation skills and learn how to present your ideas with #TLSU.\\nJoin us for High Impact Presentation Skills Program in Banglore.\\nFor more information, mail us at training@teamleaseuniversity.ac.in /n/r Develop effective #presentation skills with High Imapct Presentation Skills Program in Banglore at #TLSU.\\nFor more information, mail us at training@teamleaseuniversity.ac.in /n/r Our experienced trainers facilitate excellent Soft Skills #Training for Graduates & #Professionals. \\nEnrol Now!\\nFor more information, visit us at form.teamleasetraining.com /n/r Opportunities & risks of open & syndicated data #GartnerDA #opendata #syndicateddata #data #datascience #bigdata #infopreneurship #business /n/r Ducati & Accenture optimize racing performance using data science #GartnerDA #IoT #analytics #datascience #machinelearning #bigdata #tech /n/r Hadoop & Spark: Opportunities and Risks #GartnerDA #datascience #spark #hadoop #datamanagement #bigdata #machinelearning #tech #business /n/r ATSS-IICMR, NIGDI, PUNE !!!\\n(Approved by AICTE & Affiliated to Savitribai Phule Pune University,\\nAccredited by NAAC, Recognized by Gov. of Maharashtra).\\n\\n******Upcoming Events******\\n\\n******Industry Visit To *******\\n\\n******Mazak India*******\\n\\non 23rd March, 2017 /n/r A digital society is emerging #GartnerDA #datascience #digitalsociety #society #tech #culture #sociology #analytics #bigdata #digital #data /n/r Hello Everyone,  \\n\\nI really need your opinion about HR Analytics. You all have speciality of your own and i request you to share your opinions on the noted below points.\\n\\n1. How we can use NPS Score in the field of HR Analytics?\\n\\n2. Most of the Companies used bell curves and its significance for appraisal process in the past but now companies are rejecting it. Why?\\n\\nIt is my request, please share your views about the above mentioned points.\\n\\nRegards,\\nAtul Kumar /n/r Driving profitability with further analytics #GartnerDA #datascience #analytics #bigdata #IoT #businessintelligence #business #tech /n/r Using data lakes from pointless to profitable #GartnerDA #datascience #datalake #bigdata #analytics #businessintelligence #business #tech /n/r Hi,\\n\\nWho can help me with some game analysis task? /n/r Did you know!\\n#android #fact /n/r Top 36 #Tableau #Interview Questions and Answers For 2017 https://goo.gl/ahbyRR /n/r Visual Analytics with SSRS & SSAS on iOS Android Windows 10 \\n? https://goo.gl/jWA29w\\n==========\\nVisual Analytics with SSAS and SSRS on iOS Android and Windows 10 is a  course in which a student having no experience in analytics, reporting and visualizations would be trained step by step from basics. The intention of this course is to empower students with the knowledge of developing mobile dashboards for senior leadership of an organization like CEO, COO etc.\\nThese skills can potentially yield salaries of $85 \\x96 $150k based only on your experience of data visualizations and mobile reporting techniques using Microsoft BI Tools using the latest version of SQL Server Analysis Services and SQL Server Reporting Services 2016.\\nCourse includes practical hands-on exercises as well as theoretical coverage of key concepts. Anyone pursuing this course would be able to clearly understand about Microsoft Business Intelligence Architecture, Microsoft Mobile Reporting Architecture and understand how SSAS fits in this architecture along with other tools like Power BI. /n/r Any one knows about a big data analytics (data lakes, hadoop, spark ...) architecture case study in banking? Or any reference ?\\nThanks /n/r is there anyone working on Big data testing.. /n/r Colors for Data Science A-Z: Data Visualization Color Theory \\n================\\nA fun and entertaining journey thorough colour theory and basic colour knowledge to help you create effective Data Science visualisations.\\nSo why is this an important course for a Data Scientist?\\nThink about this\\x85\\nYou\\'ve just completed an incredible Analytics project.\\nYou did the data prep, the modeling, and now you have the insights.\\nBut we all know that this is not the end\\x85\\nYou still need to present your findings to your manager, client or even a large audience....\\n? https://goo.gl/xaEmN3 /n/r I have a simpler job this time.  I want to create a simple HTML page where people can login.  Once they login there will be two tasks: 1) I want to have an iframe through which people can do certain tasks. 2) I want the iframe to be able to get the user\\'s login ID so we can track what tasks the person did.   Ping me for more information. /n/r The Ultimate Hands-On Hadoop - Tame your Big Data! \\n       -\\nThis course is comprehensive, covering over 25 different technologies in over 14 hours of video lectures. It\\'s filled with hands-on activities and exercises, so you get some real experience in using Hadoop \\x96 it\\'s not just theory.\\nYou\\'ll find a range of activities in this course for people at every level. If you\\'re a project manager who just wants to learn the buzzwords, there are web UI\\'s for many of the activities in the course that require no programming knowledge. If you\\'re comfortable with command lines, we\\'ll show you how to work with them too. And if you\\'re a programmer, I\\'ll challenge you with writing real scripts on a Hadoop system using Scala, Pig Latin, and Python....\\n? https://goo.gl/460Tsz /n/r Hello Scraping Experts!  I need help pulling data by doing a google search on a name + city and pulling information that shows up on page search.  Customer willing to pay handsomely.  Please add me as friend or message me to discuss more. /n/r when researcher have 1 IV and 2 DVs which test should be applied to interpret data? /n/r Apache Spark 2.0 with Scala - Hands On with Big Data! \\n      -\\nNew! Updated for Spark 2.0.0.\\n\"Big data\" analysis is a hot and highly valuable skill \\x96 and this course will teach you the hottest technology in big data: Apache Spark. Employers including Amazon, EBay, NASA JPL, and Yahoo all use Spark to quickly extract meaning from massive data sets across a fault-tolerant Hadoop cluster. You\\'ll learn those same techniques, using your own Windows system right at home. It\\'s easier than you might think, and you\\'ll be learning from an ex-engineer and senior manager from Amazon and IMDb.\\nSpark works best when using the Scala programming language, and this course includes a crash-course in Scala to get you up to speed quickly. For those more familiar with Python however, a Python version of this class is also available: \"Taming Big Data with Apache Spark and Python \\x96 Hands On\".\\n? https://goo.gl/4ccWk8 /n/r This online course was specifically designed to help you understand Complex Architectures of Hadoop and its components, guide you in the right direction to start with, and quickly start working with Hadoop and its components.\\n\\nTraditional Big Data courses can cost $300 or more, but since Udemy courses are online and on-demand, you\\'ll get the same world-class instruction for only $10! Plus, you\\'ll get lifetime access to your content, can learn at your own pace on any device, and you\\'re protected by a 100% money-back guarantee.\\n\\nJoin today to avail the offer..!!\\n\\nhttps://www.udemy.com/big-data-and-hadoop-for-beginners/?couponCode=BIGDATA10 /n/r Data Science, Apache Spark & Python: Analysiere echte Daten! \\n        -\\nAuswertungen von \"Big Data\" werden immer wichtiger, Experten werden h?nderingend gesucht. Du lernst in diesem Kurs die hei?este Technologie, Apache Spark kennen. Dieses wird bereits von unz?hligen Unternehmen verwendet, darunter Amazon, eBay, Groupon, TripAdvisor! Lerne jetzt Apache Spark \\x96 ganz bequem auf deinem eigenen Computer...........\\n? https://goo.gl/R8EJZW /n/r Machine Learning for Data Science \\n      -\\nThank you all for the huge response to this emerging course!  We are delighted to have over 2300 students in over 102 different countries and for the overwhelmingly positive and thoughtful reviews.  It\\'s such a privilege to share this important topic with everyday people in a clear and understandable way.Unlock the secrets of understanding Machine Learning for Data Science! In this introductory course, the \"Backyard Data Scientist\" will guide you through wilderness of Machine Learning for Data Science.  Accessible to everyone, this introductory course not only explains Machine Learning, but where it fits in the \"techno sphere around us\", why it\\'s important now, and how it will dramatically change our world today and for days to come.\\n? https://goo.gl/69j6wf /n/r Hello everyone - I have a company looking for help with pulling data from company websites.  They have a list of 8000 websites and want to pull the name of the company, the address, phone number, products and prices.  Totally understood you cannot pull all this information at once  > please let me know if you can help.  Add me or message me to chat. /n/r What is the best regressor for predicting logarithmic relationship between dependent and independent? Python packages please.... /n/r I have got a problem. I have terms like BLUE, RED, GREEN, BENZ, AUDI, BMW, ROLEX, TIMEX, TITAN . Now how to cluster these terms in such a way that colors should become a cluster, Car companies should become a cluster and Watch brands should become one. Can anyone help me doing this? I have to implement it in my project. I am using Java. /n/r Find the MongoDB Certification Preparation Guide makes the process simpler and easy to prepare for a certification exam. https://goo.gl/ohjUJD /n/r ?? Learn Big Data: The Hadoop Ecosystem Masterclass\\n>> bit.ly/2m3Yqgx\\nThis Big Data Hadoop - The Complete Course covers the topics from the basic level of beginner till the advanced professional levels required for Big Data Hadoop Certification. \\nIt\\'s a must have course for prospective Big Data experts. \\nThe course covers #Hadoop, #HDFS, #Map_Reduce, #YARN, #Apache_Hive, #PIG, #Impala, #Scoop and #ZooKeeper /n/r Does anyone know how to access edx BerkeleyX\\'s Stat2 course as its showing Enrollment is closed. /n/r Hey guys,\\n\\nNo post de hoje abordamos o uso da declara??o SELECT no MySQL e suas v?rias formas de implementa??o.\\nhttp://programetododia.com.br/#!/mysql-select\\nEnt?o corre l?, d? uma olhada, e aprenda um pouco mais sobre esse banco de dados sensacional ! \\nAproveite e curta a nossa p?gina no face ! ( se ainda n?o tiver curtido )\\n\\nSo, see you guys soon e Programe Todo Dia ! /n/r Apache Spark 2.0 with Scala - Hands On with #Big #Data!\\n\\n \"Big data\" analysis is a hot and highly valuable skill \\x96 and this course will teach you the hottest technology in big data: Apache Spark. Employers including Amazon, EBay, NASA JPL, and Yahoo all use Spark to quickly extract meaning from massive data sets across a fault-tolerant Hadoop cluster. You\\'ll learn those same techniques, using your own Windows system right at home....\\n? https://goo.gl/4ccWk8 /n/r Hey guys anyone worked on python and R with Visual studio need some help /n/r Hey guys,\\n\\nHoje no Programe Todo Dia, te daremos alguns bons motivos para voc? aprender / usar MySQL na sua aplica??o.\\nEnt?o ? s? clicar e dar uma conferida !\\nhttp://programetododia.com.br/#!/why-mysql\\nAproveite e curta a nossa p?gina no face ! ( se ainda n?o tiver curtido )\\n\\nSo, see you guys soon e Programe Todo Dia ! /n/r IIHT Viman nagar is organizing walk-in drive for its client Cognizant Technologies. Please find the details below:\\nWalk-in Date: 3rd Week of March (Date to be announced)\\nEligibility: 2016 pass-out Engg - Electronics / Computers / IT\\nVenue: Pune (Location to be announced)\\nRegister now at https://goo.gl/3B6pzQ /n/r IIHT Viman nagar is organizing walk-in drive for its client Cognizant Technologies. Please find the details below:\\nWalk-in Date: 3rd Week of March (Date to be announced)\\nEligibility: 2016 pass-out Engg - Electronics / Computers / IT\\nVenue: Pune (Location to be announced)\\nRegister now at https://goo.gl/3B6pzQ /n/r Big Data Online Course: Learn Hadoop, HDFS, MapReduce, Hive & Pig. Enroll today at just $0.99.\\n#PremiumMembership\\nCourse Link: http://skl.sh/2lnjHS1 /n/r Hi All - I have about 200 company links for LinkedIn and trying to get the description off of those pages...any idea if there\\'s a fast way to do this i.e. scrape? I have the links already. /n/r In todays (24th Feb.,2017) Sakal news paper, article on MCA course by Dr Deepali Sawai, Director IICMR-MCA , Nigdi, Pune\\n\\nFor IICMR, MCA DTE Code (MC-6154) 100% Placement Assistance, International Certification Provided...\\n\\nFor more details call : 9921000870 / 9822951262\\ndo visit - www.iicmr.org /n/r Hello Everyone,\\nI\\'m looking for an online course for NLP, I remember one of the folks here share a link for a very good course for online program from India , but I couldn\\'t remember what was the link, any suggestions/ideas? /n/r Hello Everyone - I\\'m looking for some customer service calls to use for some text analysis - looking for calls with USA customers...anyone have access to any? Happy to pay for the right data. /n/r Can anyone help me in SEM analysis /n/r With #Mavericks, you get more a desktop application. You get a solution that\\'s unrivaled in the industry. Top quality, Integrated, Customized.\\nA perfect fit for you and your business. /n/r Hello experts,\\nCan you please explain to me what is 2D histogram?\\nI use python for this and want to understand what are the values the bins represent. /n/r Greetings from ATSS-IICMR, NIGDI, PUNE !!!\\n\\nWe Make You Future Ready.... Join MCA, a guaranteed launching pad for innumerable career options.\\n\\nFor MCA DTE Code (MC-6154) 100% Placement Assistance, International Certification Provided...\\n\\nFor more details call : 9921000870 / 9822951262\\nwww.iicmr.org /n/r There are multiple openings for engineering roles at all levels at NeoGrowth Credit Pvt. Ltd. \\nInterested candidates can share their resume at aswanimanish92@gmail.com for referral.\\nWork Location - Bangalore\\nSalary :- Best in the market\\nColleges : IIT/NIT/IIIT and other Tier 1 colleges\\nOpen position:- \\nSoftware Development Engineer I/II/III \\x96minimum 1.5 year \\nPlease share your resume only if you are interested in the above profile and have the required work ex.\\nNote: While replying change the subject based on the role and work ex, and preferably provide CV in pdf format. Feel free to forward to any other relative/friend of yours whose profile would match\\nThe turnaround would be quick for best fit candidates. /n/r Greetings from ATSS-IICMR, NIGDI, PUNE !!!\\n\\nIBM\\'s Academic Initiative Event(Watson Internet of Things) for B.Sc., BCS, BCA (All Stream) 2017 passout is expected in the 4th week of Feb. 2017 @ ATSS-IICMR, Nigdi, Pune.\\n\\nInterested & Eligible students kindly register your name @ below link as earlier...\\n\\nREGISTRATION LINK:\\nhttps://goo.gl/forms/oVQRj7sEOtgu1W002\\n\\nWill send confirmation mail to the registered students only.\\n\\nWarm wishes on behalf of ATSS-IICMR, NIGDI, PUNE!!!\\n\\n  \\n\\nBest Regards,\\n\\nMahesh  Deshmukh\\nTraining & Placement Officer,\\nATSS-IICMR,  Nigdi Pradhikaran, Pune 411 011\\nLandline : +91-020-2765 7648 | Cell No. : +91-9923234570\\nMail ID: deshmukh.mahesh11@gmail.com  I  mahesh.deshmukh@iicmr.org\\nLinkedIn : https://www.linkedin.com/pub/mahesh-deshmukh/54/72b/113\\nwww.iicmr.org /n/r Big Data is making Big buzz today due to Big opportunities with Big Salary\\nWant to understand how Big Data is changing the industry and how you can make a career in it? Grab the opportunity to learn the Data analytics it from industry expert. Register at https://goo.gl/NwncY0\\n\\nLike and Share with your family, friends, colleagues & Acquiantances who may be benefited out of it. /n/r Big Data is making Big buzz today due to Big opportunities with Big Salary\\nWant to understand how Big Data is changing the industry and how you can make a career in it? Grab the opportunity to learn the Data analytics it from industry expert. Register at https://goo.gl/NwncY0\\nLike and Share with your family, friends, colleagues & Acquiantances who may be benefited out of it. /n/r Greetings from ATSS-IICMR, NIGDI, PUNE !!!\\n\\nWe Make You Future Ready.... Join MCA, a guaranteed launching pad for innumerable career options.\\n\\nFor MCA DTE Code (MC-6154) 100% Placement Assistance, International Certification Provided...\\n\\nFor more details call : 9921000870 / 9822951262\\ndo visit - www.iicmr.org /n/r Hans Rosling died.  :-(\\nSuch a wonderful inspiration!\\nMay he rest in peace.\\n\\n#datascience #statistics #math #sciencenews #science #educator /n/r Dear All, \\nI need small assistance on following issue related to KPI\\'s scorecard:\\n1) How to calculate or define KPI weightage ?\\n2) After getting weightage, how to calculate overall score with respect to KPI Actual value, KPI Target value and weightage /n/r We Design & Develop App\\'s On\\nLatest Technology  In Pocket-Friendly Cost For All Your Business Needs /n/r The Learning Zone, Pune invites you to take that first step on the road to a great career by learning how being a student is different than being an Executive and what needs to be practiced today to become a Successful Professional Tomorrow?\\nReserve Your Seat for February 12, 2017 \\nINTRODUCING 1-DAY MAGICAL TRAINING MODULE CAMPUS TO CORPORATE JOURNEY by Renowned Senior Mindset & Behaviour Trainer, Mr. Ramesh Sood , Master Practitioner NLP\\nReserve your seats at https://goo.gl/6sIFK6 /n/r The Learning Zone, Pune invites you to take that first step on the road to a great career by learning how being a student is different than being an Executive and what needs to be practiced today to become a Successful Professional Tomorrow?\\n\\nReserve Your Seat for February 12, 2017 \\n\\nIntroducing 1-DAY MAGICAL Training module - CAMPUS TO CORPORATE JOURNEY by Renowned Senior Mindset & Behaviour Trainer, Mr. Ramesh Sood , Master Practitioner NLP\\n\\nReserve your seats at https://goo.gl/6sIFK6 /n/r Here is a head() of my DataFrame df:\\n\\n                     Temperature  DewPoint  Pressure\\nDate                                                \\n2010-01-01 00:00:00         46.2      37.5       1.0\\n2010-01-01 01:00:00         44.6      37.1       1.0\\n2010-01-01 02:00:00         44.1      36.9       1.0\\n2010-01-01 03:00:00         43.8      36.9       1.0\\n2010-01-01 04:00:00         43.5      36.8       1.0\\nI want to select from August 1 to August 15 2010 and display only the Temperature column.\\n\\nWhat I am trying to do is: df.loc[[\\'2010-08-01\\',\\'2010-08-15\\'],\\'Temperature\\']\\n\\nBut this is throwing me an error.\\n\\nGenerally speaking what I want to learn is how, using loc method I can easily take a range of row i to row k and column j to p and show it in dataframe using loc method:\\n\\ndf.loc[[i:k],[j:p]] /n/r Dear all,\\nI have started working on my Final Year Project that is Text Analysis to detect crime related articles on web . For this I have to classify web documents. But I am not sure from where to start ? I am beginner in text mining and classification.  I want to learn text mining in JAVA from beginning.Please suggest me any good book or tutorials at the beginner level.\\nI have already studied Machine Learning and Artificial Intelligence courses. I have also gone through books like Practical Machine Learning Tools and Techniques, 2nd Edition (The Morgan Kaufmann Series in Data Management Systems) but honestly they did not make much sense to me. i would like a book or a resource or tutorials that has\\ninstructions from ground zero and implements Java.\\nAlso which open source framework would be good to implement here.\\nI am using netbeans 8.2 IDE therefore if you are suggesting me any open source then please tell me how do i integrate it with netbeans too. I would like to design front end and backend programming in Java. \\nThanks /n/r Big Data is making Big buzz today due to Big opportunities with Big Salary\\nWant to understand how Big Data is changing the industry and how you can make a career in it? Grab the opportunity to learn the Data analytics it from industry expert. Register at https://goo.gl/NwncY0\\n\\nLike and Share with your family, friends, colleagues & Acquiantances who may be benefited out of it.\\n\\n#IIHTVimannagar, #IIHT, #BigData, #Hadoop, #DataScience /n/r Is there any site which enables learning R for free? /n/r Hello experts,\\n\\nI have graduated with B.Sc. Applied Math and M.Sc. Financial math.\\nWorking as business analyst mostly configuring and testing a big financial program.\\n\\nMy goal is to be data scientist.\\nPlease comment/suggest what do you think about my way of turning to be data scientist:\\n1) Learning all the courses in data camp system.\\n2) Completing 2-3 data science, machine learning and models specializations in Coursera.\\n3) Completing the MIT specialization in data science.\\n\\nHere and there I will listen and do what people do in R/Python on YouTube.\\n\\nMy pleasure,\\nSteve /n/r Heard about Cloud Computing but do you really understand it? Cloud Computing is future of present day network and system administration technologies and the change is already in making a big impact...Salaries are sky -rocketing. Attend our seminar to gain more understanding.\\n\\nKey Take Away:\\n1. Understanding of Cloud Computing\\n2. Public / Private / Hybrid Cloud\\n3. PaaS / IaaS\\n4. AWS / Azure / VMWare etc.\\n\\nLimited seats available. Ensure to reserve your seats now at https://goo.gl/3oIXF0\\n\\nLike and share with your family, friends, colleagues and acquaintances who may be benefited out of it.\\n\\n#IIHTVimannagar, #IIHT, #CloudComputing, #AWS, #VMware, #Azure /n/r Happy Lunar New Year!\\n\\nEnroll for the upcoming CASUGOL Internet of Things (IoT) MasterClass with the promo code below and get an EXCLUSIVE discount!\\n\\nPromotion Code: CMCNY2017\\n\\nSee you in Class! /n/r So People call it over-fitting.. Here you go Computer Vision. #EndOfWorld /n/r It\\'s time. Time for a new adventure, time to broaden your horizon. Today we\\'re launching BaseCamp\\'s new edition: BaseCamp Data & Travel! This summer we host the first batch in Medellin, Colombia. New location, new format, more adventure! Explore Colombia with us, step off the beaten path and find your own. Applications are open now: www.basecamp.ai/data-and-travel #futureisdata /n/r Data Science and Machine Learning with Python - Hands On!\\n4.5 (2,910 ratings) 21,605 students enrolled\\nhttps://goo.gl/BAwQIx\\nHeavy Discount $200 >> $15\\n\\n2 Days Left\\nIncludes:\\n9 hours on-demand video\\n2 Articles\\nFull lifetime access\\nAccess on mobile and TV\\nCertificate of Completion\\n#data #iot #analytics #ai #bigdata #datscience #machinelearniing #dalalearning #dataviz #rstats /n/r Data Science and Machine Learning with Python - Hands On!\\n4.5 (2,910 ratings) 21,605 students enrolled\\nhttps://goo.gl/BAwQIx\\nHeavy Discount $200 >> $15\\n\\n2 Days Left\\nIncludes:\\n9 hours on-demand video\\n2 Articles\\nFull lifetime access\\nAccess on mobile and TV\\nCertificate of Completion\\n#data #iot #analytics #ai #bigdata #datscience #machinelearniing #dalalearning #dataviz #rstats /n/r Profusion Curious Club: Marketing trends & data science trends for 2017 with Michael Brennan #marketing #datascience #2017 profusion /n/r Dear colleagues, your are welcome to joint our group. https://www.facebook.com/groups/chembioinfo.networks/ and participate on MOL2NET Conference Series on Multidisciplinary Sciences, http://sciforum.net/conference/mol2net-02 /n/r Iiht Viman Nagar wishes you a happy Makar Sankranti, Lohri, Pongal and Bihu!\\n\\nAre you looking for a career in System Administration or Networking? Get an insight from an industry expert and experienced trainer. Attend our free demo batch. Register at https://goo.gl/3oIXF0 \\nCall us @ 7720040531 to know more.\\n\\nLike and share with your family, friends, colleagues and acquitances who may be benefitted out of it.\\n\\n#IIHT, #IIHTVimannagar, #MCSA, #Networking, #MicrosoftCertification /n/r Hi All - Our team is publishing an article related to data science  which will be ~ two pages and looking for a guest writer to help contribute about a page.  The goal is to showcase interesting data science topics coming directly from a person in the industry.  Please message me directly if interested.\\n\\nWe\\'re happy to cite you and your LinkedIn profile / website in our article.  Our last article had ~ 15K views. /n/r Contact for freelancing projects and online/offline training in following technologies. Follow the link below for more information.\\n-MATLAB\\n-SAS\\n-MS Excel\\nEmail: rahulkashyap@geekycody.com\\nPhone: +91-9717480811\\nwww.geekycody.com /n/r Science matters AI & Machine Learning #RSScienceMatters #datascience #AI #statistics #machinelearning #artificialintelligence #science #tech /n/r [Reminder - #CallForStartup] Hi, I\\'m here to remind you all that Calls for #DataDriven2017 (-10 days) and #Codemotion Rome & Amsterdam (-30 days) are still open. Hurry up!\\nLink for Data Driven Innovation: http://bit.ly/C4SDDI17\\nLink for Codemotion Rome & Amsterdam: http://bit.ly/C4SCM17 /n/r Hello - all looking to see what methods members here use to train their data? Do you hire external firms (MTurk), some other process? /n/r Symbolic Machine Learning: http://buff.ly/2jaAtCn\\n#datascience #machinelearning #logic #statistics #artificialintelligence #AI #tech /n/r hii guys.. i wanna learn hadoop. i do have basics in java. how and where can i start learning hadoop. thanks for ur suggestions /n/r Despite undeniable progress, the public sector is lagging behind compared to the private sector with regard to the control and operation of large volumes of data. However, new legislations and the policies of opening public data in many countries encourage modernization of IT infrastructure projects. These massive data sets are a largely untapped resource but if governments around the world really want to take advantage of Big Data, you need to ask some basic principles.\\n\\nhttp://ecmapping.com/2016/12/29/government-services-in-the-era-of-big-data/ /n/r R-Language or MatLab? which one is better? Any good resource for MatLab? /n/r If all variables are dependent on each other then can it predicted if yes how.? and not why? /n/r The basics of data mining with practical case studies is in this book:\\n\\nGiudici,P. \"Applied data mining\", Wiley, 2003 /n/r Learn about the basic concepts of data warehousing & data mining. https://play.google.com/store/apps/details?id=com.lifekart.eduquiz.datawarehouse /n/r Merry Christmas! /n/r Just created a FB page for sharing links to cool/random/open datasets on the web. I think it would be a very valuable to resource for us involved with Data Science, ML or AI. I\\'m surprised there isn\\'t a page for this already. Feel free to join if you\\'re interested! https://www.facebook.com/groups/1251805194890487/ /n/r Is there any free online practicing platform for big data technologies e.g. hadoop, hbase, hive /n/r #CallForStartup - Codemotion opens 3 calls for tech #startups: Data Driven Innovation comes back in 2017 for a new edition with even more big-data content; Codemotion conferences in Rome and Amsterdam are waiting for startups with #developer attitude!\\nSubmit till 19th January for #DataDriven2017 and 7th February for #Codemotion Rome & Amsterdam!\\nLink for Data Driven Innovation: http://bit.ly/C4SDDI17\\nLink for Codemotion Rome & Amsterdam: http://bit.ly/C4SCM17 /n/r Kindly visit and like our #academic #writing page thank you... /n/r As an aspiring Data Scientist who is working on a Masters Degree, which 2 classes should I take out of this list: 1) Machine Learning, 2) Social Network Analysis, 3) Big Data and NoSQL, 4) Big Data and Iot? Any suggestions/advice is appreciated. Thanks /n/r i have a set of temperature humidity pressure values that are classified like rainy,cloudy,sunny etc\\nconsider all these values are measured in 24 hr duration\\nhow can i predict the weather using it? i mean how will i get a single prediction like chances of rain, heavy rain,clearing like that?? \\n\\nthanks in advance /n/r Are you serious? about your career? ?Looking for 6 months #training in Jalandhar? ?\\nIf yes, then visit #AEGIS Institute - Industrial Training Center & meet our professional staff; you will feel the difference. ??\\n#Online #Registration \\nhttp://tinyurl.com/aegis-6-months-training-2017\\nEnquire now http://www.aegiseducation.in/ or ? 9041349575 /n/r hii everyone.. as everyone knows that data science is the sexiest job in this century, i wanted to become one. But the point is i dont know from where i have to start. I am about to complete my B.Tech in CSE. please give me some suggestions( from scratch to advanced level)  so that i can start learning and begin my career as data scientist or analyst. /n/r Working in the private cloud. A series of interviews with key talent working on various cloud based technologies.   This week it\\'s Machine Learning in the private cloud http://bit.ly/YPC-KW1 /n/r Any person here in this group who already worked with rough sets for Outlier detection? /n/r I want to know how to transform text data to valid data set then how can I make data mining on it \\nplease help with step by step /n/r Interested in big data in economics and finance? Join the FB group \\nSTATECONOMICS /n/r Hello,\\n\\nI am new to Data Science and want to learn about Data Analytics.\\nSo would like to know that if there is any INSTITUTE ( Online or Classroom) or any INTERNSHIP Program in Hyderabad, India which will be the best to join in it. \\n\\nPreferably i am looking who can teach based on real time examples rather than just focusing on theory part.\\n\\nYour help would be appreciable.\\nThanks /n/r can someone help me... i don\\'t know python,r or any kind of data mining.\\n\\nnow i need to analyze a set of sensor values saved in my database.. say temp,humidity,pressure etc \\nand i need to analyze this and get results like it will rain,cloudy,rainy etc\\n\\nhow to do this.. which is the simplest algorithm to do this in python. can anyone provide me the code for it..???\\n\\nsay i will be analyzing only 100 readings of each sensor at a time..ie,100 temp values,100 humidity values,100 pressure values.. and based on this i need the results.. \\n\\nso please suggest me the simplest algorithm and provide me a python code for the same... i don\\'t know python alsoooo...  please help me\\n\\nThanks in advance /n/r Kindly visit and like our #academic #writing page thank you... /n/r Enhance your #BigData Analytics, Administration skills and Drive Businesses Forward with the Hadoop Administration Training at Koenig Solutions.\\nGet course details here:  https://goo.gl/K0NWUW   #Koenigsolutions /n/r Build your career in the World\\'s Most Popular and highly compensated technical role with the  #Hadoop  #Developer Certification at  #Koenig Solutions. Get course details here: https://goo.gl/tzgvRd #koenigsolutions /n/r Online/Offline training for SAS (Base and Advanced), MATLAB, Excel (Basic and Advanced). MATLAB projects also available. \\nContact rahulkashyap@geekycody.com\\nhttp://www.geekycody.com/ /n/r Online/Offline training for SAS (Base and Advanced), MATLAB, Excel (Basic and Advanced). MATLAB projects also available. \\nContact rahulkashyap@geekycody.com\\nhttp://www.geekycody.com/ /n/r Kindly visit and like our #academic #writing page thank you... /n/r Welcome to the Big Data, Data Science, Data Mining & Statistics group!\\n\\nWe have a very diverse group of people with many different backgrounds and skills. Take advantage of this diversity by asking questions and learning from one another. We also have two threads for learning resources, one for online courses and one for books. Have fun and learn heaps!  -  Henrik Nordmark - Data Scientist\\n\\nGround rules:\\n\\nAny posts that are advertisements will be deleted and will be grounds for banishment from the group. You can however post genuine job postings provided you are not a recruiter. \\n\\nIf you wish to share information about a free educational resource such as new course on Coursera, edX or Udacity that you plan to sign up for or have taken in the past that is perfectly ok. You can also share about books, blog posts and podcasts that you have found useful so long as it is not self-promotion. Use common sense when sharing with the community.\\n\\nWishing you all the best in your data science journey! /n/r Is the 10 fold cross validation technique is the Best methodology to evaluate a classification model with data set of 321 samples.? /n/r What is the significance of weighted average over traditional average in a classification accuracy with three class? \\nWhy I should choose weighted average accuracy over average accuracy.? /n/r I absolutely love this... a tab-bar system for hybrid apps / web apps:\\n\\nSource here: https://github.com/ErlendEllingsen/app-tab-bar\\n\\nDemo here: https://erlendellingsen.github.io/app-tab-bar/live_demo/tabs.html /n/r Henrik, thanks for letting me join. -Jeff /n/r Apache #Spark: A Unified Engine for #BigData Processing\\n\\nhttp://cacm.acm.org/magazines/2016/11/209116-apache-spark/fulltext /n/r Wide & Deep Learning for Personalized Recommendations #datascience #deeplearning #machinelearning #AI #statistics #bigdata /n/r Future of #AI panel discusses whether jobs will be obliterated? #ArtificialIntelligence Future Advocacy /n/r Transparency of AI? #ArtificialIntelligence Future Advocacy /n/r Technological unemployment in the near future? #ArtificialIntelligence Future Advocacy #Economics /n/r An intelligent future? #ArtificialIntelligence Future Advocacy /n/r Hi all , \\n\\nIn many classification learning algorithms the result always be a probability of relation of one of the classes .\\n\\nHow can i interpret this probability in term of business needs ?\\nHow can i say if this observation has these factor measure then it belongs to class X ? \\n\\nNeed help please ... Hopefully on SVM . /n/r Visually Linking #AI, #MachineLearning, #DeepLearning, #BigData and #DataScience\\n\\nhttp://bit.ly/2dobCHC ? /n/r #Creative Solution for your #Business..!\\ninfo@mavericksindia.com\\nhttp://www.mavericksindia.com/ /n/r Three Stages of AI\\n\\nhttp://buff.ly/2dFOJFm\\n\\n#BigData #MachineLearning #DataScience #AI /n/r Hard core SQL Server & Data Science   free webinar today \"SQL Server R Services - Configuration and Management\" direct from the product team, 12 pm Eastern  https://attendee.gotowebinar.com/register/372645759112367875 /n/r Select from a list of 85 MATLAB projects\\nhttp://geekycody.com/matlab-projects-list-on-geekycody/ /n/r I want to know a person with Economics &Staistical back ground have any prospect of learning bBig Data Hadoop Ramesh /n/r Mavericks Web Services (I) Pvt. Ltd.\\n \"Made with passion driven with values\" /n/r #MOL2NET #Researchgate #Collaborative #Project, click the link and follow our project: an online scientific conference devoted to promote multidisciplinary collaborations among a network of face-to-face workshops and social networks.  https://www.researchgate.net/project/MOL2NET-Conference /n/r Hackers, this is your chance to hack for a fully paid trip to Hong Kong (and much more)!!\\n\\nJoin the HackTrain this 4-6 of November for an immersive hackathon where 120 developers, designers & entrepreneurs will be building websites and apps whilst on moving trains across Europe! \\n\\nApplications are open at http://uk.hacktrain.com\\n\\nIf you have any questions feel free to comment below or drop us a line on hello@hackpartners.com /n/r look at that stuff\\n\\nHey, \\n\\nLook  at this  great and exetremely interesting stuff,  it\\'s admired by so many people, just take a look <http://kivyquoho.thearcadiaschool.com/e4crb>\\n\\nKind regards, saxena.ankit87 /n/r Don\\'t  believe the hype: The data scientist shortage is being overblown. You just need to know where to look. http://deloitte.wsj.com/cio/2016/08/11/the-myth-of-the-data-scientist-shortage/ /n/r Pok?mon Going Going Gone /n/r CALL FOR PAPERS: MOL2NET International Conference Series on Multidisciplinar Sciences, MDPI, SciForum, Basel, Swiztherland, HQ UPV/EHU, Campus Bizkaia, Basque Country, http://sciforum.net/conference/mol2net-02\\n ASSOC WORKSHOPS: The conference is online without registration, publication, or travelling costs. However, associated workshops run presentially on their organizing centers like: #IWMEDIC-04, Univ. of Coru?a, #Spain, #SUIWCS-01, Univ. of Soochow, #China, #WRSAMC2016, Univ. of Paraiba, #Brasil, etc. http://bit.do/mol2net-workshops /n/r Last call: sign up for #SpaceAppCamp at http://www.app-camp.eu/. For inspiration check ESA - European Space Agency http://bit.ly/29VJUXg. This is where #coding meets #Earthobservation. Sign up and be part of this unique event in Frascati, Italy. /n/r Last call: sign up for #SpaceAppCamp at http://www.app-camp.eu/. For inspiration check ESA - European Space Agency http://bit.ly/29VJUXg. This is where #coding meets #Earthobservation. Sign up and be part of this unique event in Frascati, Italy. /n/r Submit your idea related to #transport for the BMVI #EarthObservation Challenge for Digital Transport Applications. Exploit the potential of Copernicus in possible combination with other geo-data and develop new transport applications. Submission is open until 25 July at http://bit.ly/29Otv15 /n/r Last call for Astrosat\\'s End-to-End EO Challenge: Watch the video & submit your idea at http://bit.ly/29OdDeQ #ISS #sensors Stevenson Astrosat /n/r FREE HADOOP REGULAR BATCH starts on 20th @ 7AM by Mr.SREERAM(Data Scientist). /n/r FREE HADOOP REGULAR BATCH starts on 20th @ 7AM by Mr.SREERAM(Data Scientist). /n/r Like to be a Data Scientist?\\n Attend webinar on Data Science by  Mr. Ravi Subject Matter Expert\\n When: 14th July 2016, 7 A.M IST / 13th July 2016 9-30 PM EST\\n High Demand & Highest Pay for Data Scientist\\n Click to register  :  (https://attendee.gotowebinar.com/register/7404924216406746113 ) /n/r CALL FOR PAPERS: #MOL2NET 2016, 2nd International Conference on Multidisciplinary Sciences is accepting submissions of communications. http://sciforum.net/conference/mol2net-02\\n\\nONLINE & FREE OF COST: Participation, and Publication of Posters and Communications (1-2 pages) is free of cost and online with doi number asap upon acceptance, saving traveling and registration costs.\\n\\nASSOCIATE WORKSHOPS: The conference have multiple #online and #presential #workshops associated to it worlwide, like #IWMEDIC2016, Coru?a, Spain, #SUIWML2016, Soochow, China, #MODEC2016, Puyo, Ecuador, and #CIESA2016, Toluca, M?xico. See details: http://sciforum.net/conference/mol2net-02\\n\\nSUBMISSION: Submissions link shall be open from now on and until 2016-Nov-25 when the conference is planned to be officially started, and the online discussion begins, please use direct submission link: http://sciforum.net/user/submission_for_conference/129\\n\\nKEYWORDS: #Science (Multidisciplinar), #Chemistry (All areas), #Medicine, #Physics (Applied), #Computer sciences, #Biotechnology, #Nanotechnology, #Mathematics, #Statistics, #Data analysis, #Bionformatics, #Complex #Networks, #Systems #Biology, #Materials science, etc.\\n\\nMOL2NET International Conference on Multidisciplinary Sciences, \\n 2015, Dec, 05\\x9615, MDPI Siforum, HQ UPV/EHU Bizkaia, \\nhttp://sciforum.net/conference/mol2net-1, the first edition now officially closed; attracted Papers/abstracts: 100+, Participants: 150+, Institutions: 30+, Countries: 20+ (including committees & authors)\\n\\n* Participants (committees/authors) are affiliated to 30+ institutions such as: #Standford University, #Harvard Medical School, University of #Pennsylvania, University of #Minnesota, Commonwealth University of #Virginia, University of #Miami, University of #Nebraska, #EMBL-EBI #Cambridge, University of #Paris Sud, #CNAM Paris, University of #Reading, University of #Strathclyde, University of Rostock #Germany, University of Porto, University of Santiago de #Compostela, University of #Coru?a, Chinese Academy of Sciences, and Universidade Federal de Goi?s, etc. along with the HQ host institutions University of #Basque Country #UPV/EHU and #Ikerbasque, Basque Foundation for Sciences.\\n\\n*The countries represented include, but are not limited to, #USA, #Canada, #M?xico, and #Brasil (in #America); #UK, #France, #Germany, #Portugal, and #Spain (in #Europe) as well as #China, #India, #Japan, and #Vietnam (in #Asia).\\n\\nSincerely yours\\nConference Chairperson\\n Prof. Humberto Gonzalez-Diaz, PhD., Pharm.Lic.\\n IKERBASQUE Professor of Department of Organic Chemistry II,\\n University of Basque Country UPV/EHU, Campus Bizkaia /n/r Hi,Can someone please guide me about possible machine learning approaches (other than sequence labeling) for parsing free form text into p.o.box,city,state,country /n/r VACANTES DATA SCIENCE - MINERIA DATOS\\n\\nActualmente tenemos unas vacantes de Miner?a de Datos en un importante Banco l?der en el Sistema Financiero Colombiano.\\n\\nPERFIL: (solo enviar Hoja de Vida si cumple con el perfil)\\n\\n\\x95 Nacionalidad: Colombia\\n\\x95 Profesionales de carreras relacionadas con las ciencias exactas (matem?ticas, f?sica, ingenier?a, estad?stica, sistemas, etc\\x85) con Postgrado (de preferencia en temas relacionados con Data Science y Miner?a de Datos).\\n\\x95 Experiencia y conocimiento en  modelaci?n matem?tica, estad?stica y miner?a de datos (como regresi?n, redes neuronales, aprendizaje autom?tico, etc\\x85)\\n\\x95 Buenas habilidades en programaci?n (Phyton, R, SAS, SPSS, Spark, Hadoop, SQL\\x85)\\n\\nCONDICIONES LABORALES:\\n\\nLugar: MEDELLIN, Colombia. \\n\\nEl salario fijo Anual Total es de  61 millones de pesos (incluyendo cesant?as, primas)\\n \\nBono anual  entre 0 y 6 salarios mensuales de acuerdo a los resultados del banco.\\n\\nBeneficios extralegales como cr?ditos a tasas bajas, seguros de vida y de salud, exenci?n en cuota de manejo, etc..\\n\\nPROCESO DE SELECCI?N:\\n\\n\\x95 Enviar  hoja de vida al correo:  aibarraquiceno@hotmail.com   \\nPlazo: Domingo 31 de Julio del 2016 11:59 pm. \\n\\n\\x95 Prueba T?cnica: Si su hoja de vida clasifica recibir? un correo con las instrucciones para realizar una prueba t?cnica que consiste en el desarrollo de un modelo predictivo. \\n\\n\\x95 Entrevista. Los que tengan los mejores modelos se citar?n a entrevista con el gerente del ?rea y si clasifican entrar?n al proceso de selecci?n del Banco.   (puede ser por Skype si no se encuentran en Medell?n). \\n\\n\\x95 Decisi?n final. La idea es tener finalizado el proceso a fines del mes de Agosto 2016 para empezar labores en Septiembre. /n/r We Are Your Campus Engagement Network. /n/r The UK votes to leave the European Union: a very sad day in European and world history. #eureferendum #brexit /n/r 3 Enterprise Business Intelligence Trends That Can Benefit Your Business \\n  When it comes to changes in the business-intelligence (BI) technology market, old giants of the...  Keep on reading:  3 Enterprise Business Intelligence Trends That Can Benefit Your Business /n/r A Complete Tutorial to Learn Data Science with Python from Scratch /n/r What version of MS Office are you using in your job? /n/r I want to classify the Websites into the Business Categories, the way SIC codes does (http://siccode.com/) or even better than this using machine learning. For Initial steps I tried fetching \"About_us\" text from the websites and applied LDA on it, which gives me a list of words.\\nI want to know if my initial steps are going to help further or is there any better model/technique, I could apply for the same.\\nAny Resource or direction towards this will be very helpful\\nThanks /n/r T-Systems Open Telekom Cloud Challenge \\x96 Handling Big Data with Cloud Computing\\nDo you have a groundbreaking idea or service how to use #Sentinel data in combination with other data sources on @Telekom\\'s public cloud platform? The T-Systems Open Telekom Cloud Challenge is particularly interested in solutions that offer benefits to European citizens and their public administrations, which face challenges in meeting increased demand for mobility, urban logistics, tourism, and more while simultaneously reducing negative environmental impacts. Make #bigdata part of your idea through #cloud services, submit your promising solution at www.t-systems.copernicus-masters.com/, and get your project off the ground! http://bit.ly/1WrUaXH /n/r At Strata+Hadoop World in London over the next three days! Yay! Any of my data minded friends also here? #StrataHadoop #datascience #bigdata /n/r Does hadoop supports only HDFS file system? /n/r Lucknow tops Fast Track Smart Cities Challenge.....Chandigarh, Newtown Kolkata and Panaji included in the new list of 13.... Check out the interactive Visualization here: http://goo.gl/bjOUat #Update #Visualization #SmartCities #India /n/r #Python + #Hadoop for #BigData applications #pydatalondon #datascience #statistics #machinelearning #Spark #PySpark /n/r Indirect #Data Is the Travel Industry\\'s Secret Weapon \\n  This travel season is shaping up to be the busiest since 2008, according to results from several...  Keep on reading:  Indirect #Data Is the Travel Industry\\'s Secret Weapon /n/r Treating Information as an Asset \\n  The emergence of chief #data officers (CDOs) in many organizations and across industries indicates a growing recognition of information as a strategic business asset \\x96 one distinguished from the technology through which it flows. In fact, by 2020, Gartner predicts that 10 percent of organizations will have a highly profitable business unit specifically for productizing and commercializing their information assets. \\n   \\n  According to Douglas Laney, vice president and distinguished analyst at Gartne...  Keep on reading:  Treating Information as an Asset /n/r If \\'data is the new oil\\', why aren\\'t more companies drilling into analytics? http://deloitte.wsj.com/cio/2016/04/27/industrialized-analytics-datas-new-potential/ /n/r Predictions 2018. Like our page for more https://www.facebook.com/CTEPL /n/r Data Scientist is needed for immediate hiring for a multinational company located in Gurgaon, 10-12 years\\' overall experience with 7-8 years in Data Science with qualification as: - Graduated from Computer science or engineering  - Experience with Machine Learning, predictive analysis, regression modelling, R, Phython, etc. Data mining experience is must- For all interested candidates, please send your resume to sameer.johar@aslhr.com mentioning \"Data Scientist\" in the subject. /n/r Game Of Math...The MaxEnt Algorithm and Game of Thrones: \\nhttp://www.mathisintheair.com/eng/2016/03/24/game-of-math-the-maxent-algorithm-and-game-of-thrones/ /n/r Great coverage of Israeli Big Data Spy Tech on the Financial Times /n/r Analytics can transform audit, but it won\\'t put auditors out of work. http://deloitte.wsj.com/cio/2016/03/22/innovation-in-audit-takes-analytics-ai-route/ /n/r Does anyone have useful resources for data science use cases in education?\\n\\nMany thanks fellows. /n/r I am preparing a possible lab session for the PASS Business Analytics Conference.  The conference attendees are analysts and directors:  the two-hour lab is intended for hands-on experience.  \\n\\nMy working title is \"SQL Server Data Science with R and Python\"   based on what you know of what BI analysts may do with SQL Server 2016, what do you believe I should promise in an abstract? /n/r My first attempt to create a Data Visualization App using R.  This App lets you view the details of the Smart Cities of India(in different stages) on a Map and a Table. I would love to hear your feedback on this and please share the link to the app if you find it useful. \\n\\nApp Link: https://avinashr.shinyapps.io/Smart_Cities_India /n/r It has just started this morning. Don\\'t miss the boat ! Join for FREE at https://goo.gl/G5VOPl /n/r What Industries Will Be Next to Adopting Data Science? /n/r What is needed to build a data science team from the ground up?\\nThe recruitment and hiring managers should focus on the individual skills that are needed on the data science team and aim to hire people with strengths in these skills.\\n\\nRead full article here: http://goo.gl/yCAoCe\\n\\nFeel free to share it on your blog, but please remember to keep a link to our site www.3blades.io or better yet, to the original post url http://goo.gl/yCAoCe\\n\\nThanks! /n/r J-3 BREAKING NEWS > le patron d\\'AIRBNB rejoint les 900 dirigeants du \\n#WEB2B2016 \\n18/02 - PARIS\\nTheEvent\\n17h50 salle amphit?atre - KEYNOTE 7 - Economie du PARTAGE, UBERISATION\\x85 Qu\\'allons nous devenir ?\\n> Denis JACQUET Entrepreneur du Net depuis 2000, fondateur de Parrainer la Croissance, edufactory\\n> Gr?goire LECLERCQ Pr?sident de la F?d?ration des Auto-entrepreneurs\\n> Vincent RICORDEAU Co-Fondateur et PDG Kisskissbankbank\\n> Pascal PICQ pal?oanthropologue, ma?tre de conf?rences du Coll?ge de France\\n> Olivier MATHIOT Cofounder, CEO, Priceminister (sous reserve)\\nNEW > Nicolas FERRARY, CEO, Airbnb /n/r Getting Started with Azure Machine Learning & DataZen (Mobile BI and Data Analytics for Any Device)\\n\\nhttps://msevents.microsoft.com/CUI/EventDetail.aspx?EventID=1032733296&Culture=en-IN&community=0 /n/r Data science, undoubtedly, requires numerous and varied skills. Analytics Week, the world\\'s largest analytics network, identified 25 data skills that make up the field of data science, trough a survey conducted to 490 data professionals from different companies. Their project ended up being an excellent way of providing a look into the field of data science. /n/r Data science, undoubtedly, requires numerous and varied skills. Analytics Week, the world\\'s largest analytics network, identified 25 data skills that make up the field of data science, trough a survey conducted to 490 data professionals from different companies. Their project ended up being an excellent way of providing a look into the field of data science. /n/r Do Big Data help To Evolve or Adapt ? /n/r You don\\'t need \\'perfect\\' data for analytics. http://deloitte.wsj.com/cio/2016/02/04/you-dont-need-perfect-data-for-analytics-analytics/ /n/r What do you think about the explosion of solution for Big Data ? /n/r Start Up Facts.\\nFor more, Like Us https://www.facebook.com/CTEPL/ /n/r App and Workspace Discovery Demo: http://goo.gl/ZSNBrm /n/r I want to switch my career.please advise me what needs to done to get job in data science profile. /n/r Thank you for accepting me in the group. Greetings to all. I am here for whatever you need me. /n/r Our client is a technology startup in the online media space. Leveraging automation and crowdsourcing, they create high quality media data including captions, timed transcripts, indexes and video intelligence for clients in the online education, enterprise and entertainment industries. \\n\\nThey are currently searching for a Data Scientist who is passionate about machine learning, NLP and big data.\\nResponsibilities include deriving insights from data from various sources - libraries of online media, user data, social media, etc using natural language processing, big data and machine learning techniques.\\n\\nThe ideal candidate will be highly skilled in NLP, Machine Learning Techniques. Hands on development skills in Python and R. \\nThe ability to communicate technical concepts to business users, debate technical tradeoffs and create technical specifications is a must.\\n\\nYou will work in premier locations in NYC, casual environment, on diverse and fast-paced projects, and get experience by working with scary-smart people in a dynamic startup environment. We are hiring now, if you are interested, send your resume at dejan@ivyexec.com /n/r Analytics is no longer a nice to have, leading organizations will link analytic initiatives firmly to financial objectives, increase investments in advanced analytics, evolve comprehensive analytics centers of excellence, and incorporate a wider range of exogenous data. /n/r [ BigData Startups aboard the world\\'s first #RailTech accelerator! ]\\n\\nA 3-month programme where startups can obtain up to 25,000 funding, office space in central London, and unique opportunities to trial + sell products to the largest train operating companies in the world!\\n\\nWe are looking for startups and fast-growing scale-ups solving problems in industries outside the railway sector! Examples can be resource management, indoor navigation, data analytics, prediction, between many other disruptive enterprise services!\\n\\nApply at accelerator.hacktrain.com! /n/r The purpose of this group is to bring together \"young\" (not-so-young are welcome as well) Bayesian statisticians (not-so Bayesian are welcome as well) in order to foster interactions, tell people about interesting workshops, conferences, jobs, scholarships, or anything related to the Bayesian world. /n/r Big Data Market -Forecast And Analysis 2018 -Transform the Unstructured Data for Government \\n\\n#Big #Data is a term use to describe the process of collecting, organizing, and analyzing large sets of big data to discover hidden patterns, unknown correlations, and other useful information.\\n\\nGet Brief Information@ http://goo.gl/NVkrXa\\n\\nGermany is one of the emerging countries in big data technology market, and it is expected to grow in the upcoming years due to the adoption by various sectors such as internet, e-commerce, advertising, and others.  \\n\\nBig data help you to understand the information contained within the data, and help to identify the data which is most important for the business future business decisions.\\n\\n Big data also find disruptions to the production process before and as they take place. These findings can save noteworthy amount of money on equipment or machinery, and reduce labor expenditure on accidental maintenance and repairs. /n/r How and where can i learn big data analytics /n/r Analytics can reduce the complexity from your business processes. http://deloitte.wsj.com/cio/2015/12/21/taming-complexity-with-analytics/ /n/r Big data is a term use to describe the process of collecting, organizing, and analyzing large sets of big data to discover hidden patterns, unknown correlations, and other useful information. Germany is one of the emerging countries in big data technology market, and it is expected to grow in the upcoming years due to the adoption by various sectors such as internet, e-commerce, advertising, and others.  Big data help you to understand the information contained within the data, and help to identify the data which is most important for the business future business decisions. Big data also find disruptions to the production process before and as they take place. These findings can save noteworthy amount of money on equipment or machinery, and reduce labor expenditure on accidental maintenance and repairs.\\n\\nDownload Complete PDF Brochure : http://goo.gl/NVkrXa /n/r STATISTICAL INFERENCE, a must read !!! /n/r Hey every one , i have a problem understading Data Analysis & Statistics (ACP method , ....) , if you have something  can help me like tutorial videos Or good links to check out , thank you /n/r FINAL CALL! For all those interested in data and statistics and making them more accessible and understandable for all. #BeyondGDP #Data #StatsForAll #Stats  #SocEnt  #SocEntData  #Wellbeing  #SocialInnovation  #Innovation /n/r Spring XD 1.3 GA introduces Flo for Spring XD 1.0 + a job composition DSL http://spring.io/blog/2015/11/19/spring-xd-1-3-ga-and-flo-for-spring-xd-1-0-ga-released /n/r How many bits mem0ry are occupied by logical address and physical address /n/r Hey guys,\\nI have a dilemma I wish to do a certification in data science(R, Predictive modelling), but most companies who hire for data science also requires Big data knowledge( Hadoop, Hbase etc). So wats the difference between the them. How these two differ from Machine learning. On the whole which course should I do? /n/r Big Data Halloween /n/r Are there any courses (short term/post-graduate...) in Data Science in Indian Universities? /n/r How to decide number of nodes in artificial neural network? please suggest some material(s) on this, if possible. /n/r The Big Data HackTrain hackathon is coming with double the prizes, hence double the fun :D\\n\\nThe winners of the HackTrain will not only get a fully paid trip for four to Singapore, but they will also obtain a fully paid trip for two to Amsterdam!!\\n\\nThe most epic hackathon in a train is coming at 200km/h, and you don\\'t want to miss it!!\\n\\nApplications are still open at http://hacktrain.com /n/r ?Please Like and Share the following FB Page: Big-Data Analytics Company?\\n\\nDear all, \\n\\nI\\'m sure you are doing great as always!\\nMy name is Ken Tanaka (4th Year Medical student at Chiba University Medical School in Japan). \\n\\nI founded Big-Data Analytics company and entered business plan competitions. Since the number of \"Likes\" of the company\\'s facebook page is one of the selection criteria of the competition, I would appreciate so much if you can kindly \"Like\" and \"Share\" the following FB page. \\n\\n https://facebook.com/profile.php?id=1084593791580673&_rdr\\n\\nThanks again and I wish you all the best for your future success!\\n\\nBest, \\nKen /n/r [All aboard the Train Hackathon]\\n\\nHack. Build. Design. Launch Big Data prototypes whilst on a moving train! Obtain a chance to win a fully paid trip to Singapore and Amsterdam!\\n\\nApply now at http://hacktrain.com/\\n\\nIf you have any questions please feel free to send us a message!\\n\\nSee you aboard Trainhackers! #HackTheRails #LetsDoThis /n/r Big Data Economics, Towards Data Market Places: Nature of Data, Exchange Mechanisms, Prices, Choices, Agents & Ecosystems\\x85.\\n\\nThe utilization of enormous information to accomplish operational proficiency, ascend in value-based and unstructured information, development in use of huge information for showcasing exercises, development out in the open segment, and issues in regards to information security are a key\\'s portion calculates that are in charge of the Japan\\'s development huge information market. On the other hand, it is extremely vital to investigate all arrangements of information to recognize all the concealed examples in the information. Huge information serves to comprehend distinctive data identified with information, and empowers parameters which are essential to take business-related choices later on.\\n\\nTo Know More Access PDF Brochure@ http://goo.gl/ncfv2J , /n/r NEWS I Congratulations to the Top 12 World Finalists at 2015 Big Data Analytics World Championships for Enterprise (TEXATA).  Well done to: IBM, FICO, KPMG, Barclays, HP Labs, TNG Quant, Intrum Justitia, 6Sense, Universidad Aut?noma de Madrid, University of Potsdam and Castlight Health >> http://www.texata.com/finalists/ /n/r The factors which turned the decision for Munich Re in favor of #SAS were the speed at which the #analyses were carried out, the upward graph in the tech graph, the performance of the team for SAS overall and the ability of the system to deliver and deploy results swiftly. Read more https://goo.gl/IlAoP9 /n/r #MOL2NET (ONLINE & FREE OF COST), 2015,15-30 Nov, MDPI Sciforum\\nInternational Conference to Foster Interdisciplinary Collaboration in Science. \\nEvent call: https://www.facebook.com/events/545795472245161/\\nOfficial Conference web: http://sciforum.net/conference/mol2net-1 \\nThe scope includes, but is not limited to, Experimental #Chemistry (all branches), #Materials, #Nanosciences, #Medicine, #Neurosciences, #Biomedical #Engineering, #Biosciences, #Biotechnology, #Statistics, #Bionformatics, #BigData #Analytics, #Computer and #Network #Sciences. \\n\\nCONFERENCE CHAIRMAN: Prof. Humbert Gonzalez-Diaz, IKERBASQUE Professor, Department of Organic Chemistry II, University of Basque Country UPV/EHU, Bizkaia.\\n\\nADVISORY COMMITTEE (See full list in the following link)\\nhttp://sciforum.net/conference/MOL2NET-1/page/organizers\\n\\nNOTES: \\n * The conference is Totally Online, no physical presence is needed saving travelin costs. We accept experimental works, theoretical works, or experimental-theoretic works in different areas including, but not limited to, all areas mentioned above; as well as #Legal and #Regulatory issues. \\n\\n * Proceedings wil be Published Online, Open Access, Totally and Free of Charges (not cahrges wil be lieved to authors). Online submission system is ready and will be open until 2015, Nov, 10, please, follow the instructions: \\n\\n(1) read call for papers: http://sciforum.net/conference/MOL2NET-1/page/call\\n\\n(2) Read instructions to authors: http://sciforum.net/conference/MOL2NET-1/page/instructions\\n\\n(3) Download template: http://sciforum.net/bundles/sciforumversion2/images/conference/MOL2NET-1/MOL2NET-template.docx\\n\\n(4) Sing up and submit the title, authors, and abstract of your short communication: http://sciforum.net/user/submission_for_conference/83\\n\\n(5) Wait for abstract approval email to login and submit the full version (.doc and .pdf) of your short communication (2-3 papes) or research paper, slide presentation, or video (optional): http://sciforum.net/user/submission_for_conference/83 /n/r Please help me.What are the Decision tree multi label classification algorithms like Naive bayes in Data Mining.. /n/r Hi,\\nI would like to know, that is it mandatory to scale a data set for implementing a two class(binary), logistic regression algorithm for  classification ? Does it improve precision ?\\nThanks in advance. Any suggestion will be much appreciated !! /n/r Round 1 of TEXATA Big Data Analytics Competition starts in 3 days. Who\\'s in your company or university league? Share with interested friends. http://www.texata.com/ /n/r Hi all. Can anyone recommend a good resource (a book, a course, a youtube channel, etc) on Bayesian Probability and Statistics?  Intermediate level and ideally something that is free to view/download, please :).  Thanks in advance for your help! /n/r Aoa.......Can anyone have the derivation of expected mean squares for two factor factorial experiment.......E(MSA),E(MSB),E(MSAB) and E(MSE)???? /n/r Aoa.......Can anyone tell me the applications of factorial design??? /n/r UNICORE (Uniform Interface to Computing Resources) offers a ready-to-run Grid system including client and server software. You are invited to join and post pertinent content: https://www.facebook.com/groups/320795044682/ /n/r Thought of Day- Co-relation does not imply Caucasian /n/r Hi Everyone\\nI am planning to take up Data Science/Big Data as my specialization for my Masters study.I came across different courses such as Data analytics, business analytics. Are they both same.\\nI also wanted to know which mid-ranged universities (preferably Public) in the US offer the best of Data Science courses.\\nAnd what are the Job Prospects in the US for a person with Data science as his specialization. /n/r If you are interested in Tech then stay in touch...jobs, news, jokes, pics, discussions and much much more.\\nhttps://www.linkedin.com/grp/home?gid=7470692\\nhttps://www.facebook.com/revolutiontechnologyltd\\nwww.revolutiontechnology.co.uk \\n@revtechnology1 /n/r If you are interested in Tech then stay in touch...jobs, news, jokes, pics, discussions and much much more.\\n\\nhttps://www.linkedin.com/grp/home?gid=7470692\\nhttps://www.facebook.com/revolutiontechnologyltd\\nwww.revolutiontechnology.co.uk \\n@revtechnology1 /n/r i was able to download only 7 days (one weak) tweets using twitter API but i am in need of all tweets related to particular hash tag and geo code . any one please share or email the python script at mail4rajesh87@gmail.com thanks in advance . Plse help to share. /n/r Hi All,\\n\\nI am very happy to join such a knowledgeable group.\\n\\nIs here anyone can give me a formula to calculate KPI with four parameters,\\n\\nQuantities\\nObtained Quantities\\nTime Utilization \\nError\\n\\nI hope you guys will respond me asap.\\n\\nThank you.\\n\\nKind Regards,\\nTariq Hussain /n/r hi friends... please suggest Books related to basics of Big Data. i want to make those books as reference books to my bachelor of degree students. \\nthanks in advance..... /n/r Group focused in topics related to software agents and agent systems. You are invited to join and post pertinent content: https://www.facebook.com/groups/299640474516/ /n/r Going to the European Conference on Data Analysis! Yay! #ecda2015 #datascience #statistics #machinelearning #dataanalysis #essex #bigdata /n/r Hello guys I\\'m planning to create a music library in my college accessible to all students inside the campus.It will be available only on intranet for that I want to implement concepts of Hadoop in it,can anyone suggest what frameworks should I use and in what ways I can implement them?? /n/r Hey guys! \\n\\nI\\'m currently developing my undergraduate thesis for Computer Science and I\\'m looking for a problem related to Data Science that I can solve in 6-8 months. It would be great if this problem is also related to sales, marketing or organizational development since I like those topics too. \\n\\nHave you worked in this fields? What kind of problems have you encountered? What are the trending applications of Data Science right now?\\n\\nThanks for your response! /n/r Fundamental Basics of Information Technologies; You are invited to join and post pertinent content: https://www.facebook.com/groups/184721844885433/ /n/r Know some great Data Scientists and Big Data friends in your Business or University? Enter the TEXATA 2015 Big Data Analytics World Championships for Business and Enterprise. Two Online Rounds and Live World Finals in Austin Texas. Round 1 Starts in 5 Weeks >> http://www.texata.com /n/r The Annals of Computer Science and Information Systems (ACSIS) is an online journal-style series reporting theoretical and applied research results in computer science and information systems. The series publishes contributions in a broad area that crosses the boundaries between science (including social science), engineering and management. It accepts publications resulting from the commercial market, as long as the discussed industrial problems stimulate the related sciences and can impact the engineering profession. Publisher: Polish Information Processing ociety. Volume proposals are welcome. More info available at: annals-csis.org. You are invited to join and post pertinent content: https://www.facebook.com/groups/938470596191055/ /n/r Love CSS? Join this group for CSS4 updates. Share for a reason. \\n\\nhttps://www.facebook.com/groups/css4.group /n/r #DexLab #Analytics presents an Introductory Session on #Ms #Excel #VBA Macros & Dashboards. To be conducted by industry professional working in to consulting and analytics, you will be given an insight in to the following areas:\\n\\n \\x95 Overview of Excel and its application in various domains\\n \\x95 Functions in Excel\\n \\x95 Graphs\\n \\x95 V Look Up and Pivot Tables\\n \\x95 Macro recording in Excel and automation\\n \\x95 Over view if Dashboards.\\n\\nDate: Saturday, 22nd August 2015\\nTime: 2:00 to 3:30 PM\\nVenue: K 3/5, DLF Phase 2, Gurgaon, Haryana - 122 002.\\n\\nThe session will be followed by Q&A sessions. The session is free of cost and would require prior registration. To register your self call at +91124450222. Visit at www.dexlabanalytics.com /n/r #DexLab #Analytics presents an Introductory Session on #Ms #Excel #VBA Macros & Dashboards. To be conducted by industry professional working in to consulting and analytics, you will be given an insight in to the following areas:\\n\\n \\x95 Overview of Excel and its application in various domains\\n \\x95 Functions in Excel\\n \\x95 Graphs\\n \\x95 V Look Up and Pivot Tables\\n \\x95 Macro recording in Excel and automation\\n \\x95 Over view if Dashboards.\\n\\nDate: Saturday, 22nd August 2015\\nTime: 2:00 to 3:30 PM\\nVenue: K 3/5, DLF Phase 2, Gurgaon, Haryana - 122 002.\\n\\nThe session will be followed by Q&A sessions. The session is free of cost and would require prior registration. To register your self call at +91124450222. Visit at www.dexlabanalytics.com /n/r DexLab Analytics presents an Introductory Session on Ms Excel VBA Macros & Dashboards. To be conducted by industry professional working in to consulting and analytics, you will be given an insight in to the following areas:\\n\\n \\x95 Overview of Excel and its application in various domains\\n \\x95 Functions in Excel\\n \\x95 Graphs\\n \\x95 V Look Up and Pivot Tables\\n \\x95 Macro recording in Excel and automation\\n \\x95 Over view if Dashboards.\\n\\nDate: Saturday, 22nd August 2015\\nTime: 2:00 to 3:30 PM\\nVenue: K 3/5, DLF Phase 2, Gurgaon, Haryana - 122 002.\\n\\nThe session will be followed by Q&A sessions. The session is free of cost and would require prior registration. To register your self call at +91124450222. Visit at www.dexlabanalytics.com /n/r The RuleML Initiative is an international non-profit organization covering aspects of Web rules and their interoperation. You are invited to join and post pertinent content: https://www.facebook.com/groups/ruleml/ /n/r 1- Incremental clustering algorithms\\n2- Online clustering algorithms\\n3- Data stream clustering algorithms\\n\\nWhat does it means incremental clustering , Are the following expressions related? Does some of them include others? What is the difference between them? What are the constraints that each one should face unlike others? /n/r The area of Scalable Computing has matured and reached a point where new issues and trends require a professional forum. As a response to this need, the SCPE Facebook group focuses on all topics pertinent to, broadly understood, scalable computing. In this way it goes hand-in-hand with the \"Scalable Computing: Practice and Experience\" international peer-reviewed journal. The SCPE publishes original refereed papers that address the present as well as the future of parallel and distributed computing. The journal focus on algorithm development, implementation and execution on parallel and distributed architectures, as well on application of parallel and distributed computing to the solution of real-life problems. You are invited to join and post pertinent content: https://www.facebook.com/groups/251002403356/ /n/r Why Python is mostly used in big data analytics? /n/r Hi guys,i hav chosen my final year project in web analytics,i have only some basic knowledge about that,can anybody give some modules or pls refer me some websites to develop my project /n/r Extension du deadline de notre conf?rence KDDA\\'2015 jusqu\\'au 15/08/2015. Pri?re diffuser au maximum aupr?s de vos contacts nationaux et internationaux. Merci.\\nhttp://www.esi.dz/kdda/ /n/r New group devoted to issues related to establishing voluntary interoperability among heterogeneous Internet of Things (IoT) platforms. \\nIt is to be somewhat more narrow in scope / focused than \"more general\" groups dealing with IoT. You are invited to join and post pertinent content: https://www.facebook.com/groups/487893988043170/ /n/r Hi guys, have any of you worked on meta-search engine for travel, and especially for flights? Data Science Society invited Data Scientists from Skyscanner to tell us more about it tonight.\\nWe will do a live event and webinar so book your time slot tonight and check out the FB event for the streaming link. You will have the opportunity to raise all your questions at slido.com #DSS. /n/r Group for people interested in Advances in Business ICT (ABICT) approached from a multidisciplinary perspective. You are invited to join and post pertinent content: https://www.facebook.com/groups/282894838704/ /n/r Sign up to One R Tip A Day /n/r Group devoted to selected aspects of computer science and information systems (covered by the annual FedCSIS conference; organized in cooperation with units of IEEE and ACM; indexed in Web of Science; acceptance rate for regular papers ~25%). You are invited to join and post pertinent content: https://www.facebook.com/groups/367888070292/ /n/r I\\'d like to draw your attention to Crunch, a Practical Big Data conference with Alistair Croll (author of Lean Analytics), Doug Cutting (founder of Hadoop) and several international experts from all around the world, including Spotify, Pinterest and SurveyMonkey: http://www.crunchconf.com/#speakers . Crunch is Prezi.com\\'s and USTREAM\\'s continued non-profit effort to organize world-class conferences in the region (Oct 29-30, Budapest). Early Bird tickets are still on sale until 15 July, check it out! http://www.crunchconf.com/#tickets . /n/r Group devoted to sharing information concerning distributed systems, grid computing, cloud computing, scalable computing, large scale distributed computing, and related topics. You are invited to join and post pertinent content: https://www.facebook.com/groups/278357194562/ /n/r Group devoted to distributed (primarily agent-based) computer systems in which autonomous entities negotiate with one another, typically on behalf of humans, in order to come to mutually acceptable agreements. You are invited to join and post pertinent content: https://www.facebook.com/groups/122962947743348/ /n/r Today\\'s One R Tip A Day /n/r #RiseUp   to  the challenges  to be a  winner...\\nwww.adgeco.com /n/r Today\\'s One R Tip A Day /n/r Persons interested in Agent-based computing are invited to join our Facebook group: https://www.facebook.com/groups/299640474516/ /n/r Submission Deadline, July 2015 : Conferences of Computer Science & Electronics\\nFB Page : www.facebook.com/guide2research\\nPDF File : www.guide2research.com/conf/july-2015.pdf\\n\\nBy Publisher:\\n->IEEE www.guide2research.com/conferences/ieee\\n->ACM   www.guide2research.com/conferences/acm\\n->Springer www.guide2research.com/conferences/springer\\n->Elsevier www.guide2research.com/conferences/elsevier /n/r Persons interested in Cyber-Physical Systems are invited to join our Facebook group: https://www.facebook.com/groups/584689371646354/ /n/r I am using intelli j idea (like eclipse)with scala integration , i create a scala sbt project , i use spark 1.4.0 and scala 2.11.6 , I am getting error on :\\nimport org.apache.spark.{SparkContext, SparkConf}\\n\\nthe file buid.sbt contains this code :\\n\\nname := \"simple\"\\n\\nversion := \"1.0\"\\n\\nscalaVersion := \"2.11.6\"\\n\\nlibraryDependencies += \"org.apache.spark\" % \"spark-core_2.10\" % \"1.4.0\"\\n\\nwhen i use maven to build java application i have not problem just the problem is when i try to build scala application with sbt using intellij idea /n/r Need Jobs? Need to Post Your Offers? Questions About HR? \\nJoin a great hub for everyone involved in HR \\x96 Business Owners, Entrepreneurs, HR Executives, Managers, Supervisors, Officers and anyone who deals with people. We hope to learn from one another through a compendium of articles, learning ideas and nuggets of wisdom. In this regard, everyone is enjoined to participate, and we especially encourage those who have a wealth of experience to share their knowledge and be an instrument for the widening of the horizon of all the HR Professionals and Practitioners in the Philippines. /n/r Looking for a Business and Big Data Competition? Registrations Open for TEXATA 2015 World Championships >> http://www.texata.com /n/r Dear Friends! By filling in this questionnaire about the global IT education you would make an impact! Please share it with your friends!\\nhttps://ru.surveymonkey.com/s/XROOKIE2015 /n/r a movie called regression /n/r Hello,\\n\\nWhat are the key aspects of data quality on data mining ? \\n\\nAny advice or suggestion will be much appreciated .\\nThank you . /n/r SanDisk InfiniFlash Rethinking Delivering Flash at Data Center Scale [Video]\\nhttp://bit.ly/1KUBSYJ /n/r SanDisk InfiniFlash Use Cases [Video]\\nhttp://bit.ly/1HSSrxQ /n/r The Persuasiveness of a Chart Depends on the Reader, Not Just the Chart\\nhttp://bit.ly/1KMNDx8 /n/r Map Design Gone Wrong Do People Even Care Anymore?\\nhttp://tinyurl.com/njufbry /n/r SanDisk InfiniFlash Use Cases [Video]\\nhttp://tinyurl.com/oqbnvte /n/r 5 Trends in Big Data revealed\\nhttp://bit.ly/1G24P0l /n/r At the Allen & Overy Fintech event on Democratising Finance thru P2P lending #datascience #fintech #p2plending #FundingCircle #TransferWise /n/r Dadeh Kavan Company Implement your ADF project with low cost\\nPlease send Email for Oracle Projects\\nHamedoracle@gmail.com /n/r Call for Position Papers: 2015 Federated Conference on Computer Science and Information Systems (FedCSIS); deadline on Monday: June 1, 2015; https://www.facebook.com/events/1610586142548063/ /n/r Call for Position Papers   FedCSIS 2015: more info at: https://www.facebook.com/events/1610586142548063/ /n/r Call for Papers; High Performance Computing Solutions for Complex Problems; Paper submission: September 19, 2015; more info: \\nhttps://www.facebook.com/events/386079591589615/ /n/r Teri mehfil se nikle kisi ko khabar tak na hui,Tera mud mud ke dekhna hamein badnam kargaya\\x85.. /n/r Ola, o Summit ? gratuito. H? uma trilha dedicada a Big Data onde farei uma das palestras. /n/r Music video-sample from FedCSIS 2014 conference is available at:  https://www.fedcsis.org/resources/fedcsis-records/FedCSIS_2014_band_videoclip.mp4 (evenings are very social ;-) /n/r FedCSIS 2015; in cooperation ACM + IEEE; indexed in Web of Science and Scopus; 4+ days till deadline; https://www.facebook.com/events/684283518363800/ /n/r Advances in Artificial Intelligence and Applications 2015; technical cooperation ACM + IEEE; indexed in Web of Science and Scopus; submission in 9 days; https://www.facebook.com/events/962405283784396/ /n/r Do you want to learn Hadoop but confused from where to start?\\nCheck this beginner\\'s guide for Hadop.\\n\\nhttp://saphanatutorial.com/prerequisites-for-learning-hadoop/ /n/r Less than 2 weeks till submission deadline   precise down-clock on the conference www site: https://www.facebook.com/events/684283518363800/ /n/r AnalytiX DS invites all to \"Texas Modeling User Group Spring 2015 Meeting\" \\nhosted by ?#CAERwin? on 28th April in ?#Texas?. Register yourself before 24th April \\nat http://bit.ly/1PrARI9. http://analytixds.com/events/ ?#DataManagement? ?#DataGovernance? ?#DataModeling? /n/r WANT to LEARN MORE or EARN MORE?\\n\\n-\\n\\nWebsite link ===>>> https://NewEdgeMath.com /n/r ? ? WANT to LEARN MORE or EARN MORE? ? ?\\n\\n-\\n\\nWebsite link ===>>> https://NewEdgeMath.com /n/r AnalytiX DS invites all to \"Texas Modeling User Group Spring 2015 Meeting\" hosted by #CAERwin on 28th April in #Texas. Register yourself before 24th April at http://bit.ly/1PrARI9. #DataManagement #DataGovernance #DataModeling /n/r Some good speakers lined up for the last day of Hadoop Summit Europe 2015! :-D\\n#datascience #bigdata #hadoopsummit /n/r Christian, Leo and I in front of the venue for #HadoopSummit2015 (Proof that we were there) #datascience #hadoop /n/r Christian, Leo and I in front of the venue of the #HadoopSummit2015 (Proof that we were there) #datascience #hadoop /n/r ? ? Want to Learn More or Earn More? ? ?\\n\\nhttps://www.facebook.com/video.php?v=839855122763388\\n\\nWebsite link ===>>> https://NewEdgeMath.com /n/r ? ? Want to Learn More or Earn More? ? ?\\n\\nhttps://www.facebook.com/video.php?v=839855122763388\\n\\nWebsite link ===>>> https://NewEdgeMath.com /n/r trick to buy rs 634 sandisk 16 gb dual pendrive for rs 46\\nsteps to get the offer-\\ndownload the snapdeal app after that u will get a pop up message put invite code. code is given below\\nimportant step-\\nsign up with this invite code- 7TVJ105095\\nto get rs 50\\nthan sign up with your email id or facebook id.\\nafter this open the snapdeal app share section and their u will came to know how to buy this pendrive only for rs 46\\nfor proof- /n/r Calling all students, teachers, and professionals who want to be part of \\n\\nan exceptional social community $$$\\n\\nhttps://www.facebook.com/video.php?v=839855122763388 /n/r FedCSIS 2014 Proceedings are now available in Thomson Reuters Web of Science Core Collection: http://apps.webofknowledge.com/Search.do?product=WOS&SID=P2GVKkoj9Lbyzqhbefd&search_mode=GeneralSearch&prID=1651d1e1-186b-4e49-8a14-e69a2b28a10c /n/r EECSI 2015 Call for papers: Deadline 15 Apr 2015 http://iaesonline.com/eecsi /n/r KDDA\\'2015 is an annual leading International Conference on Knowledge Discovery and Data Analysis. The purpose of the conference is to bring together researchers and actors from academia, industry, and government to advance the science, engineering, and technology in Data science in general. \\n\\nhttp://www.esi.dz/kdda /n/r Hi guys,\\nPredicting outcomes with data has reached a new level in today\\'s time. Here is a #free #webinar on #Predictive modeling with #R titled \\'The Whys and Hows of Predictive modelling\\' which will cover interesting practices in the 21st Century that has changed the way we look at data and captivated the attention of #statistical programmers, #analysts and #data scientists worldwide. Want to know more?click here. http://bit.ly/1aMzwvw /n/r Hi guys,\\n\\nPredicting outcomes with data has reached a new level in today\\'s time. Here is a #free #webinar on #Predictive modeling with #R titled \\'The Whys and Hows of Predictive modelling\\' which will cover interesting practices in the 21st Century that has changed the way we look at data and captivated the attention of #statistical programmers, #analysts and #data scientists worldwide.  Want to know more?click here. http://bit.ly/1aMzwvw /n/r Can someone answer these questions  ?\\n\\n1)How can the Mapper process each word in order to produce a more refined output?  Describe at least two processes.\\n2)What could be some unintended consequences of your processing methods on input that is not a word? For example, your input files includes \"words\" such as:\\n1.21.11\\n-65\\n1,000,000 /n/r The Clouds Economy on New Delhi World Book Fair,2015\\n\\nNew Delhi World Book Fair,2015 at Pragati Maidan,New Delhi.Our Stall No.was 210 at Hall No:11.The event was scheduled from 14th Feb to 22nd Feb.\\n\\nOnly now, The Clouds Economy [Kindle Edition] for only  $6.29\\nhttp://amzn.to/1EtZTSf /n/r Hi BI Gurus,\\nI am database architect and have primarily worked with SQL Server. I\\'m now trying to hone my skills on BI (SSAS,SSIS with a bit of SSRS).  I would really appreciate if you could guide me to some online study material with some HandsOn labs. Cheers!! /n/r This is a heads-up for aspirants who specialize #Hadoop #java, a study on current job trends for #Bigdata jobs in 2015. To view the complete video click here. http://goo.gl/hL0mQQ /n/r Dear Friends,\\nI found a #free #webinar on #mapreduce #design #patterns titled \" #Tailored #Big #Data #Solutions using MapReduce Design Patterns \". If you are interested to learn more about various applications of design patterns in Big Data using MapReduce, you can also attend the webinar for free !\\nClick here to register:http://bit.ly/1w0yp5d /n/r Dear Friends,\\nI found a #free #webinar on #mapreduce #design #patterns titled \" #Tailored #Big #Data #Solutions using MapReduce Design Patterns \". If you are interested to learn more about various applications of design patterns in Big Data using MapReduce, you can also attend the webinar for free !\\nClick here to register:http://bit.ly/1w0yp5d /n/r Hi Every one kindly send me your personal no and Email Id\\'s I have 50 Hours of Hadoop Training Videos with Cloudera Certification Dumps Hurry Up its free.\\nIf You see these types of posts any where please do not give your id rather ask them questions why cnt they share it over here? Why will they waste there time sending to all the emails and what happens many people write these types of posts and get ids of all hadoop bigdata datascience enthusiastic people and later they send them promotion mails and use these ids Data base . they sell it to other companies also. Kindly read it properly I do not have anything to send. /n/r Hi Every one kindly send me your personal no and Email Id\\'s I have 50 Hours of Hadoop Training Videos with Cloudera Certification Dumps Hurry Up its free.\\nIf You see these types of posts any where please do not give your id rather ask them questions why cnt they share it over here? Why will they waste there time sending to all the emails and what happens many people write these types of posts and get ids of all hadoop bigdata datascience enthusiastic people and later they send them promotion mails and use these ids Data base . they sell it to other companies also. Kindly read it properly I do not have anything to send. /n/r Why is Cloud Computing growing so rapidly! #Tyronesystems /n/r Hadoop Ecosystem summary to keep the track of hadoop related projects,focused on FLOSS environment\\nClick on this link   >http://www.hadooptpoint.com/hadoop-ecosystem-2/ /n/r Your Data Center Nightmare could make you win Amazon Shopping vouchers! /n/r Bridges: Connecting Researchers, Data, and HPC: January 30, 2:30pm EST, by Google Hangout and in person at Pittsburgh Supercomputing Center, 300 S Craig St, Pittsburgh PA.\\n\\nJoin us for a preview of Bridges, #PSC\\'s innovative new computing resource for #BigData and #DataAnalytics which will be available at no charge to the U.S. open research community. Bridges will serve a wide variety of research and applications, providing a high degree of interactivity, gateways and tools for gateway-building, and a very flexible user environment.\\n\\nIf you can\\'t come in person, you can watch this event via the web. Please let us know if you will attend so we can plan for you: http://bit.ly/1zrgpC7 /n/r How to upload files on cloud ? Learn from Rajni Sir :) /n/r Do You Really Know #Big #Data! #Tyronesystems /n/r Dear Friends,\\nI found a #free #webinar on #DevOps titled Redefining your #IT #Strategy. It is absolutely free and anyone who wants to learn more about #DevOps Culture, #Tools, #Automation, #Integration in short about #Business #Management  can join and attend. It is not spam!\\nClick here to register: http://bit.ly/1une4ke /n/r in just 1 day of starting this we got 25 users registered. hope we can now move on to this new platform and build india\\'s own social networking platform by monetization process inside india and a  lot of employment in india. ( http://www.ucctu.com ) /n/r Follow these tips to protect your #data from #Data #Disaster! #Tyronesystems /n/r This article clears our confusion between choosing Online mode of learning and Classroom format especially for Big Data Education Space\\nRead here: http://www.bigdataeducation.in/is-online-course-good-for-me-or-classroom-format/ /n/r Hello to all members of Big Data, Data Science, Data Mining /n/r If you are looking for Job change or to build your career, Learn Cyber Security- Ethical Hacking on weekends it\\'s the right option\\nPlacement Assistance available \\n contact: +91-9840730610 / 9787401008 / 9600077954\\nwww.htcitmr.ac.in\\n YouTube: ITMR India\\n Facebook: HTC Institute of Technology Management and Research \\n Twitter: HTC ITMR\\n LinkedIn : HTC Institute of Technology Management and Research \\n write to us : training@htcitmr.ac.in /n/r Hi Folks..Greetings!!..I m analyzing Different Classification Algorithms to do predictive analysis on a dataset. The training data I have has 5 attributes, 4 of which are independent.2 of the independent attributes are continuous and 2 are discrete.I have been working to fit this data into Logistic Regression model and generate Predictions. Now on this background I have couple of questions for which I require answers  -\\n\\nQ1). Does the ratio of my negative and positive cases in the training data affects learning of my Regression Model ?\\nQ2). Should I consider co-related attributes for training the model or only independent ones. Does it affect Performance or Cost Function ?\\n\\nWill Seek Suggestions or Open Discussion. Thanks in advance !! /n/r Hello Friends plz help me,\\n  I want to perform comparision between two tables.\\n  like I have two table \"product1\" and \"product2\". both have columns \"product_name\" and    \"product_price\" so now I want to compare product_price of both tables on the behalf product_price comparision display the product_name.\\nExample:-\\nfirst product1 table\\nhbase(main):001:0> create \\'product1\\',\\'cf1\\'\\n0 row(s) in 1.5980 seconds\\nhbase(main):008:0> put \\'product1\\',\\'row1\\',\\'cf1:product_name\\', \\'shoe\\'\\n0 row(s) in 0.0240 seconds\\n\\nhbase(main):009:0> put \\'product1\\',\\'row1\\',\\'cf1:product_price\\',\\'2000\\'\\n0 row(s) in 0.0090 seconds\\n\\nhbase(main):010:0> put \\'product1\\',\\'row2\\',\\'cf1:product_name\\', \\'shoe\\'\\n0 row(s) in 0.0040 seconds\\n\\nhbase(main):011:0> put \\'product1\\',\\'row2\\',\\'cf1:product_price\\',\\'4000\\'\\n0 row(s) in 0.0040 seconds\\n\\nhbase(main):013:0> put \\'product2\\',\\'row1\\',\\'cf2:product_name\\',\\'shoe\\'\\n0 row(s) in 0.0190 seconds\\n\\nhbase(main):014:0> put \\'product2\\',\\'row1\\',\\'cf2:product_price\\',\\'2500\\'\\n0 row(s) in 0.0090 seconds\\n\\nhbase(main):015:0> put \\'product2\\',\\'row2\\',\\'cf2:product_name\\',\\'shoe\\'\\n0 row(s) in 0.0110 seconds\\n\\nhbase(main):016:0> put \\'product2\\',\\'row2\\',\\'cf2:product_price\\',\\'3500\\'\\n0 row(s) in 0.0130 seconds\\nhbase(main):012:0> scan \\'product1\\'\\nROW                                   COLUMN+CELL                                                                                               \\n row1                                 column=cf1:product_name, timestamp=1418879753601, value=shoe                                              \\n row1                                 column=cf1:product_price, timestamp=1418879810299, value=2000                                             \\n row2                                 column=cf1:product_name, timestamp=1418879834315, value=shoe                                              \\n row2                                 column=cf1:product_price, timestamp=1418879843868, value=4000                                             \\n2 row(s) in 0.0310 seconds\\n\\nSecond product2 table:\\nhbase(main):001:0> create \\'product2\\',\\'cf2\\'\\n0 row(s) in 1.5980 seconds\\nhbase(main):013:0> put \\'product2\\',\\'row1\\',\\'cf2:product_name\\',\\'shoe\\'\\n0 row(s) in 0.0190 seconds\\n\\nhbase(main):014:0> put \\'product2\\',\\'row1\\',\\'cf2:product_price\\',\\'2500\\'\\n0 row(s) in 0.0090 seconds\\n\\nhbase(main):015:0> put \\'product2\\',\\'row2\\',\\'cf2:product_name\\',\\'shoe\\'\\n0 row(s) in 0.0110 seconds\\n\\nhbase(main):016:0> put \\'product2\\',\\'row2\\',\\'cf2:product_price\\',\\'3500\\'\\n0 row(s) in 0.0130 seconds\\n\\nNow I want to comapre these two colomns fields on the behalf of price display the product_name. /n/r How to compare two columns in Hbase? How to perform comparisons in Hbase? /n/r HADOOP BIG DATA\\nQLIKVIEW\\nTABLEAU\\nMICRO STRATEGY\\nETL TESTING\\nHADOOP BIG DATA\\n\\nCLASS ROOM AND ONLINE TRAINING @ BANGALORE @ 09620684481 /n/r Filters in HBase\\nClick on this link ->http://www.hadooptpoint.com/filters-in-hbase-shell/ /n/r Hello friends, I have Installed Hbase but when I want to create table in Hbase I am getting Exception please help me how can I solve this issue.\\n\\nhbase(main):002:0> create \\'test\\',\\'cf\\'\\n\\nERROR: org.apache.hadoop.hbase.PleaseHoldException: org.apache.hadoop.hbase.PleaseHoldException: Master is initializing\\n\\nHow can I solve this????? /n/r Hello frnds I want to work on Nosql. so please guide me how to install Hbase how to configure Hbase, I tried but its not Working.\\nPlease help me how should i do this. /n/r FIRST of ITS KIND CAREER-CHANGING DIPLOMA IS NOW WITHIN YOUR REACH.\\n Professional Diploma in Business Intelligence & Analytics \\x96 University Recognized Program only on Weekends with Placement Assistance.\\n\\nTalk to our experts\\n HTC Towers, No. 41, GST Road, Guindy Chennai \\x96 600 032\\n +91-9840730610 / +91-9787401008 / +91-9600077954\\n training@htcitmr.ac.in\\nwww.htcitmr.ac.in\\n FB: HTC Institute of Technology Management & Research\\n You Tube: ITMR India \\n Twitter: HTC ITMR /n/r Apache Tez Introduction In Simple Way :-)\\n\\nWhy Apache Tez Much Faster Than Mapredue ? :-)\\n\\nSubscribe&Read @   > http://www.hadooptpoint.com/apache-tez-introduction/ /n/r Mapreduce,Hive,Pig,Sqoop,Hbase And More Real Time Interview Questions&Answers :-)\\n\\nhttp://www.hadooptpoint.com/category/interview-questions/ /n/r REPORTING/ Analytical TOOLS:\\nTABLEAU\\nQLIKVIEW\\nHADOOP BIG DATA\\nSAP HANA\\nCLASS ROOM AND ONLINE TRAINING@ BANGALORE @ 09620684481 /n/r How Facebook uses Hadoop and Hive (Interesting Article ) :-) :-)\\n\\nRead & Subscribe @    > http://www.hadooptpoint.com/facebook-uses-hadoop-hive/ /n/r Introduction to Hive In Simple Way (Exclusive) :-) (y)\\n\\nRead & Subscribe To Our Page    > http://www.hadooptpoint.com/introduction-hive/ /n/r Hadoop MapReduce Counters\\nclick on this link  >http://www.hadooptpoint.com/hadoop-mapreduce-counters/ /n/r can anyone suggest me some good research areas in data mining for my Ms thesis.I will be thankful. /n/r REPORTING / ANALYTICAL TOOLS\\nHADOOP BIG DATA\\nTABLEAU\\nQLIKVIEW\\nMICRO STRATEGY\\n\\nHADOOP BIG DATA\\nINFORMATICA\\nCLASS ROOM AND ONLINE TRAINING @ BANGALORE @ 09620684481 /n/r How does Hadoop MapReduce Job works\\nclick on this link  >http://www.hadooptpoint.com/hadoop-mapreduce-job-works/ /n/r Hadoop MapReduce Introduction & with example\\nclick on this link ->http://www.hadooptpoint.com/hadoop-mapreduce/ /n/r Hadoop Hive ORC File Format\\nclick on this link  ->http://www.hadooptpoint.com/hadoop-hive-orc-file-format/ /n/r REPORTING/ Analytical Tools\\nHADOOP BIG DATA\\nTABLEAU\\nQLIKVIEW \\n\\nSALESFORCE CRM\\nClass room and online training @ banagalore @ 09620684481 /n/r Hadoop Hive Input Format Selection\\nClick on this link    >http://www.hadooptpoint.com/hadoop-hive-input-format-selection/ /n/r Distributed Execution Engines for solving bigdata problems\\nclick on this link  >http://www.hadooptpoint.com/parallel-distributed-processing/ /n/r REPORTING/ Analytical TOOLS:\\n\\nHADOOP BIG DATA\\nQLIKVIEW\\nTABLEAU\\nTIBCO SPOTFIRE\\nHADOOP BIG DATA\\nSALESFORCE CRM\\nCLASS ROOM AND ONLINE TRAINING @\\nBANGALORE @ 09620684481 /n/r Hive Buckets Optimization Techniques:\\nclick on this link ->http://www.hadooptpoint.com/hive-buckets-optimization-techniques/ /n/r Reporting / Analytics tools:\\nQLIKVIEW\\nTABLEAU\\nTIBCO SPOTFIRE\\nHADOOP\\nSALESFORCE CRM\\nCLASS ROOM AND ONLINE TRAINING @ BANGALORE @ 09620684481. /n/r Introduction to hive partition \\nhttp://www.hadooptpoint.com/introduction-hive-partition-big-data/ /n/r Machine Learning on Bigdata\\nclick on this link ->http://www.hadooptpoint.com/big-data-machine-learing/ /n/r QLIKVIEW\\nTABLEAU\\nTIBCI SPOTFIRE\\nHADOOP\\nSALESFORCE CRM\\nCLASS ROOM AND ONLINE TRAINING @ BANGALORE @ 09620684481 /n/r Top 10 Hadoop Use Cases\\nclick on this link  >http://www.hadooptpoint.com/hadoop-use-cases/ /n/r Top 25 HBase Interview Questions for freshers and experience\\nclick on this link  ->http://www.hadooptpoint.com/hbase-interview-questions/ /n/r Preliminary Call for Papers; 6th International Workshop on Advances in Business ICT (ABICT\\'15); https://www.fedcsis.org/2015/abict /n/r Congratulations to the inaugural TEXATA 2014 Big Data Analytics World Champion - St?phane Sbizzera (KPMG France). VIVE LA FRANCE!! Second Place = Kristin Nguyen (HP Labs Singapore). Third Place = Konstantin Tretyakov (University of Tartu, Estonia). Amazing weekend in #BigDataAnalytics #Business #Austin #Texas  Sign up for 2015 www.texata.com /n/r Purchase Review of Predictive Analytics with Microsoft @Azure Machine Learning bit.ly/1yK4730 #MLatMSFT /n/r If you post many public  FB texts our research  is looking for you.\\nEnroll to this academic research, share your public posts and make up to 50$ per upload\\n(For academic research use only)\\nFor more details:\\nwww.semantic-networks.com /n/r Top 25 Hive interview questions for freshers and experience  part2\\nhttp://www.hadooptpoint.com/hive-interview-questions-part-2/ /n/r Introduction to Apache Spark...click on below link\\nhttp://www.hadooptpoint.com/introduction-apache-spark/ /n/r JdbcConnection for Hive....click on below link\\nhttp://www.hadooptpoint.com/hive-jdbc-connection/ /n/r Top 25  Hive interview questions for freshers and experience\\nhttp://www.hadooptpoint.com/hadoop-hive-interview-questions/ /n/r HADOOP (BIG DATA),\\nQLIKVIEW\\nTABLEAU\\nTIBCO SPOTFIRE\\nCLASS ROOM AND ONLINE TRAINING @ BANGALORE @ 09620684481. /n/r Top 25 Pig Interview Questions For Freshers and Experience :-)\\n\\nRead & Register @   > http://www.hadooptpoint.com/pig-interview-questions/ /n/r we can store data into HBase but retreive is difficult because most of the people know SQL queries.There is a one solution for that.The solution is Apache Phoenix\\n\\nhttp://www.hadooptpoint.com/apache-phoenix-over-hbase/ /n/r I have done webCrawling using Jsoup. Now I want a ready-mate tool for web-crawling. Please help me if u know  any best tool for this friends. /n/r Invite your friends in Texas/USA along to the Sunday Awards Ceremony from 2:00pm-5:00pm at the University of Texas at Austin on Sunday 23rd November 2014.  If you love Big Data & Analytics - it\\'s Free and a great networking and career opportunity:  http://www.texata.com/livefinals/ /n/r Top 25 Frequently Asked Interview Questions On Big Data Hadoop :-(\\nRead&Register @  > http://www.hadooptpoint.com/bigdata-hadoop-interview-questions/\\n#bigdata #hadoop #interview /n/r simple explanation and advantages of HDFS\\nhttp://www.hadooptpoint.com/hadoop-distributed-file-system/ /n/r Hadoop Interview Questions for freshers and experience\\nhttp://www.hadooptpoint.com/bigdata-hadoop-interview-questions/ /n/r *How Facebook uses Hadoop and Hive*\\n\\nhttp://www.hadooptpoint.com/facebook-uses-hadoop-hive/ /n/r Apache Phoenix is a SQL skin over HBase........,,,,\\nhttp://www.hadooptpoint.com/apache-phoenix-over-hbase/ /n/r I want to fetch all the product details. My concern is that I get the details of the loaded products(one page that is shown) only, When I scroll down the page ,  no. of product inceases . But I want to fetch all the product details without scrolling the page. /n/r BIg data hadoop Mapreduce Java Programs on Real Time Flights Data :-)\\n\\nhttp://tutorialshadoop.com/big-data-hadoop-mapreduce-java-programs/ /n/r Hi All,\\n\\nThis isn\\'t a \\'big\\' data question but I\\'m going to post up here as I think there is not a bad chance that someone here will have some good suggestions.\\n\\nI have some data on a process ... this process occurs in the low five figures of per year and originates in a dozen or so different settings. There are about 8 or so variations of the process itself (some of which even merge but I want to ignore that for now). The process also has a handful of different outcomes. We know the duration of the process for all settings and outcome types although one outcome is \\'un-outcomed\\' which has a duration from origin until \\'today\\' (rolling).\\n\\nThere are some values for how long the process \\'should\\' take but sometimes it takes longer.\\n\\n What I want is one-big-in-your-face graphic that shows all of that.\\n\\nA table won\\'t do it. A crude tree diagram won\\'t do it.\\n\\nI need a tree diagram template that allows me to do the following:\\n\\nFor each origin show branches from a tree with widths determined by the relative frequency of that source. For each sub-type of the process twigs coming off those branches also with their width determined by relative frequency of the whole. Twigs should vary in length with the average duration of that sub-type coming from that origin. When a twig is longer than the recommended time it should change from green to red. \\n\\nDoes anyone know of (preferably free) software that can do this?\\n\\nMichael Gibney /n/r Imperial Society of Innovative Engineers bring one more platform where you can develop you design and manufacturing skills and show your Iron \" ISIE-Indian Karting Race .....\\nGrab this opportunity and be part of this grand event No Virtual Round ..only 70 teams allowed.\\nDownload rule book : http://imperialsociety.in/isie_karting.pdf visit at us: www.imperialsociety.in Event Venue: Kari Motor Speed, Coimbatore dates: January, 2015 contact us for any sort of query: +91-8427417781/9041466699 /n/r Have a number of openings for folks looking to break into the IT industry. Looking for recent college graduates any major. \\nContact; Anil\\n732-791-2971- anild@cloudeeva.com\\nhttp://www.cloudeeva.com/ /n/r More Clouds ...\\n\\nCloud ?konomie\\nhttp://amzn.to/1wkCtsn\\n\\nThe Clouds Economy\\nhttp://amzn.to/1uC7Yhx\\n\\nEconom?a de las Nubes\\nhttp://amzn.to/Y2ToVc\\n\\nand FanPage\\nhttp://bit.ly/thecloudseconomy /n/r Hello every body\\n\\nI need this program \" SPSS Clementine\"\\n\\nor any other program that I can use for Data Mining\\n\\nAny one can help me, please ?! /n/r 2-Day Workshop on SEM and its Applications at University of Malaya, Kuala Lumpur (27th - 28th September, 2014). We also accept Local order / Purchase order. /n/r HTC Institute of Technology Management & Research\\n FIRST of ITS KIND CAREER-CHANGING DIPLOMA IS NOW WITHIN YOUR REACH. Professional Diploma in Business Intelligence & Analytics University Recognized Program only on Weekends!\\n Talk to our experts\\n HTC Towers, No. 41, GST Road, Guindy Chennai \\x96 600 032\\n +91-9840730610 / +91-9787401008 / +91-9600077954\\n mohammed.samiuddin@htcitmr.ac.in\\nwww.htcitmr.ac.in\\n FB: HTC Institute of Technology Management & Research /n/r How is EDUREKA online training ? Anyone who has taken this course can provide their valuable feedback please. Thank you :) /n/r STATISTICAL ANALYSIS USING SPSS\\nat University of Malaya, Kuala Lumpur\\n(13th to 14th September 2014)\\n\\nTime:\\n9:00 am \\x96 5:00 pm\\n\\nSpeaker: \\nProf. Dr. Ananda Kumar\\nUniversity of Malaya\\n\\nFee:\\nRM 300 / person\\nLocal Order/Purchase Order Accepted\\n\\nFor further information and registration please call us at Tel: +60322422387; Mobile: +60102610787; or email us at register@panoplyconsultancy.com or visit: www.panoplyconsultancy.com /n/r 1-Day Workshop on Microsoft OneNote at University of Malaya, Kuala Lumpur (23rd August, 2014). We also accept Local order / Purchase order. Thank you. /n/r anybody really working on big data projects or its just a hype???? why i am asking this because i am planning to go for hadoop and related technologies would it be worth /n/r Hi,\\n\\nI want to interest you in participating in the project of \"The Clouds Economy\", and thus in promoting knowledge and new cloud solutions among new customers. \\n\\nThe project consists of three components:\\n\\n1. \"The Clouds Economy\" is a richly illustrated knowledge publication describing the multidimensionality of cloud computing. A must read for customers of clouding.\\n2. \"The International Atlas of Cloud Services and Tools\" is an appendix attached to \"The Clouds Economy\". The purpose of the supplement is to create a global, unique discount offer of cloud computing services only for readers / users of the publication. \\n3. CloudsEconomy.com is planned, the extended online version of \"Atlas\". Web-service to be used as Cloud Laboratory allows viewing and testing the latest cloud computing solutions.\\n\\nOctober 2, 2013: First edition of \"The Clouds Economy\" in five languages (DE, EN, ES, PL, RU) in ebook form.\\n1 April 2014: The second edition of \"The Clouds Economy\". The book was published by Academic Press Chiron - Sweden in English as a paperback and ebook. Publisher plans to release the next version of the language in the form of an ebook.\\n\\nIn October 2014, we plan to publish additional special version of \"The Clouds Economy\" in the form of paperback and ebook. A special version will include an expanded version of the \"Atlas\".\\n\\nAs part of the promotion of \"The Clouds Economy\" along with \"The International Atlas of Cloud Services and Tools\" will be distributed free of charge during the following events:\\n\\nMobility    \\nDate: October 23, 2014\\nLocation: Warsaw, Poland\\n\\nCompTIA EMEA Member & Partner Conference\\nDate: November 5, 2014 - November 6, 2014\\nLocation: Queen Elizabeth II Conference Centre, London, United Kingdom \\n\\nIDC Third Platform ICT: The New Enterprise DNA\\nDate: November 27, 2014\\nLocation: Warsaw, Poland\\n\\nIf you are interested in participating in the project, please let me know. I\\'ll send you a booklet. \\n\\nIf you have an account on FB, I invite you to \\'like\\' project Page: https://www.facebook.com/thecloudseconomy\\n\\nCheers!\\n\\n  \\nMatt Mayevsky /n/r Hurry! Last Few Slots Left! Free Webinar: Data Visualization - How to unlock value in Data \\x96 30th July, 2014, 1 PM EST\\n[RSVP: https://attendee.gotowebinar.com/register/1157063724852071682]\\nHow to join?\\nRegister here to join the webinar: https://attendee.gotowebinar.com/register/1157063724852071682\\n\\nWhen? 30th July, 2014 at 1:00 PM EST.\\n\\nWhere? From the comfort of your laptops, desktops, smartphones etc.\\nConveying meaning in data quickly is the focal point of analytics. Visual analytics helps you discover new relationships in data, prompts you to ask new questions, and helps you convey what you see to others. Join us for this webinar to learn how to unlock the potential of your data using data visualizations. \\nSave the date! 30th July, 2014 (Wednesday), Time: 1 PM EST\\nFor more details and upcoming webinars, stay tuned to our webinar hub page: http://www.perceptive-analytics.com/data-visualization-designer/#webinar\\n\\nWebinar Objectives:\\n\\x95 Understand how to make sense of vast data quickly\\n\\x95 Elicit questions you did not ask before \\n\\x95 Using visualizations to discover new data relationships \\n\\x95 Learn how data visualization can help identify hidden insights in data\\n\\x95 Explore various visualizations hand-picked by experts\\n\\nAbout Speaker:  Chaitanya Sagar, CEO of Perceptive Analytics.\\n(http://in.linkedin.com/in/chaitanyasagar/)\\nChaitanya Sagar is the founder and CEO of Perceptive Analytics. He is a Chartered Accountant (equivalent to CPA) and also holds a MBA from the Indian School of Business. He has a total experience of 15 years serving 300+ clients from medium to large companies in the USA, India, Australia, Europe and Middle East. He is an expert in creating Data Visualizations and has made presentations at international conferences. \\n\\nAbout Perceptive Analytics:  \\n\\nPerceptive Analytics is a Data Analytics company, offering specialized services in Data Visualization, Dashboard Design, Marketing Analytics, Web Marketing Analytics, Spreadsheet Modeling and Application Solutions. We have the reputation of being a trusted advisor with a penchant to deliver compelling value. We help clients unlock hidden insights using our cutting edge data visualizations. The clientele we serviced include a wide range of companies from listed companies to start ups in Silicon Valley to privately owned multi-billion dollar companies. \\nRSVP here: https://attendee.gotowebinar.com/register/1157063724852071682\\n\\nContact Info:  Chaitanya Sagar, cs@perceptive-analytics.com /n/r Quinnox Off-Campus Drive for Freshers on 4th August 2014 \\nhttp://goo.gl/R4VVXx\\n\\nMeridium Inc Hirings Freshers as Software Engineer Trainee.....\\nhttp://goo.gl/D2GCj7\\n\\nWalkin Drive for Freshers on 30th-31st Jul 2014\\nhttp://goo.gl/FpZeo1\\n\\nIBM Off-Campus Drive for Freshers on 31 Jul 2014\\nhttp://goo.gl/HQK1CM\\n\\nSyntel Walkin Drive for Freshers on 31st Jul 2014\\nhttp://goo.gl/sDHlFi\\n\\nChennai Walkin Drive for Testing on 2 Aug 2014\\nhttp://goo.gl/gSoq3D\\n\\nGrapeCity Walkin for Freshers on 2nd Aug 2014\\nhttp://goo.gl/RO4drb\\n\\nWalkin Drive for Freshers on 30th Jul-1st Aug 2014\\nhttp://goo.gl/myjBZ1\\n\\nInnoBeez Walkin Drive for Freshers on 3rd Aug 2014\\nhttp://goo.gl/ldv4eU\\n\\nWalkin Drive for Freshers on 2nd Aug 2014 \\nhttp://goo.gl/KYHp8l\\n\\nAllscripts Hirings Freshers for Associate Software Engineer .......\\nhttp://goo.gl/vZm4Yf\\n\\nVery Good Opportunity for Freshers @ Amazon.\\nhttp://goo.gl/bzScNc\\n\\nAdfactors PR Private Limited Hirings Freshers as Junior Software Engineer ,........\\nhttp://goo.gl/Ch5OEJ\\n\\nStellent Soft Walkin Drive for Freshers on 30th-31st Jul 2014\\nhttp://goo.gl/C4JRnR\\n\\nWipro Walkin Drive for Freshers on 2nd-3rd Aug 2014 ......\\nhttp://goo.gl/FrVbo3 /n/r Only 5 Weeks until Round 1 \\x96 TEXATA Big Data Analytics World Championships 2014. Two Online Qualification Rounds. Live World Finals in Austin Texas. Share with any colleagues or friends that love Big Data. Use 100% Free discount code - TEXATA398646P - if you need it (normally $30 entry). Good luck and register online \\x96www.texata.com /n/r Only 5 Weeks until Round 1 \\x96 TEXATA Big Data Analytics World Championships 2014.  Two Online Qualification Rounds. Live World Finals in Austin Texas. Share with any friends or colleagues that are awesome with big data. Register online \\x96 www.texata.com /n/r Openings for 2012,2013 & 2014 B.Tech/MCA Freshers @ Web Synergies , Hyderabad for Various Skills[C#, ASP.Net, MS SQL ,JAVA, Spring, Hibernate, MS , QL/Oracle, jQuery, PHP, My SQL, JQuery, HTML 5, CSS3, Drupal, Magento, wordpress, MS SharePoint, MS SQL, MS CRM,MS SQL, MS BI (SSIS, SSRS,SSAS), MS SQL, .Net ] ...............\\nhttp://goo.gl/WF7ntj\\n\\nIGATE Walkin Drive for Freshers on 21st-25th Jul 2014 ......\\nhttp://goo.gl/NXemJB\\n\\nCMS Info Systems Walkin Drive for Freshers on 18th-19th Jul 2014\\nhttp://goo.gl/fJatC4\\n\\nPersistent Systems Walkin Drive for Java Developer on 20th Jul 2014\\nhttp://goo.gl/V01Dil\\n\\nAtum IT Walkin Drive for Freshers on 19th Jul 2014 Hyderabad, Andhra Pradesh...............\\nhttp://goo.gl/H47Hr3\\n\\nAricent Walkin Drive for Trainees on 19th Jul 2014\\nhttp://goo.gl/0BMWxY\\n\\n20 Openings for Freshers on 19th Jul 2014 in NCR @ Nacre Software Services, Hyderabad.........\\nhttp://goo.gl/pBgt8I\\n\\nNua Trans Media Walkin Drive for Freshers on 18th-19th Jul 2014 \\nhttp://goo.gl/6yiHJY /n/r 1-Day Workshop on Microsoft OneNote at University of Malaya, Kuala Lumpur (23rd August, 2014). We also accept Local order / Purchase order. Thank You. /n/r #Germany destroyed #Brazil by (7-1). Brazil may be in utter grief & pain after facing their worst-ever defeat in #Fifaworldcup in first semi-final.Team BDI Systems & Technologies Pvt Ltd  #Social #Sentiment Analyzer captures the sentiments of the people all around the world on #Twitter. Check out \\n#BravsGer #bigdata /n/r #Germany destroyed #Brazil by (7-1). Brazil may be in utter grief & pain after facing their worst-ever defeat in #Fifaworldcup in first semi-final.Team BDI Systems & Technologies Pvt Ltd  #Social #Sentiment Analyzer captures the sentiments of the people all around the world on #Twitter. Check out \\n#BravsGer #bigdata /n/r Mastering Data Analysis Using SPSS at University of Malaya, Kuala Lumpur (9th - 10th August, 2014). We also accept Local order / Purchase order. Thank you. /n/r Just following up on big data world championship - register online at http://www.texata.com /n/r I am coming..\\n\\nhttp://www.bigdatapoc.com/ /n/r HOT TECHNOLOGIES IN IT, HADOOP big data CLASS ROOM AND ONLINE TRAINING @ BANGALORE @ /n/r Registrations open for Big Data World Championships - www.texata.com /n/r Workshop on NVivo 10 at University of Malaya, Kuala Lumpur (8th June, 2014) /n/r Final Call for Position Papers; 2014 Federated Conference on Computer Science and Information Systems (FedCSIS); submission deadline: May 23, 2014; http://fedcsis.org/call_for_position_papers /n/r English Writing Workshop Designed for Scientific Manuscripts at University of Malaya, Kuala Lumpur (31st May - 1st June, 2014) /n/r 2-Day MATLAB Workshop for Journal Publication\\nUniversity of Malaya, Kuala Lumpur\\n(24th \\x96 25th May, 2014) /n/r 2- Day Workshop SPSS FOR RESEARCH (Advance Level) at University of Malaya, Kuala Lumpur (21st - 22nd June, 2014). We also accept Local order / Purchase order /n/r 2-Day Workshop on High Impact ISI Journals Writing And Publishing at University of Malaya, Kuala Lumpur (14th - 15th June, 2014) /n/r Hello everyone,\\n\\nI have been given this opportunity to have an interview with a major IT corporate for a Big Data consultant opening. However, my major is in chemistry and I have no idea about the questions I may be asked through the interview, or the skills I should be familiar with (of course I have heard about software and applications such as hadoop). Can anybody here help me out? I appreciate any sort of insight!\\n\\nCheers!!! /n/r can any one please suggest me to join good institute for big data hadoop in Hyderabad ?? /n/r JOB: Statistical Analyst : Must be proficient in SAS (MACROS) and R Language + Strong in statistics. Salary NO BAR Share cv - vivek.shrivastava@innovaccer.com /n/r Call for Position Papers; 2014 Federated Conference on Computer Science and Information Systems (FedCSIS); submission deadline: May 23, 2014; http://fedcsis.org/call_for_position_papers /n/r Scalable Computing: Practice and Experience; Vol 14, No 4 (2013); Table of Contents available at: ttp://www.scpe.org/index.php/scpe/issue/view/114 /n/r 2-Day MATLAB Workshop for Journal Publication at University of Malaya, Kuala Lumpur (24th - 25th May, 2014). We also accept Local order / Purchase order. Please like and share to your friends.Thank you. /n/r How is Bigdata  summer training at HP educational services any  idea ?? cost is 14900-/ for 90 hours training . plz do help me :) Thanks in advance :) /n/r BIG DATA = BIG BUCKS!\\nAttend the BIGDATA Counselling session at our corporate office HTC Towers \\x96 3rd floor, Guindy - Chennai from morning 10.00 AM to 7.00 P.M on all days of the week including Saturday and Sunday or Talk to our experts+91-9840730610 / 9787401008 website http://itmr.ac.in/dip_bd.html or Facebook : HTC Institute of Technology Management and Research /n/r Federated Conference on Computer Science and Information Systems (FedCSIS; http://www.fedcsis.org) submission deadline for ALL events (strict) April 23, 2014 (5 days) /n/r Get trained in Big Data and upgrade your career \\nContact: +91 - 9840730610\\nwww.itmr.ac.in /n/r Workshop on Two-Level of Structural Equation Modeling (Multilevel Modeling-Advanced SEM) using EQS at University of Malaya (17th - 18th May, 2014)\\n\\nTime:\\n9:00 am \\x96 5:00 pm\\n\\nSpeaker: \\nDr. Nasser Alareqe\\nInternational Islamic University, Malaysia\\n\\nFee:\\nRM 300 / person\\nAccepted Local Order/Purchase Order\\n\\n \\nIf you have any questions please contact Ms. Anura Azlan Shah (+60102610787 / +60322422387). /n/r Get trained in Big Data and upgrade your career \\nContact: +91 - 9840730610\\nwww.itmr.ac.in /n/r Call for Papers; Security Transparency in Cloud based Services; Paper submission: April 30, 2014; more info: \\nhttp://www.scpe.org/index.php/scpe/pages/view/Call-issue-2-2014 /n/r Call for Papers; 3rd Workshop on Scalable Computing in Distributed Systems (SCoDiS\\'14) and 8th Workshop on Large Scale Computations on Grids (LaSCoG\\'14); submission deadline: April 23, 2014; http://www.fedcsis.org/2014/scodis-lascog /n/r Registrations Open - Big Data Analytics World Championships 2014\\n\\nInvites just opened for Big Data Analytics World Championships 2014.  \\nTwo Online Rounds. LIVE Top 16 World Finals in Dallas/Austin Texas USA. Starts 23 August 2014.  Register at www.texata.com /n/r Where can i get learning material for HADOOP from basics ? /n/r What is Cloud Computing ?\\n\\nCloud technology  is everywhere, desktops, tablets, smartphones or Netbooks.\\nThe Big Data  concept entwined with Cloud technology is fuelling the technological future. /n/r Call for Papers; Performance of Business Database Applications (PBDA\\'14); submission deadline: April 11, 2014; http://www.fedcsis.org/pbda /n/r FOR JOB - \"Placement Co-Ordinators of all the Engineering Colleges and all B.Tech Freshers 2014 Batch\" through out India We are going to organize an event in mid of April for Recruitment of Interns as well as Full time Employees. Please try to Connect ASAP on LinkedIn (mr.vivekmba@gmail.com ) if willing to Participate and grab a hi speed career in Analytics Industry.\\n\\nJob Title- Data Scientist\\nOpenings- 50 + Positions\\nMinimum Salary - 5 LPA\\nIndustry- Analytics \\nLocation - Noida\\nProcess- Online Test and Personal Interview /n/r Tending to the needs and wants of your employees is crucial for company success and growth. /n/r Hi Folks! \\nWhy Do We Need Big Data?  #bigdata #cloudcomputing #CRMS6 /n/r Workshop on Two-Level of Structural Equation Modeling (Multilevel Modeling-Advanced SEM) using EQS at University of Malaya (12th - 13th April, 2014).We also accept Local order / Purchase order. Please like & share to  your friends.Thank you. /n/r Workshop on Two-Level of Structural Equation Modeling (Multilevel Modeling-Advanced SEM) using EQS at University of Malaya (12th - 13th April, 2014).We also accept Local order / Purchase order. Please like & share to  your friends.Thank you. /n/r Dear JOB SEEKERS Please connect with me on LinkedIN to Explore NEW Job Opportunities ! \\nDATA SCIENTIST LEAD / Analyst LEAD , Exp 2 to 3 years ! SALARY NO BAR , NOiDA LOCATION ! \\nDATA QUALITY MANAGER - SALARY NO BAR APPLICATION DEVELOPER - SAlaRY - NO BAR \\nUI/UX DESIGNER - SALARY UPTO 5 LPA S\\nSTATS HEAD - SALARY NO BAR \\nMARKETING MANAGER- SALARY NO BAR \\nPYTHON DEVELOPERS - FRESHER \\nDATA SCIENTIST FRESHER Many Other Openings ! HURRY UP ! /n/r 1-Day workshop on Writing Research Proposal /Thesis\\nat University of Malaya, Kuala Lumpur\\n(5th April, 2014)\\n\\nTime:\\n9:00 am \\x96 5:00 pm\\n\\nSpeaker: \\nDr. Nasser Alareqe\\nInternational Islamic University, Malaysia\\n\\nFee:\\nRM 180 / person /n/r SAP IDES Server Access | SAP Training Videos | SAP Online Training\\n\\nHANA 1.0 SP 7 | BPC 10.1 | GRC 10 | BO BI BW | CRM 7.0 | SRM | SCM | IS Oil & Gas | IS Utilities | IS Retail | FICO | ABAP | BASIS | SECURITY | ECC 6.0 EHP6 & Many More..,\\n\\nSAP IDES Server Access 24/7 server support ,Certification Materials\\n\\nPlease Contact Us : Email : erpwebaccess@gmail.com , Call : USA : +1 908 982 1555, IND :+91 9885078067 , Skype : erpwebaccess /n/r SAP IDES Server Access | SAP Training Videos | SAP Online Training\\n\\nHANA 1.0 SP 7 | BPC 10.1 | GRC 10 | BO BI BW | CRM 7.0 | SRM | SCM | IS Oil & Gas | IS Utilities | IS Retail | FICO | ABAP | BASIS | SECURITY | ECC 6.0 EHP6 & Many More..,\\n\\nSAP IDES Server Access 24/7 server support ,Certification Materials\\n\\nPlease Contact Us : Email : erpwebaccess@gmail.com , Call : USA : +1 908 982 1555, IND :+91 9885078067 , Skype : erpwebaccess /n/r Spend your Weekends in ITMR and Learn Big Data \\nContact : +91-9840730610 / 9787401008\\nmohammed.samiuddin@htcitmr.ac.in\\nwww.itmr.ac.in /n/r SMALL SLIDE CAN OPEN BIG DOOR - DIPLOMA IN BIG DATA TRAINING PROGRAM AT YOUR CONVENIENCE !\\nContact : +91-9840730610 / 9787401008 mohammed.samiuddin@htcitmr.ac.in\\nwww.htcitmr.ac.in /n/r Small Slide can open Big Door \\nLearn Professional Diploma in Big data in Weekends & Upgrade your Career!\\nContact +91 9840730610 | mohammed.samiuddin@htcitmr.ac.in\\nhttp://itmr.ac.in/dip_bd.html /n/r Actuary is a business professional who uses mathematics, statistics and financial and investment theory to study uncertain future events, analyze their financial consequences and develop programs\\nto reduce the impact of the associated risks, especially those of concern to insurance companies. /n/r BigData is in real demand today. This technology will add value for the developers, System administrators, System analyst, Marketing analyst, Business analyst, Security analyst, Project Managers & Marketing research people. /n/r What is the difference between Hadoop and Twitter storm?\\nCan we use twitter storm with Hadoop for Real time machine learning instead of Mahout? /n/r hello everybody!! =) there are someone italian? /n/r Dear Sir/Mam you can earn through facebook advertising from your home \"payment guaranteed\" \\nfor details contact 0301-4538149 /n/r Hello Every one I am MCA graduate 2012 passed out student with the knowledge of Bigdata Hadoop Technology. I am Having 6 month hand on Experience so if any find Hadoop jobs for my profile mean please let me know the details to Thiru3355@gmail.com.... /n/r join www.bloodbankcell.com as a volunteer blood donor. You can updated sms,email about who is looking for blood in locality and also healthcare related information. join www.bloodbankcell.com website. now 155 member. are you in our database /n/r join www.bloodbankcell.com as a volunteer blood donor. You can updated sms,email about who is looking for blood in locality and also healthcare related information. join www.bloodbankcell.com website. now 155 member. are you in our database /n/r I am looking for good ways of integrating R and Hadoop. I have read online reviews of different ways of integrating the two but I would love to hear some advice and comments from anyone who has had experiences doing so.  :-) /n/r I am working on a Web Data Extractor which fetches data from the unstructured pages on the web.\\nI\\'m building a \"generic\" one which can fetch data from almost any kind of web page and across multiple pages with few clicks.\\nI\\'ve been able to make the algorithm for getting data but the bigger question in front of me now is : \"What could be done with that data?\"\\n\\nI understand that it depends on the user, how he/she wants to look at the data and analyze it accordingly. But I want to provide some predefined utilities to the user, no matter whether the user fetches data from e-commerce websites or from social media ones.\\nPlease guide me on this! :) /n/r We\\'ve reached 1000 members!  :-D /n/r Hi everybody, \\n\\nI started this group slightly over a year ago in late December 2012 and it has been astonishing to see how quickly the group grew. And I take this as a sign that there is a desire for a forum like this to share ideas with other data scientists and individuals interested in big data.\\n\\nAs the group grew in size, there were gradually more and more posts which were job offers or offers for various forms of training. I decided to allow this because at the time I felt that it these might be valuable resources for members in the community. However, this trend has spiraled out of hand and it now feels like those types of messages have drowned out the fruitful exchange of ideas that we once had in the community.\\n\\nThus, as of Saturday January 11th 2014, any posts that are advertisements will be deleted and will be grounds for banishment from the group. This includes but is not limited to posts about job offers and paid trainings. \\n\\nIf you wish to share information about a free educational resource such as new course on Coursera, edX or Udacity that you plan to sign up for or have taken in the past that is perfectly ok. You can also share about books, blog posts and podcasts that you have found useful so long as it is not self-promotion. Use common sense when sharing with the community.\\n\\nWishing you all the best in your data science journey! - Henrik Nordmark /n/r Hi,\\n\\nAnybody want,\\n\\nHadoop Big-data  latest training videos \\n\\nOracle BPM  11g Training videos\\n\\nOracle SOA developer 11g Training videos\\n\\nsap-security & BI  pl/sql\\n\\nselenium -software testing\\n\\ncore java\\n \\nTraining videos\\n\\nIt contains 40 hours training session\\n\\nYou just send me a  mail to  alya9642@gmail.com\\n\\nNote: Not for Free\\n\\nIn the mail mention your Name,Place of living, and Contact Number !!\\n\\nNot for windows viewers! \\n\\nDon\\'t comment send me a mail only!\\n \\nThank you ! /n/r Hi,\\n\\nAnybody want \\n\\nOracle BPM  11g Training videos\\n\\nOracle SOA developer 11g Training videos\\n\\nOracle Weblogic Admin 11g Training videos\\n\\nOracle OSB developer Video  11g Training videos\\n\\nOracle  AIA  11g Training videos\\n\\nOracle  ODI  11g Training videos\\n\\nYou just send me a  mail to  alya9642@gmail.com\\n\\nNote: Not for Free\\n\\nIn the mail mention your Name and Place of living !!\\n\\nNot for windows viewers! \\n\\nDon\\'t comment send me a mail only!\\n\\nThank you ! /n/r Hi friends,i need one help, i have some data, the data is date wise sales report, i want to predict or forecast , the data having up and down in graph.. so which model is suitable this situation? any body help me.. i need ur helps...   /n/r Hi,\\nAnybody want,\\nHadoop Big-data  latest training videos \\nOracle BPM  11g Training videos\\nOracle SOA developer 11g Training videos\\nYou just send me a  mail to  alya9642@gmail.com\\nNote: Not for Free\\nIn the mail mention your Name,Place of living, and Contact Number !!\\nNot for windows viewers! \\nDon\\'t comment send me a mail only!\\n \\nThank you ! /n/r respected sir\\'s,and friends.i am ramana,i am very new to hadoop.i have the theoretical knowledge of hadoop,but don\\'t have any practical knowledge.i need your help,can any one give me any soft copy of commands and material of hadoop\\'s map reduce,pig,and hive.plzzz share to me,some information guysssss /n/r respected sir\\'s,and friends.i am ramana,i am very new to hadoop.i have the theoretical knowledge of hadoop,but don\\'t have any practical knowledge.i need your help,can any one give me any soft copy of commands and material of hadoop\\'s map reduce,pig,and hive.plzzz share to me,some information guysssss /n/r Hello. I am sadat from Bangladesh. Just recently I have discovered the power of big data and trying to harness its true power. However my technical skills are only limited to SQL. I am thinking to learn hadoop and map reduce. Do you think learning these two will give me any significant positive edge in my career? Will it be at all possible for me to learn all these technical things considering that I am a business graduate? And by learning I mean ms from University. Or should I go for MBA in business analytics (focused for business students). Need your expert opinion on this. Thanks. :) /n/r Celebrating the fact that as of yesterday, I have an exciting new job doing data science for a digital marketing company! :-D\\n\\nAnd as if that were not good news enough... My very good friend and colleague Vladimir, with whom I work so well with has as of today also got a job at the same company as me! \\n\\nI look forward to some very awesome times ahead! :-D :-D :-D /n/r Join Us in the MapReduce Group\\n\\nhttps://www.facebook.com/groups/mapreducegroup/ /n/r Everyone know Hadoop and Mahout can process with very big data. But how big is it? Would you send me some source contains volume of data they are processed. Thank you. /n/r Hello everyone. Would you help me to find some \"related work\" that related to Apache Mahout. Because Mahout is new, so it\\'s difficult to find some research literature related to Mahout. Thank you very much. /n/r I am looking to import a massive  JSON file into a database for analysis. Does anyone have any recommendations for database compatibility with a JSON file? I am going to be doing network analysis from the data and would probably be using graphical interfaces to present the results. Anyone got any thoughts? /n/r Do you like to write Blogs about BI, Analytics and Big Data ? Please let me know.. /n/r Many companies are saddled with data warehouses that weren\\'t designed to handle big data. http://deloitte.wsj.com/cio/2013/07/17/the-future-of-data-warehouses-in-the-age-of-big-data/?mod=wsjcio_hp_deloitte /n/r Hi Guys! thanks for the accept. I would like to apologize in advance if my question would be a bit noob .I think I have a hint of what Big Data is for ( base from random articles I found on the net)  but I\\'m completely confused on Data Science. Do you need heavy mathematical background for it? Sorry for the very noob question. I\\'ll try to read more so that I can contribute to the group :) /n/r Has anyone here read the Nate Silver book yet? If so what did you think? /n/r Would you like to buy an online BI, Analytics and Big Data courses and certification package ? cost around USD $400 total. Please help in answering to this small survey... we are working on it right now and will launch in near future.. just trying to understand that if it is really a good idea or not? /n/r White Paper - 3 Ways to Improve Customer Care Through Data Hygiene  http://goo.gl/6IXgR /n/r What would people say are some of the seminal papers in the Big Data field, both classically, presently and considering the future of the field. /n/r Thank you for letting me in on the group!\\nI have no back ground in Big Data so I am looking to chat to this group about how it all works with the knowledge I have on health and predicting health trends based on increasing and decreasing markets, individual probability and predicting health conditions and symptoms (genetic predisposition) example of my Back ground on this subject... My presentation to the Scottish Respiratory Nurse Forum (SRNF) Conference 16-17th Novenber 2012\\nPresentation title: The Buteyko Method & My Asthma\\nAlex Spence, I am going to talk about my experience with my asthma and what was taught to me 11 years ago and has kept me 99.99% asthma free. \\nCorrecting Two Myths About Breathing\\nThe importance of The Bohr Effect\\nWhy do asthma symptoms come and go\\nHow healthy are you\\nWhat does normal breathing look like\\nGenetic predisposition to Asthma\\nCan Asthma be Cured\\nDominant protective mechanism\\nBreath Increasing Factors\\nBreath Reducing Factors\\nAudience: Respiratory Nurses /n/r For the sake of being very focused with studying, I will be temporarily deactivating my Facebook account for about a month, possibly a bit longer.\\n\\nYou can direct any admin questions to Rene Romero Benavides and Lillian Pierson, who are now also admins for the group.\\n\\nTake care, Henrik. /n/r I would like to hear from anybody in the group who has experience with data science applications with SME\\'s, i.e. Small and Medium size Enterprises.\\n\\nWhat are the needs of an SME in relation to data mining information that is relevant to their company? And from what you\\'ve experienced, is there an awareness of those needs or is it the case that the SME needs to be educated about the benefits of data mining?\\n\\nI would imagine that SME\\'s in different industries would have both different needs and different levels of awareness of how those needs could be met through the power of data science techniques.\\n\\nAny input on this would be greatly appreciated!\\n\\nKind regards,\\nHenrik. /n/r How can I find the dataset, source code to implement about Collaborative Filtering and recommendation system, or another subject related big data, data mining. ? \\nThank you so much. /n/r Hi Henrik , all.Thanks for your approval on my joining request. I am really interested in data driven research and decision making. I have been trained as architect and urban planner, and currently im phding on urban studies on the topic of data driven decision making in urban management. I will be happy to learn about any question or comment you may have on my research subject, and i\\'m very much looking forward for any opportunities that may come from this interesting group. Wishing you all a nice day. /n/r Hi, is anyone out here a mobile app programer? Or at least a java programer who wants to experiment with apps. I have an interesting project I want to work on but my java dyslexia is inhibiting my progress. Would anyone like to cooperate. We can discuss incentives privately. /n/r Funny slide to break the ice for a Big Data presentation. /n/r Not sure I agree with this diagram entirely... but it does provide an interesting way of thinking about Data Science. ;-) /n/r hello ,Please tell me about different data mining tools which available. Please provide supported links and tools.\\nExample :\\nBinning method (Bin means)\\ndata smoothing\\nFinding missing value /n/r Hi! Could you suggest me some research point about Collaborative Filtering?\\nThanks. /n/r Share your Linked In profile page: /n/r Greetings, I have been working with data in a tangential way. Mostly geospatial analysis, crime analysis and, descriptive statistics of educational data. I would like to pursue a path as a data analyst. I realize that SQL skills are very important and I have studied this in the past but I really need to brush up. Are there any free online courses that teach you SQL and provide practice databases to query? Also, which job roles should I look out for if becoming a data analyst is my ultimate goal? It seems companies want a lot of experience before considering you for this role. Thanks in advance for the feedback. /n/r Very soon, harnessing the power of information won\\'t just be a matter of profitability, but of survival. Find out about Microsoft tools to help your business thrive in the age of big data. /n/r Would you let me know What is the \\'data sparsity\\' term?\\nThanks all. /n/r Check these guys out... Applying machine learning and topology to big data: http://www.wired.com/design/2013/01/data-viz-ayasdi-iris/ /n/r So we have a small group going of people interested in data science, big data and statistics.  :-)  I would love to know what you are all up to, in which contexts you have been or are interested in applying data science techniques. It would also be great to know a little bit about your backgrounds.\\n\\nI am originally a mathematician / logician by training but I have been moving more recently in a more applied direction using statistics to solve real world problems. I am currently working with Vladimir Metodiev on a project at a textile company.\\n\\nWhat interesting things have you been up to in data science?   :-) /n/r Sorry if this is a double posting or redundant in any way. Saw it and I thought I would share with group... \\n\\nBig Data Computerworld Strategy Guide\\nPerhaps you\\'ve heard that the next new thing in IT is \\'big data\\' and concluded that the hype-cycle machine is turning out another attention-getter. I\\'m not big on predicting paradigm shifts, so I won\\'t in this case. But I will say that if you\\'re an IT professional, you ignore big data at your peril. I believe this one is all it\\'s cracked up to be and more. Read more. /n/r We\\'ve just hit 80 members! Woohoo! :-) /n/r Wow, there are a lot of new members! We\\'ve gone from a handful of people when I created the group 4 days ago to over 50 members today!  :-D\\n\\nA very warm welcome to all of you who have just recently joined! I hope you find this to be a useful forum in which to share ideas, questions, techniques and in which to collaborate with others interested in Data Science. <3\\n\\nIf you were dragged in here by a friend and are not that interested in being part of this group, there are no hard feelings in you choosing to leave. \\n\\nThe goal is for this community is to thrive with engaged members that actively interact and support one another in whatever way possible independently of whether the community is large or small.\\n\\nSo far we have gone through an initial round of introductions and we have begun to compile a list of resources for Data Science. We have thread on free online courses in statistics, SQL, Hadoop, R and related topics. And we have an analogous thread on recommended books related to these topics.\\n\\nIf you haven\\'t already done so, I would encourage you to scroll to the introductions thread and just say a few words about your background, your data science interests and any interesting data driven projects you have been involved in just to get a flavor for what everybody is up to.  :-) /n/r This is a forum for Big Data, Data Science, Data Mining and Statistics. /n/r Welcome to the Big Data, Data Science, Data Mining & Statistics group! /n/r '"
      ]
     },
     "execution_count": 247,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "male_text = male_text.replace(r'\\[.*?\\]|\\(.*http.+\\)|\\(.*https.+\\)|\\<.*http.+\\>', '')\n",
    "male_text = male_text.replace(r'Rado([^\\s]+)|Skarp([^\\s]+)', '')\n",
    "male_text = male_text.replace(r'\\=[A-Z|0-9][A-Z|0-9]|\\=', '')\n",
    "#female_text = female_text.replace('\\n',' '+ line_break + ' ')\n",
    "male_text = male_text.replace('\\r','')\n",
    "male_text = male_text.replace('--',' ')\n",
    "#female_text = female_text.replace('. ',' ' )\n",
    "female_text = female_text.lower()\n",
    "male_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "male_text2 = text_to_word_sequence(male_text, lower=False, split=\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Does',\n",
       " 'Gmail',\n",
       " 'sell',\n",
       " 'information',\n",
       " 'is',\n",
       " \"one's\",\n",
       " 'private',\n",
       " 'emails',\n",
       " 'Judging',\n",
       " 'by',\n",
       " 'adverts',\n",
       " 'in',\n",
       " 'my',\n",
       " 'Facebook',\n",
       " 'news',\n",
       " 'feed',\n",
       " 'I',\n",
       " 'would',\n",
       " 'say',\n",
       " 'Yes',\n",
       " 'But',\n",
       " 'perhaps',\n",
       " 'this',\n",
       " \"isn't\",\n",
       " 'news',\n",
       " 'for',\n",
       " 'anyone',\n",
       " 'and',\n",
       " 'I',\n",
       " 'have',\n",
       " 'been',\n",
       " 'under',\n",
       " 'a',\n",
       " 'rock',\n",
       " 'for',\n",
       " 'years',\n",
       " 'Clarification',\n",
       " 'please',\n",
       " 'Is',\n",
       " 'private',\n",
       " 'email',\n",
       " 'private',\n",
       " 'in',\n",
       " 'name',\n",
       " 'only',\n",
       " 'n',\n",
       " 'r',\n",
       " 'Hold',\n",
       " 'the',\n",
       " 'applause']"
      ]
     },
     "execution_count": 249,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "male_text2[0:50]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, initialize the tokenizer to create the sequences and fit the text onto it, with nb_words=900 representing top 900 words in the text.  \n",
    "\n",
    "\n",
    "Each word will be represented by a vector of size 900, and the the row will show 1 where the row word matched the word column if its in the top 900 words.\n",
    "\n",
    "for that, we use text_to_matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "token = Tokenizer(nb_words=900,char_level=False)\n",
    "token.fit_on_texts(male_text2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text_mtx = token.texts_to_matrix(male_text2, mode='binary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(49466, 900)"
      ]
     },
     "execution_count": 252,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_mtx.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>code</th>\n",
       "      <th>word</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>17455</th>\n",
       "      <td>0</td>\n",
       "      <td>finally</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24335</th>\n",
       "      <td>0</td>\n",
       "      <td>aux</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24336</th>\n",
       "      <td>0</td>\n",
       "      <td>yeux</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24340</th>\n",
       "      <td>0</td>\n",
       "      <td>viol</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24341</th>\n",
       "      <td>0</td>\n",
       "      <td>vole</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24343</th>\n",
       "      <td>0</td>\n",
       "      <td>tue</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24345</th>\n",
       "      <td>0</td>\n",
       "      <td>cachette</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24332</th>\n",
       "      <td>0</td>\n",
       "      <td>fois</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24348</th>\n",
       "      <td>0</td>\n",
       "      <td>diable</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24350</th>\n",
       "      <td>0</td>\n",
       "      <td>vient</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24352</th>\n",
       "      <td>0</td>\n",
       "      <td>pour</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24353</th>\n",
       "      <td>0</td>\n",
       "      <td>voler</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24355</th>\n",
       "      <td>0</td>\n",
       "      <td>rober</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24357</th>\n",
       "      <td>0</td>\n",
       "      <td>gorger</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24359</th>\n",
       "      <td>0</td>\n",
       "      <td>bon</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24349</th>\n",
       "      <td>0</td>\n",
       "      <td>ne</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24330</th>\n",
       "      <td>0</td>\n",
       "      <td>Allah</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24329</th>\n",
       "      <td>0</td>\n",
       "      <td>crie</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24320</th>\n",
       "      <td>0</td>\n",
       "      <td>plein</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24290</th>\n",
       "      <td>0</td>\n",
       "      <td>seulement</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24292</th>\n",
       "      <td>0</td>\n",
       "      <td>libye</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24295</th>\n",
       "      <td>0</td>\n",
       "      <td>arabie</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24296</th>\n",
       "      <td>0</td>\n",
       "      <td>saoudite</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24298</th>\n",
       "      <td>0</td>\n",
       "      <td>liban</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24300</th>\n",
       "      <td>0</td>\n",
       "      <td>dans</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24301</th>\n",
       "      <td>0</td>\n",
       "      <td>tous</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24302</th>\n",
       "      <td>0</td>\n",
       "      <td>ces</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24304</th>\n",
       "      <td>0</td>\n",
       "      <td>arabes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24306</th>\n",
       "      <td>0</td>\n",
       "      <td>plusieurs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24308</th>\n",
       "      <td>0</td>\n",
       "      <td>hypocrites</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>686</th>\n",
       "      <td>883</td>\n",
       "      <td>done</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3446</th>\n",
       "      <td>884</td>\n",
       "      <td>PROGRESS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7812</th>\n",
       "      <td>884</td>\n",
       "      <td>Progress</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31617</th>\n",
       "      <td>884</td>\n",
       "      <td>progress</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33794</th>\n",
       "      <td>885</td>\n",
       "      <td>areas</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24084</th>\n",
       "      <td>885</td>\n",
       "      <td>Areas</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2502</th>\n",
       "      <td>886</td>\n",
       "      <td>worked</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4931</th>\n",
       "      <td>887</td>\n",
       "      <td>re</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36960</th>\n",
       "      <td>887</td>\n",
       "      <td>Re</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12541</th>\n",
       "      <td>888</td>\n",
       "      <td>Model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32806</th>\n",
       "      <td>888</td>\n",
       "      <td>model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3237</th>\n",
       "      <td>889</td>\n",
       "      <td>January</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2824</th>\n",
       "      <td>890</td>\n",
       "      <td>temperature</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30399</th>\n",
       "      <td>890</td>\n",
       "      <td>Temperature</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7643</th>\n",
       "      <td>891</td>\n",
       "      <td>recently</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8849</th>\n",
       "      <td>892</td>\n",
       "      <td>faster</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42261</th>\n",
       "      <td>892</td>\n",
       "      <td>Faster</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32555</th>\n",
       "      <td>893</td>\n",
       "      <td>Base</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8497</th>\n",
       "      <td>893</td>\n",
       "      <td>base</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44370</th>\n",
       "      <td>894</td>\n",
       "      <td>visualization</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28011</th>\n",
       "      <td>894</td>\n",
       "      <td>Visualization</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4604</th>\n",
       "      <td>895</td>\n",
       "      <td>J</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30529</th>\n",
       "      <td>895</td>\n",
       "      <td>j</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40810</th>\n",
       "      <td>896</td>\n",
       "      <td>attend</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30956</th>\n",
       "      <td>896</td>\n",
       "      <td>Attend</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30641</th>\n",
       "      <td>897</td>\n",
       "      <td>Edition</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10278</th>\n",
       "      <td>897</td>\n",
       "      <td>edition</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2029</th>\n",
       "      <td>898</td>\n",
       "      <td>journey</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30286</th>\n",
       "      <td>898</td>\n",
       "      <td>JOURNEY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6820</th>\n",
       "      <td>899</td>\n",
       "      <td>Center</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10244 rows  2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       code           word\n",
       "17455     0        finally\n",
       "24335     0            aux\n",
       "24336     0           yeux\n",
       "24340     0           viol\n",
       "24341     0           vole\n",
       "24343     0            tue\n",
       "24345     0       cachette\n",
       "24332     0           fois\n",
       "24348     0         diable\n",
       "24350     0          vient\n",
       "24352     0           pour\n",
       "24353     0          voler\n",
       "24355     0          rober\n",
       "24357     0         gorger\n",
       "24359     0            bon\n",
       "24349     0             ne\n",
       "24330     0          Allah\n",
       "24329     0           crie\n",
       "24320     0          plein\n",
       "24290     0      seulement\n",
       "24292     0          libye\n",
       "24295     0         arabie\n",
       "24296     0       saoudite\n",
       "24298     0          liban\n",
       "24300     0           dans\n",
       "24301     0           tous\n",
       "24302     0            ces\n",
       "24304     0         arabes\n",
       "24306     0      plusieurs\n",
       "24308     0     hypocrites\n",
       "...     ...            ...\n",
       "686     883           done\n",
       "3446    884       PROGRESS\n",
       "7812    884       Progress\n",
       "31617   884       progress\n",
       "33794   885          areas\n",
       "24084   885          Areas\n",
       "2502    886         worked\n",
       "4931    887             re\n",
       "36960   887             Re\n",
       "12541   888          Model\n",
       "32806   888          model\n",
       "3237    889        January\n",
       "2824    890    temperature\n",
       "30399   890    Temperature\n",
       "7643    891       recently\n",
       "8849    892         faster\n",
       "42261   892         Faster\n",
       "32555   893           Base\n",
       "8497    893           base\n",
       "44370   894  visualization\n",
       "28011   894  Visualization\n",
       "4604    895              J\n",
       "30529   895              j\n",
       "40810   896         attend\n",
       "30956   896         Attend\n",
       "30641   897        Edition\n",
       "10278   897        edition\n",
       "2029    898        journey\n",
       "30286   898        JOURNEY\n",
       "6820    899         Center\n",
       "\n",
       "[10244 rows x 2 columns]"
      ]
     },
     "execution_count": 253,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab2 = pd.DataFrame({'word':male_text2,'code':np.argmax(text_mtx,axis=1)})\n",
    "vocab2=vocab2.drop_duplicates()\n",
    "vocab2.sort_values(by=\"code\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Shift to predict the next word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((49465, 900), (49465, 900))"
      ]
     },
     "execution_count": 254,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ = text_mtx[:-1]\n",
    "output_ = text_mtx[1:]\n",
    "\n",
    "input_.shape, output_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model2 = Sequential()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we create a sequential model format, which is a linear stack of neural network layers. This is one of the formats we learned in class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model2.add(Embedding(input_dim=input_.shape[1],output_dim= 42, input_length=input_.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model2.add(Flatten())\n",
    "model2.add(Dense(output_.shape[1], activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model2.compile(loss='categorical_crossentropy', optimizer='rmsprop',metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 39572 samples, validate on 9893 samples\n",
      "Epoch 1/10\n",
      "39572/39572 [==============================] - 279s - loss: 3.9986 - acc: 0.0358 - val_loss: 4.8548 - val_acc: 0.0203\n",
      "Epoch 2/10\n",
      "39572/39572 [==============================] - 279s - loss: 3.7018 - acc: 0.0722 - val_loss: 4.5468 - val_acc: 0.0955\n",
      "Epoch 3/10\n",
      "39572/39572 [==============================] - 280s - loss: 3.4123 - acc: 0.1086 - val_loss: 4.5491 - val_acc: 0.0997\n",
      "Epoch 4/10\n",
      "39572/39572 [==============================] - 279s - loss: 3.2313 - acc: 0.1213 - val_loss: 4.4779 - val_acc: 0.1088\n",
      "Epoch 5/10\n",
      "39572/39572 [==============================] - 276s - loss: 3.1008 - acc: 0.1312 - val_loss: 4.6289 - val_acc: 0.1132\n",
      "Epoch 6/10\n",
      "39572/39572 [==============================] - 276s - loss: 3.0136 - acc: 0.1368 - val_loss: 4.5247 - val_acc: 0.1148\n",
      "Epoch 7/10\n",
      "39572/39572 [==============================] - 279s - loss: 2.9499 - acc: 0.1387 - val_loss: 4.9670 - val_acc: 0.1178\n",
      "Epoch 8/10\n",
      "39572/39572 [==============================] - 283s - loss: 2.9142 - acc: 0.1403 - val_loss: 4.4517 - val_acc: 0.1163\n",
      "Epoch 9/10\n",
      "39572/39572 [==============================] - 280s - loss: 2.8848 - acc: 0.1426 - val_loss: 4.9576 - val_acc: 0.1157\n",
      "Epoch 10/10\n",
      "39572/39572 [==============================] - 278s - loss: 2.8624 - acc: 0.1423 - val_loss: 4.4496 - val_acc: 0.1079\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x6d0a9198>"
      ]
     },
     "execution_count": 263,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2.fit(input_, y=output_, batch_size=200, nb_epoch=10, verbose=1, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "score = model2.evaluate(input_,output_, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3.1289339194437087, 0.13736985747136735]"
      ]
     },
     "execution_count": 266,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We received an accuracy of 13%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n",
      "1/1 [==============================] - 0s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['institutions in the same techniques Using data science and the same techniques Using data science and the same techniques Using data science and the same techniques Using data science and the same techniques Using data science and the same techniques Using',\n",
       " 'Lands in the same techniques Using data science and the same techniques Using data science and the same techniques Using data science and the same techniques Using data science and the same techniques Using data science and the same techniques Using',\n",
       " 'Transgender in the same techniques Using data science and the same techniques Using data science and the same techniques Using data science and the same techniques Using data science and the same techniques Using data science and the same techniques Using',\n",
       " 'blueberries in the same techniques Using data science and the same techniques Using data science and the same techniques Using data science and the same techniques Using data science and the same techniques Using data science and the same techniques Using',\n",
       " '1846529728965286 in the same techniques Using data science and the same techniques Using data science and the same techniques Using data science and the same techniques Using data science and the same techniques Using data science and the same techniques Using',\n",
       " 'greenish in the same techniques Using data science and the same techniques Using data science and the same techniques Using data science and the same techniques Using data science and the same techniques Using data science and the same techniques Using',\n",
       " 'Future of the same techniques Using data science and the same techniques Using data science and the same techniques Using data science and the same techniques Using data science and the same techniques Using data science and the same techniques Using',\n",
       " 'perhaps in the same techniques Using data science and the same techniques Using data science and the same techniques Using data science and the same techniques Using data science and the same techniques Using data science and the same techniques Using',\n",
       " 'techno in the same techniques Using data science and the same techniques Using data science and the same techniques Using data science and the same techniques Using data science and the same techniques Using data science and the same techniques Using',\n",
       " 'slots in the same techniques Using data science and the same techniques Using data science and the same techniques Using data science and the same techniques Using data science and the same techniques Using data science and the same techniques Using',\n",
       " 'Self call for the same techniques Using data science and the same techniques Using data science and the same techniques Using data science and the same techniques Using data science and the same techniques Using data science and the same techniques',\n",
       " 'awakening in the same techniques Using data science and the same techniques Using data science and the same techniques Using data science and the same techniques Using data science and the same techniques Using data science and the same techniques Using',\n",
       " 'Idiot in the same techniques Using data science and the same techniques Using data science and the same techniques Using data science and the same techniques Using data science and the same techniques Using data science and the same techniques Using',\n",
       " 'ecda2015 in the same techniques Using data science and the same techniques Using data science and the same techniques Using data science and the same techniques Using data science and the same techniques Using data science and the same techniques Using',\n",
       " 'western in the same techniques Using data science and the same techniques Using data science and the same techniques Using data science and the same techniques Using data science and the same techniques Using data science and the same techniques Using',\n",
       " 'Stars in the same techniques Using data science and the same techniques Using data science and the same techniques Using data science and the same techniques Using data science and the same techniques Using data science and the same techniques Using',\n",
       " 'solve in the same techniques Using data science and the same techniques Using data science and the same techniques Using data science and the same techniques Using data science and the same techniques Using data science and the same techniques Using',\n",
       " 'objectives in the same techniques Using data science and the same techniques Using data science and the same techniques Using data science and the same techniques Using data science and the same techniques Using data science and the same techniques Using',\n",
       " '1651d1e1 in the same techniques Using data science and the same techniques Using data science and the same techniques Using data science and the same techniques Using data science and the same techniques Using data science and the same techniques Using',\n",
       " 'Anchorage in the same techniques Using data science and the same techniques Using data science and the same techniques Using data science and the same techniques Using data science and the same techniques Using data science and the same techniques Using',\n",
       " 'vilifying in the same techniques Using data science and the same techniques Using data science and the same techniques Using data science and the same techniques Using data science and the same techniques Using data science and the same techniques Using',\n",
       " 'bell in the same techniques Using data science and the same techniques Using data science and the same techniques Using data science and the same techniques Using data science and the same techniques Using data science and the same techniques Using',\n",
       " 'Sqoop in the same techniques Using data science and the same techniques Using data science and the same techniques Using data science and the same techniques Using data science and the same techniques Using data science and the same techniques Using',\n",
       " 'Bookings in the same techniques Using data science and the same techniques Using data science and the same techniques Using data science and the same techniques Using data science and the same techniques Using data science and the same techniques Using',\n",
       " 'ila in the same techniques Using data science and the same techniques Using data science and the same techniques Using data science and the same techniques Using data science and the same techniques Using data science and the same techniques Using',\n",
       " 'mitigate in the same techniques Using data science and the same techniques Using data science and the same techniques Using data science and the same techniques Using data science and the same techniques Using data science and the same techniques Using',\n",
       " \"'overlords' in the same techniques Using data science and the same techniques Using data science and the same techniques Using data science and the same techniques Using data science and the same techniques Using data science and the same techniques Using\",\n",
       " 'Perlman in the same techniques Using data science and the same techniques Using data science and the same techniques Using data science and the same techniques Using data science and the same techniques Using data science and the same techniques Using',\n",
       " 'Bionformatics in the same techniques Using data science and the same techniques Using data science and the same techniques Using data science and the same techniques Using data science and the same techniques Using data science and the same techniques Using',\n",
       " 'Solely in the same techniques Using data science and the same techniques Using data science and the same techniques Using data science and the same techniques Using data science and the same techniques Using data science and the same techniques Using',\n",
       " 'equal in the same techniques Using data science and the same techniques Using data science and the same techniques Using data science and the same techniques Using data science and the same techniques Using data science and the same techniques Using',\n",
       " 'Etee in the same techniques Using data science and the same techniques Using data science and the same techniques Using data science and the same techniques Using data science and the same techniques Using data science and the same techniques Using',\n",
       " 'Understands in the same techniques Using data science and the same techniques Using data science and the same techniques Using data science and the same techniques Using data science and the same techniques Using data science and the same techniques Using',\n",
       " 'friends colleagues and the same techniques Using data science and the same techniques Using data science and the same techniques Using data science and the same techniques Using data science and the same techniques Using data science and the same techniques',\n",
       " '1wkCtsn in the same techniques Using data science and the same techniques Using data science and the same techniques Using data science and the same techniques Using data science and the same techniques Using data science and the same techniques Using',\n",
       " 'Mapper in the same techniques Using data science and the same techniques Using data science and the same techniques Using data science and the same techniques Using data science and the same techniques Using data science and the same techniques Using',\n",
       " 'settlements in the same techniques Using data science and the same techniques Using data science and the same techniques Using data science and the same techniques Using data science and the same techniques Using data science and the same techniques Using',\n",
       " 'heterogeneous in the same techniques Using data science and the same techniques Using data science and the same techniques Using data science and the same techniques Using data science and the same techniques Using data science and the same techniques Using',\n",
       " 'premier in the same techniques Using data science and the same techniques Using data science and the same techniques Using data science and the same techniques Using data science and the same techniques Using data science and the same techniques Using',\n",
       " 'discussing in the same techniques Using data science and the same techniques Using data science and the same techniques Using data science and the same techniques Using data science and the same techniques Using data science and the same techniques Using',\n",
       " 'if you are invited to the same techniques Using data science and the same techniques Using data science and the same techniques Using data science and the same techniques Using data science and the same techniques Using data science and the',\n",
       " 'Bible in the same techniques Using data science and the same techniques Using data science and the same techniques Using data science and the same techniques Using data science and the same techniques Using data science and the same techniques Using',\n",
       " 'Stevenson in the same techniques Using data science and the same techniques Using data science and the same techniques Using data science and the same techniques Using data science and the same techniques Using data science and the same techniques Using',\n",
       " '2Db1Rdz in the same techniques Using data science and the same techniques Using data science and the same techniques Using data science and the same techniques Using data science and the same techniques Using data science and the same techniques Using',\n",
       " 'before and the same techniques Using data science and the same techniques Using data science and the same techniques Using data science and the same techniques Using data science and the same techniques Using data science and the same techniques Using',\n",
       " 'Calls in the same techniques Using data science and the same techniques Using data science and the same techniques Using data science and the same techniques Using data science and the same techniques Using data science and the same techniques Using',\n",
       " 'IEEE in the same techniques Using data science and the same techniques Using data science and the same techniques Using data science and the same techniques Using data science and the same techniques Using data science and the same techniques Using',\n",
       " 'photographer in the same techniques Using data science and the same techniques Using data science and the same techniques Using data science and the same techniques Using data science and the same techniques Using data science and the same techniques Using',\n",
       " 'especially to the same techniques Using data science and the same techniques Using data science and the same techniques Using data science and the same techniques Using data science and the same techniques Using data science and the same techniques Using',\n",
       " 'berry in the same techniques Using data science and the same techniques Using data science and the same techniques Using data science and the same techniques Using data science and the same techniques Using data science and the same techniques Using',\n",
       " '1863 in the same techniques Using data science and the same techniques Using data science and the same techniques Using data science and the same techniques Using data science and the same techniques Using data science and the same techniques Using',\n",
       " 'SmartCities in the same techniques Using data science and the same techniques Using data science and the same techniques Using data science and the same techniques Using data science and the same techniques Using data science and the same techniques Using',\n",
       " \"'product1' in the same techniques Using data science and the same techniques Using data science and the same techniques Using data science and the same techniques Using data science and the same techniques Using data science and the same techniques Using\",\n",
       " 'Interested in the same techniques Using data science and the same techniques Using data science and the same techniques Using data science and the same techniques Using data science and the same techniques Using data science and the same techniques Using',\n",
       " 'Heavy in the same techniques Using data science and the same techniques Using data science and the same techniques Using data science and the same techniques Using data science and the same techniques Using data science and the same techniques Using',\n",
       " 'NwncY0 in the same techniques Using data science and the same techniques Using data science and the same techniques Using data science and the same techniques Using data science and the same techniques Using data science and the same techniques Using',\n",
       " 'factor in the same techniques Using data science and the same techniques Using data science and the same techniques Using data science and the same techniques Using data science and the same techniques Using data science and the same techniques Using',\n",
       " 'dedicated in the same techniques Using data science and the same techniques Using data science and the same techniques Using data science and the same techniques Using data science and the same techniques Using data science and the same techniques Using',\n",
       " 'vZm4Yf in the same techniques Using data science and the same techniques Using data science and the same techniques Using data science and the same techniques Using data science and the same techniques Using data science and the same techniques Using',\n",
       " '1073741831 in the same techniques Using data science and the same techniques Using data science and the same techniques Using data science and the same techniques Using data science and the same techniques Using data science and the same techniques Using',\n",
       " '15th in the same techniques Using data science and the same techniques Using data science and the same techniques Using data science and the same techniques Using data science and the same techniques Using data science and the same techniques Using',\n",
       " 'gives in the same techniques Using data science and the same techniques Using data science and the same techniques Using data science and the same techniques Using data science and the same techniques Using data science and the same techniques Using',\n",
       " '831928046986099 in the same techniques Using data science and the same techniques Using data science and the same techniques Using data science and the same techniques Using data science and the same techniques Using data science and the same techniques Using',\n",
       " 'Rhode in the same techniques Using data science and the same techniques Using data science and the same techniques Using data science and the same techniques Using data science and the same techniques Using data science and the same techniques Using',\n",
       " 'MEDELLIN in the same techniques Using data science and the same techniques Using data science and the same techniques Using data science and the same techniques Using data science and the same techniques Using data science and the same techniques Using',\n",
       " 'Depressive in the same techniques Using data science and the same techniques Using data science and the same techniques Using data science and the same techniques Using data science and the same techniques Using data science and the same techniques Using',\n",
       " 'Interface in the same techniques Using data science and the same techniques Using data science and the same techniques Using data science and the same techniques Using data science and the same techniques Using data science and the same techniques Using',\n",
       " 'bored in the same techniques Using data science and the same techniques Using data science and the same techniques Using data science and the same techniques Using data science and the same techniques Using data science and the same techniques Using',\n",
       " 'Patrick in the same techniques Using data science and the same techniques Using data science and the same techniques Using data science and the same techniques Using data science and the same techniques Using data science and the same techniques Using',\n",
       " 'Delhi in the same techniques Using data science and the same techniques Using data science and the same techniques Using data science and the same techniques Using data science and the same techniques Using data science and the same techniques Using',\n",
       " 'Decision making money out of the same techniques Using data science and the same techniques Using data science and the same techniques Using data science and the same techniques Using data science and the same techniques Using data science and the',\n",
       " 'located in the same techniques Using data science and the same techniques Using data science and the same techniques Using data science and the same techniques Using data science and the same techniques Using data science and the same techniques Using',\n",
       " 'speaking in the same techniques Using data science and the same techniques Using data science and the same techniques Using data science and the same techniques Using data science and the same techniques Using data science and the same techniques Using',\n",
       " 'aux in the same techniques Using data science and the same techniques Using data science and the same techniques Using data science and the same techniques Using data science and the same techniques Using data science and the same techniques Using',\n",
       " 'King in the same techniques Using data science and the same techniques Using data science and the same techniques Using data science and the same techniques Using data science and the same techniques Using data science and the same techniques Using',\n",
       " 'petition in the same techniques Using data science and the same techniques Using data science and the same techniques Using data science and the same techniques Using data science and the same techniques Using data science and the same techniques Using',\n",
       " 'YOU are invited to the same techniques Using data science and the same techniques Using data science and the same techniques Using data science and the same techniques Using data science and the same techniques Using data science and the same',\n",
       " 'Chile in the same techniques Using data science and the same techniques Using data science and the same techniques Using data science and the same techniques Using data science and the same techniques Using data science and the same techniques Using',\n",
       " \"'big' in the same techniques Using data science and the same techniques Using data science and the same techniques Using data science and the same techniques Using data science and the same techniques Using data science and the same techniques Using\",\n",
       " 'dishonest in the same techniques Using data science and the same techniques Using data science and the same techniques Using data science and the same techniques Using data science and the same techniques Using data science and the same techniques Using',\n",
       " 'onde in the same techniques Using data science and the same techniques Using data science and the same techniques Using data science and the same techniques Using data science and the same techniques Using data science and the same techniques Using',\n",
       " 'City in the same techniques Using data science and the same techniques Using data science and the same techniques Using data science and the same techniques Using data science and the same techniques Using data science and the same techniques Using',\n",
       " 'tasks in the same techniques Using data science and the same techniques Using data science and the same techniques Using data science and the same techniques Using data science and the same techniques Using data science and the same techniques Using',\n",
       " 'Hurry in the same techniques Using data science and the same techniques Using data science and the same techniques Using data science and the same techniques Using data science and the same techniques Using data science and the same techniques Using',\n",
       " 'havens in the same techniques Using data science and the same techniques Using data science and the same techniques Using data science and the same techniques Using data science and the same techniques Using data science and the same techniques Using',\n",
       " 'approve in the same techniques Using data science and the same techniques Using data science and the same techniques Using data science and the same techniques Using data science and the same techniques Using data science and the same techniques Using',\n",
       " 'aspiring in the same techniques Using data science and the same techniques Using data science and the same techniques Using data science and the same techniques Using data science and the same techniques Using data science and the same techniques Using',\n",
       " 'SAS in the same techniques Using data science and the same techniques Using data science and the same techniques Using data science and the same techniques Using data science and the same techniques Using data science and the same techniques Using',\n",
       " 'Astrophysics in the same techniques Using data science and the same techniques Using data science and the same techniques Using data science and the same techniques Using data science and the same techniques Using data science and the same techniques Using',\n",
       " 'hair in the same techniques Using data science and the same techniques Using data science and the same techniques Using data science and the same techniques Using data science and the same techniques Using data science and the same techniques Using',\n",
       " 'MDPISciforum in the same techniques Using data science and the same techniques Using data science and the same techniques Using data science and the same techniques Using data science and the same techniques Using data science and the same techniques Using',\n",
       " 'hoje in the same techniques Using data science and the same techniques Using data science and the same techniques Using data science and the same techniques Using data science and the same techniques Using data science and the same techniques Using',\n",
       " 'ABICT in the same techniques Using data science and the same techniques Using data science and the same techniques Using data science and the same techniques Using data science and the same techniques Using data science and the same techniques Using',\n",
       " 'AuroraAddicts in the same techniques Using data science and the same techniques Using data science and the same techniques Using data science and the same techniques Using data science and the same techniques Using data science and the same techniques Using',\n",
       " 'Dish in the same techniques Using data science and the same techniques Using data science and the same techniques Using data science and the same techniques Using data science and the same techniques Using data science and the same techniques Using',\n",
       " 'storage in the same techniques Using data science and the same techniques Using data science and the same techniques Using data science and the same techniques Using data science and the same techniques Using data science and the same techniques Using',\n",
       " 'rocketing in the same techniques Using data science and the same techniques Using data science and the same techniques Using data science and the same techniques Using data science and the same techniques Using data science and the same techniques Using',\n",
       " 'V Look at the same techniques Using data science and the same techniques Using data science and the same techniques Using data science and the same techniques Using data science and the same techniques Using data science and the same techniques',\n",
       " 'proton in the same techniques Using data science and the same techniques Using data science and the same techniques Using data science and the same techniques Using data science and the same techniques Using data science and the same techniques Using',\n",
       " 'guard in the same techniques Using data science and the same techniques Using data science and the same techniques Using data science and the same techniques Using data science and the same techniques Using data science and the same techniques Using',\n",
       " 'greedy in the same techniques Using data science and the same techniques Using data science and the same techniques Using data science and the same techniques Using data science and the same techniques Using data science and the same techniques Using',\n",
       " 'Israeli in the same techniques Using data science and the same techniques Using data science and the same techniques Using data science and the same techniques Using data science and the same techniques Using data science and the same techniques Using',\n",
       " 'globe in the same techniques Using data science and the same techniques Using data science and the same techniques Using data science and the same techniques Using data science and the same techniques Using data science and the same techniques Using',\n",
       " 'choices in the same techniques Using data science and the same techniques Using data science and the same techniques Using data science and the same techniques Using data science and the same techniques Using data science and the same techniques Using',\n",
       " 'lawsuits in the same techniques Using data science and the same techniques Using data science and the same techniques Using data science and the same techniques Using data science and the same techniques Using data science and the same techniques Using',\n",
       " 'enhance in the same techniques Using data science and the same techniques Using data science and the same techniques Using data science and the same techniques Using data science and the same techniques Using data science and the same techniques Using',\n",
       " 'underachiever in the same techniques Using data science and the same techniques Using data science and the same techniques Using data science and the same techniques Using data science and the same techniques Using data science and the same techniques Using',\n",
       " 'notion in the same techniques Using data science and the same techniques Using data science and the same techniques Using data science and the same techniques Using data science and the same techniques Using data science and the same techniques Using',\n",
       " 'irrational in the same techniques Using data science and the same techniques Using data science and the same techniques Using data science and the same techniques Using data science and the same techniques Using data science and the same techniques Using',\n",
       " 'Ripping in the same techniques Using data science and the same techniques Using data science and the same techniques Using data science and the same techniques Using data science and the same techniques Using data science and the same techniques Using',\n",
       " 'exactly in the same techniques Using data science and the same techniques Using data science and the same techniques Using data science and the same techniques Using data science and the same techniques Using data science and the same techniques Using',\n",
       " 'CSS3 in the same techniques Using data science and the same techniques Using data science and the same techniques Using data science and the same techniques Using data science and the same techniques Using data science and the same techniques Using',\n",
       " 'Unlock in the same techniques Using data science and the same techniques Using data science and the same techniques Using data science and the same techniques Using data science and the same techniques Using data science and the same techniques Using',\n",
       " 'pitfalls in the same techniques Using data science and the same techniques Using data science and the same techniques Using data science and the same techniques Using data science and the same techniques Using data science and the same techniques Using',\n",
       " 'instrucciones in the same techniques Using data science and the same techniques Using data science and the same techniques Using data science and the same techniques Using data science and the same techniques Using data science and the same techniques Using',\n",
       " 'ACLU in the same techniques Using data science and the same techniques Using data science and the same techniques Using data science and the same techniques Using data science and the same techniques Using data science and the same techniques Using',\n",
       " 'exogenous in the same techniques Using data science and the same techniques Using data science and the same techniques Using data science and the same techniques Using data science and the same techniques Using data science and the same techniques Using',\n",
       " 'Mere in the same techniques Using data science and the same techniques Using data science and the same techniques Using data science and the same techniques Using data science and the same techniques Using data science and the same techniques Using',\n",
       " 'affects in the same techniques Using data science and the same techniques Using data science and the same techniques Using data science and the same techniques Using data science and the same techniques Using data science and the same techniques Using',\n",
       " 'wave in the same techniques Using data science and the same techniques Using data science and the same techniques Using data science and the same techniques Using data science and the same techniques Using data science and the same techniques Using',\n",
       " 'Down the same techniques Using data science and the same techniques Using data science and the same techniques Using data science and the same techniques Using data science and the same techniques Using data science and the same techniques Using data',\n",
       " 'Funny in the same techniques Using data science and the same techniques Using data science and the same techniques Using data science and the same techniques Using data science and the same techniques Using data science and the same techniques Using',\n",
       " 'USA photo credit in the same techniques Using data science and the same techniques Using data science and the same techniques Using data science and the same techniques Using data science and the same techniques Using data science and the same',\n",
       " 'bear in the same techniques Using data science and the same techniques Using data science and the same techniques Using data science and the same techniques Using data science and the same techniques Using data science and the same techniques Using',\n",
       " '9600077954 in the same techniques Using data science and the same techniques Using data science and the same techniques Using data science and the same techniques Using data science and the same techniques Using data science and the same techniques Using',\n",
       " 'automation in the same techniques Using data science and the same techniques Using data science and the same techniques Using data science and the same techniques Using data science and the same techniques Using data science and the same techniques Using',\n",
       " 'galactic in the same techniques Using data science and the same techniques Using data science and the same techniques Using data science and the same techniques Using data science and the same techniques Using data science and the same techniques Using',\n",
       " '584689371646354 in the same techniques Using data science and the same techniques Using data science and the same techniques Using data science and the same techniques Using data science and the same techniques Using data science and the same techniques Using',\n",
       " 'attracted in the same techniques Using data science and the same techniques Using data science and the same techniques Using data science and the same techniques Using data science and the same techniques Using data science and the same techniques Using',\n",
       " 'form of the same techniques Using data science and the same techniques Using data science and the same techniques Using data science and the same techniques Using data science and the same techniques Using data science and the same techniques Using',\n",
       " 'Spooky in the same techniques Using data science and the same techniques Using data science and the same techniques Using data science and the same techniques Using data science and the same techniques Using data science and the same techniques Using',\n",
       " 'Nicky in the same techniques Using data science and the same techniques Using data science and the same techniques Using data science and the same techniques Using data science and the same techniques Using data science and the same techniques Using',\n",
       " 'underachiever in the same techniques Using data science and the same techniques Using data science and the same techniques Using data science and the same techniques Using data science and the same techniques Using data science and the same techniques Using',\n",
       " \"conditions' in the same techniques Using data science and the same techniques Using data science and the same techniques Using data science and the same techniques Using data science and the same techniques Using data science and the same techniques Using\",\n",
       " 'pleased in the same techniques Using data science and the same techniques Using data science and the same techniques Using data science and the same techniques Using data science and the same techniques Using data science and the same techniques Using',\n",
       " 'capital in the same techniques Using data science and the same techniques Using data science and the same techniques Using data science and the same techniques Using data science and the same techniques Using data science and the same techniques Using',\n",
       " 'Damn in the same techniques Using data science and the same techniques Using data science and the same techniques Using data science and the same techniques Using data science and the same techniques Using data science and the same techniques Using',\n",
       " 'ciencias in the same techniques Using data science and the same techniques Using data science and the same techniques Using data science and the same techniques Using data science and the same techniques Using data science and the same techniques Using',\n",
       " 'Immigration in the same techniques Using data science and the same techniques Using data science and the same techniques Using data science and the same techniques Using data science and the same techniques Using data science and the same techniques Using',\n",
       " '1251805194890487 in the same techniques Using data science and the same techniques Using data science and the same techniques Using data science and the same techniques Using data science and the same techniques Using data science and the same techniques Using',\n",
       " 'evolve in the same techniques Using data science and the same techniques Using data science and the same techniques Using data science and the same techniques Using data science and the same techniques Using data science and the same techniques Using',\n",
       " 'CAN be able to the same techniques Using data science and the same techniques Using data science and the same techniques Using data science and the same techniques Using data science and the same techniques Using data science and the same',\n",
       " 'Build a Little more than the same techniques Using data science and the same techniques Using data science and the same techniques Using data science and the same techniques Using data science and the same techniques Using data science and the',\n",
       " 'obliterated in the same techniques Using data science and the same techniques Using data science and the same techniques Using data science and the same techniques Using data science and the same techniques Using data science and the same techniques Using',\n",
       " 'Saumyadeb in the same techniques Using data science and the same techniques Using data science and the same techniques Using data science and the same techniques Using data science and the same techniques Using data science and the same techniques Using',\n",
       " 'Dallas in the same techniques Using data science and the same techniques Using data science and the same techniques Using data science and the same techniques Using data science and the same techniques Using data science and the same techniques Using',\n",
       " 'actively in the same techniques Using data science and the same techniques Using data science and the same techniques Using data science and the same techniques Using data science and the same techniques Using data science and the same techniques Using',\n",
       " 'Graham in the same techniques Using data science and the same techniques Using data science and the same techniques Using data science and the same techniques Using data science and the same techniques Using data science and the same techniques Using',\n",
       " 'vice in the same techniques Using data science and the same techniques Using data science and the same techniques Using data science and the same techniques Using data science and the same techniques Using data science and the same techniques Using',\n",
       " 'wsws in the same techniques Using data science and the same techniques Using data science and the same techniques Using data science and the same techniques Using data science and the same techniques Using data science and the same techniques Using',\n",
       " 'physician in the same techniques Using data science and the same techniques Using data science and the same techniques Using data science and the same techniques Using data science and the same techniques Using data science and the same techniques Using',\n",
       " 'w5 in the same techniques Using data science and the same techniques Using data science and the same techniques Using data science and the same techniques Using data science and the same techniques Using data science and the same techniques Using',\n",
       " 'richness in the same techniques Using data science and the same techniques Using data science and the same techniques Using data science and the same techniques Using data science and the same techniques Using data science and the same techniques Using',\n",
       " 'Physics the same techniques Using data science and the same techniques Using data science and the same techniques Using data science and the same techniques Using data science and the same techniques Using data science and the same techniques Using data',\n",
       " 'unification in the same techniques Using data science and the same techniques Using data science and the same techniques Using data science and the same techniques Using data science and the same techniques Using data science and the same techniques Using',\n",
       " 'look at the same techniques Using data science and the same techniques Using data science and the same techniques Using data science and the same techniques Using data science and the same techniques Using data science and the same techniques Using',\n",
       " 'Called a Little more than the same techniques Using data science and the same techniques Using data science and the same techniques Using data science and the same techniques Using data science and the same techniques Using data science and the',\n",
       " 'London in the same techniques Using data science and the same techniques Using data science and the same techniques Using data science and the same techniques Using data science and the same techniques Using data science and the same techniques Using',\n",
       " \"Kautokeino's in the same techniques Using data science and the same techniques Using data science and the same techniques Using data science and the same techniques Using data science and the same techniques Using data science and the same techniques Using\",\n",
       " 'Government n r Hi all the same techniques Using data science and the same techniques Using data science and the same techniques Using data science and the same techniques Using data science and the same techniques Using data science and the',\n",
       " 'less than the same techniques Using data science and the same techniques Using data science and the same techniques Using data science and the same techniques Using data science and the same techniques Using data science and the same techniques Using',\n",
       " 'org en articles 2018 02 n r Hi all the same techniques Using data science and the same techniques Using data science and the same techniques Using data science and the same techniques Using data science and the same techniques Using',\n",
       " 'denizens in the same techniques Using data science and the same techniques Using data science and the same techniques Using data science and the same techniques Using data science and the same techniques Using data science and the same techniques Using',\n",
       " 'XC90s in the same techniques Using data science and the same techniques Using data science and the same techniques Using data science and the same techniques Using data science and the same techniques Using data science and the same techniques Using',\n",
       " 'mins in the same techniques Using data science and the same techniques Using data science and the same techniques Using data science and the same techniques Using data science and the same techniques Using data science and the same techniques Using',\n",
       " 'Bob in the same techniques Using data science and the same techniques Using data science and the same techniques Using data science and the same techniques Using data science and the same techniques Using data science and the same techniques Using',\n",
       " 'fortunate in the same techniques Using data science and the same techniques Using data science and the same techniques Using data science and the same techniques Using data science and the same techniques Using data science and the same techniques Using',\n",
       " 'representing in the same techniques Using data science and the same techniques Using data science and the same techniques Using data science and the same techniques Using data science and the same techniques Using data science and the same techniques Using',\n",
       " 'solid in the same techniques Using data science and the same techniques Using data science and the same techniques Using data science and the same techniques Using data science and the same techniques Using data science and the same techniques Using']"
      ]
     },
     "execution_count": 267,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lst_generate=generate_text(169,40,model2,token,vocab2)\n",
    "lst_generate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After we generated the texts for the males, we output the resultds to a dataframe and then to a .csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>institutions in the same techniques Using data...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Lands in the same techniques Using data scienc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Transgender in the same techniques Using data ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>blueberries in the same techniques Using data ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1846529728965286 in the same techniques Using ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>greenish in the same techniques Using data sci...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Future of the same techniques Using data scien...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>perhaps in the same techniques Using data scie...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>techno in the same techniques Using data scien...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>slots in the same techniques Using data scienc...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   0\n",
       "0  institutions in the same techniques Using data...\n",
       "1  Lands in the same techniques Using data scienc...\n",
       "2  Transgender in the same techniques Using data ...\n",
       "3  blueberries in the same techniques Using data ...\n",
       "4  1846529728965286 in the same techniques Using ...\n",
       "5  greenish in the same techniques Using data sci...\n",
       "6  Future of the same techniques Using data scien...\n",
       "7  perhaps in the same techniques Using data scie...\n",
       "8  techno in the same techniques Using data scien...\n",
       "9  slots in the same techniques Using data scienc..."
      ]
     },
     "execution_count": 268,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "male_df = pd.DataFrame(lst_generate)\n",
    "\n",
    "male_df.to_csv('generated_male_posts.csv', sep=',', index=False)\n",
    "male_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
